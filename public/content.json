{"meta":{"title":"rainbowhorse's blog","subtitle":"不忘初心 方得始终","description":"总结个人心得，记录个人知识","author":"rainbowhorse","url":"https://sunny0715.github.io"},"pages":[{"title":"","date":"2017-08-14T09:26:38.043Z","updated":"2017-06-04T01:41:33.000Z","comments":true,"path":"404.html","permalink":"https://sunny0715.github.io/404.html","excerpt":"","text":"404 - rainbowhorse's blog"},{"title":"About","date":"2017-03-07T01:00:07.000Z","updated":"2017-06-04T01:41:33.000Z","comments":true,"path":"about/index.html","permalink":"https://sunny0715.github.io/about/index.html","excerpt":"","text":"​ ​ 喜欢新鲜的事物，钻研技术，热爱互联网行业。 ​ 做自己喜欢做的事。 ​ 菜鸟程序猿一只，欢迎分享知识。 ​ From 彩虹马 🐧 1223585957"},{"title":"标签","date":"2017-03-22T01:01:22.000Z","updated":"2017-06-04T01:41:33.000Z","comments":true,"path":"tags/index.html","permalink":"https://sunny0715.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2017-03-22T01:01:55.000Z","updated":"2017-06-04T01:41:33.000Z","comments":true,"path":"categories/index.html","permalink":"https://sunny0715.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Mybatis之缓存","slug":"Mybatis之缓存","date":"2018-03-20T02:43:53.000Z","updated":"2018-03-20T06:17:57.946Z","comments":true,"path":"2018/03/20/Mybatis之缓存/","link":"","permalink":"https://sunny0715.github.io/2018/03/20/Mybatis之缓存/","excerpt":"引言Mybatis中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存是指SqlSession级别的缓存，当在同一个SqlSession中进行相同的SQL语句查询时，第二次以后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存1024条SQL。二级缓存是指可以跨SqlSession的缓存。","text":"引言Mybatis中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存是指SqlSession级别的缓存，当在同一个SqlSession中进行相同的SQL语句查询时，第二次以后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存1024条SQL。二级缓存是指可以跨SqlSession的缓存。 Mybatis缓存缓存的意义：将用户经常查询的数据放在缓存（内存）中，用户去查询数据就不用从磁盘上(关系型数据库数据文件)查询，从缓存中查询，从而提高查询效率，解决了高并发系统的性能问题。 Mybatis提供一级缓存和二级缓存 mybatis一级缓存是一个SqlSession级别，sqlsession只能访问自己的一级缓存的数据。 二级缓存是跨sqlSession，是mapper级别的缓存，对于mapper级别的缓存不同的sqlsession是可以共享的。 Mybatis一级缓存Mybatis的一级缓存原理： 第一次发出一个查询sql，sql查询结果写入sqlsession的一级缓存中，缓存使用的数据结构是一个map&lt;key,value&gt; key：hashcode + sql + sql输入参数 + 输出参数（sql的唯一标识） value：用户信息 同一个sqlsession再次发出相同的sql，就从缓存中取不走数据库。如果两次中间出现commit操作（修改、添加、删除），本sqlsession中的一级缓存区域全部清空，下次再去缓存中查询不到所以要从数据库查询，从数据库查询到再写入缓存。 Mybatis一级缓存值得注意的地方： Mybatis默认就是支持一级缓存的，并不需要我们配置。 mybatis和spring整合后进行mapper代理开发，不支持一级缓存，mybatis和spring整合，spring按照mapper的模板去生成mapper代理对象，模板中在最后统一关闭sqlsession。 Mybatis二级缓存二级缓存原理： 二级缓存的范围是mapper级别（mapper同一个命名空间），mapper以命名空间为单位创建缓存数据结构，结构是map&lt;key、value&gt;。 二级缓存配置Mybatis二级缓存需要手动开启，需要在Mybatis的配置文件中配置二级缓存 12345&lt;!-- 全局配置参数 --&gt;&lt;settings&gt; &lt;!-- 开启二级缓存 --&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt;&lt;/settings&gt; 上面已经说了，二级缓存的范围是mapper级别的，因此Mapper如果要使用二级缓存，还需要在对应的映射文件中配置。 123&lt;mapper namespace=\"com.rainbowhorse.test.dao.TestDataDao\"&gt;&lt;!-- 在mapper中开启二级缓存 --&gt;&lt;cache/&gt; 查询结果映射的pojo序列化 mybatis二级缓存需要将查询结果映射的pojo实现 java.io.serializable接口，如果不实现则抛出异常： 1org.apache.ibatis.cache.CacheException: Error serializing object. Cause: java.io.NotSerializableException: com.rainbowhorse.test.po.User 二级缓存可以将内存的数据写到磁盘，存在对象的序列化和反序列化，所以要实现java.io.serializable接口。 如果结果映射的pojo中还包括了pojo，都要实现java.io.serializable接口。 禁用二级缓存对于变化频率较高的sql，需要禁用二级缓存： 在statement中设置useCache=false可以禁用当前select语句的二级缓存，即每次查询都会发出sql去查询，默认情况是true，即该sql使用二级缓存。 1&lt;select id=\"findOrderListResultMap\" resultMap=\"ordersUserMap\" useCache=\"false\"&gt; 刷新缓存我们的缓存都是在查询语句中配置，而使用增删改的时候，缓存默认就会被清空【刷新了】。缓存其实就是为我们的查询服务的，对于增删改而言，如果我们的缓存保存了增删改后的数据，那么再次读取时就会读到脏数据了！ 我们在特定的情况下，还可以单独配置刷新缓存【但不建议使用】flushCache，默认是的true。 123&lt;update id=\"updateUser\" parameterType=\"cn.itcast.mybatis.po.User\" flushCache=\"false\"&gt; update user set username=#&#123;username&#125;,birthday=#&#123;birthday&#125;,sex=#&#123;sex&#125;,address=#&#123;address&#125; where id=#&#123;id&#125; &lt;/update&gt; 缓存参数mybatis的cache参数只适用于mybatis维护缓存。 flushInterval（刷新间隔）可以被设置为任意的正整数，而且它们代表一个合理的毫秒形式的时间段。默认情况是不设置，也就是没有刷新间隔，缓存仅仅调用语句时刷新。size（引用数目）可以被设置为任意正整数，要记住你缓存的对象数目和你运行环境的可用内存资源数目。默认值是1024。readOnly（只读）属性可以被设置为true或false。只读的缓存会给所有调用者返回缓存对象的相同实例。因此这些对象不能被修改。这提供了很重要的性能优势。可读写的缓存会返回缓存对象的拷贝（通过序列化）。这会慢一些，但是安全，因此默认是false。 如下例子：&lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt;这个更高级的配置创建了一个 FIFO 缓存,并每隔 60 秒刷新,存数结果对象或列表的 512 个引用,而且返回的对象被认为是只读的,因此在不同线程中的调用者之间修改它们会导致冲突。可用的收回策略有, 默认的是 LRU: LRU – 最近最少使用的:移除最长时间不被使用的对象。 FIFO – 先进先出:按对象进入缓存的顺序来移除它们。 SOFT – 软引用:移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用:更积极地移除基于垃圾收集器状态和弱引用规则的对象。 Mybatis和Ehcache整合Ehcache是专门用于管理缓存的，Mybatis的缓存交由ehcache管理会更加得当。在mybatis中提供一个cache接口，只要实现cache接口就可以把缓存数据灵活的管理起来。 整合jar包 mybatis-ehcache-1.0.2.jar ehcache-core-2.6.5.jar Ehcache对cache接口的实现类： ehcache.xml配置信息这个xml配置文件是配置全局的缓存管理方案 12345678910111213141516&lt;ehcache xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"../config/ehcache.xsd\"&gt; &lt;!--diskStore：缓存数据持久化的目录 地址 --&gt; &lt;diskStore path=\"F:\\develop\\ehcache\" /&gt; &lt;defaultCache maxElementsInMemory=\"1000\" maxElementsOnDisk=\"10000000\" eternal=\"false\" overflowToDisk=\"false\" diskPersistent=\"true\" timeToIdleSeconds=\"120\" timeToLiveSeconds=\"120\" diskExpiryThreadIntervalSeconds=\"120\" memoryStoreEvictionPolicy=\"LRU\"&gt; &lt;/defaultCache&gt;&lt;/ehcache&gt; 如果我们Mapper想单独拥有一些特性，需要在mapper.xml中单独配置 12345678910&lt;!-- 单位：毫秒 --&gt;&lt;cache type=\"org.mybatis.caches.ehcache.EhcacheCache\"&gt; &lt;property name=\"timeToIdleSeconds\" value=\"12000\"/&gt; &lt;property name=\"timeToLiveSeconds\" value=\"3600\"/&gt; &lt;!-- 同ehcache参数maxElementsInMemory --&gt; &lt;property name=\"maxEntriesLocalHeap\" value=\"1000\"/&gt; &lt;!-- 同ehcache参数maxElementsOnDisk --&gt; &lt;property name=\"maxEntriesLocalDisk\" value=\"10000000\"/&gt; &lt;property name=\"memoryStoreEvictionPolicy\" value=\"LRU\"/&gt;&lt;/cache&gt; 应用场景与局限性应用场景 对查询频率高，变化频率低的数据建议使用二级缓存。 对于访问多的查询请求且用户对查询结果实时性要求不高，此时可采用mybatis二级缓存技术降低数据库访问量，提高访问速度。 业务场景比如： 耗时较高的统计分析sql 电话账单查询sql等。 实现方法如下：通过设置刷新间隔时间，由mybatis每隔一段时间自动清空缓存，根据数据变化频率设置缓存刷新间隔flushInterval，比如设置为30分钟、60分钟、24小时等，根据需求而定。 局限性mybatis二级缓存对细粒度的数据级别的缓存实现不好，比如如下需求：对商品信息进行缓存，由于商品信息查询访问量大，但是要求用户每次都能查询最新的商品信息，此时如果使用mybatis的二级缓存就无法实现当一个商品变化时只刷新该商品的缓存信息而不刷新其它商品的信息，因为mybaits的二级缓存区域以mapper为单位划分，当一个商品信息变化会将所有商品信息的缓存数据全部清空。解决此类问题需要在业务层根据需求对数据有针对性缓存。 总结 Mybatis的一级缓存是SqlSession级别的。只能访问自己的sqlSession内的缓存。如果Mybatis与Spring整合了，Spring会自动关闭sqlSession的。所以一级缓存会失效。 一级缓存的原理是map集合，Mybatis默认就支持一级缓存。 二级缓存是Mapper级别的。只要在Mapper namespace下都可以使用二级缓存。需要自己手动去配置二级缓存。 Mybatis的缓存我们可以使用Ehcache框架来进行管理，Ehcache实现Cache接口就代表使用Ehcache来环境Mybatis缓存。 参考Mybatis【逆向工程，缓存，代理】知识要点","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"mybatis","slug":"mybatis","permalink":"https://sunny0715.github.io/tags/mybatis/"}]},{"title":"线程池之ThreadPoolExecutor","slug":"线程池之ThreadPoolExecutor","date":"2018-03-17T06:09:10.000Z","updated":"2018-03-20T01:58:33.008Z","comments":true,"path":"2018/03/17/线程池之ThreadPoolExecutor/","link":"","permalink":"https://sunny0715.github.io/2018/03/17/线程池之ThreadPoolExecutor/","excerpt":"引言JAVA对于多线程的封装非常丰富，提供了多种适用于不同场景的多并发实现。但如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，这样频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间。这里就引入了线程池来管理线程，其中最基础、最核心的线程池要属ThreadPoolExecutor类了。","text":"引言JAVA对于多线程的封装非常丰富，提供了多种适用于不同场景的多并发实现。但如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，这样频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间。这里就引入了线程池来管理线程，其中最基础、最核心的线程池要属ThreadPoolExecutor类了。 Java中的ThreadPoolExecutor类java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类，因此如果要透彻地了解Java中的线程池，必须先了解这个类。下面我们来看一下ThreadPoolExecutor类的具体实现源码。 在ThreadPoolExecutor类中提供了四个构造方法： 123456789101112131415161718192021public class ThreadPoolExecutor extends AbstractExecutorService &#123; ..... public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,RejectedExecutionHandler handler); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory,RejectedExecutionHandler handler); ...&#125;//corePoolSize： 线程池维护线程的最少数量 //maximumPoolSize：线程池维护线程的最大数量 //keepAliveTime： 线程池维护线程所允许的空闲时间 //unit： 线程池维护线程所允许的空闲时间的单位 //workQueue： 线程池所使用的缓冲队列 //handler： 线程池对拒绝任务的处理策略 从上面的代码可以得知，ThreadPoolExecutor继承了AbstractExecutorService类，并提供了四个构造器，事实上，通过观察每个构造器的源码具体实现，发现前面三个构造器都是调用的第四个构造器进行的初始化工作。 corePoolSize：核心池的大小。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数，它表示在线程池中最多能创建多少个线程； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性： 1234567TimeUnit.DAYS; //天TimeUnit.HOURS; //小时TimeUnit.MINUTES; //分钟TimeUnit.SECONDS; //秒TimeUnit.MILLISECONDS; //毫秒TimeUnit.MICROSECONDS; //微妙TimeUnit.NANOSECONDS; //纳秒 workQueue：一个阻塞队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列有以下几种选择： 123ArrayBlockingQueue;LinkedBlockingQueue;SynchronousQueue; ArrayBlockingQueue和PriorityBlockingQueue使用较少，一般使用LinkedBlockingQueue和Synchronous。线程池的排队策略与BlockingQueue有关。 threadFactory：线程工厂，主要用来创建线程； handler：表示当拒绝处理任务时的策略，有以下四种取值： 1234ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 由图中我们可得知ThreadPoolExecutor继承了AbstractExecutorService，AbstractExecutorService是一个抽象类，它实现了ExecutorService接口，而ExecutorService又是继承了Executor接口。 线程池实现原理线程池状态在ThreadPoolExecutor中定义了几个static final变量表示线程池的各个状态： 1234567891011121314151617181920private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; * RUNNING -&gt; SHUTDOWN * On invocation of shutdown(), perhaps implicitly in finalize() * (RUNNING or SHUTDOWN) -&gt; STOP * On invocation of shutdownNow() * SHUTDOWN -&gt; TIDYING * When both queue and pool are empty * STOP -&gt; TIDYING * When pool is empty * TIDYING -&gt; TERMINATED * When the terminated() hook method has completed **/private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; 下面的几个static final变量表示runState可能的几个取值。 RUNNING：允许接收新任务并且处理队列中的任务。 SHUTDOWN：不再接收新的任务，仅消化完队列中的任务。 STOP：不仅不再接收新的任务，连队列中的任务都不再消化处理了，并且尝试中断正在执行任务的线程。 TIDYING：所有任务被终止了，工作线程数workCount也被设为0，线程的状态也被设为TIDYING，并开始调用钩子函数terminated()。 TERMINATED：钩子函数terminated()执行完毕。 任务的执行123456789101112131415161718private final BlockingQueue&lt;Runnable&gt; workQueue; //任务缓存队列，用来存放等待执行的任务private final ReentrantLock mainLock = new ReentrantLock(); //线程池的主要状态锁，对线程池状态（比如线程池大小/runState等）的改变都要使用这个锁private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); //用来存放工作集private volatile long keepAliveTime; //线程存活时间 private volatile boolean allowCoreThreadTimeOut; //是否允许为核心线程设置存活时间private volatile int corePoolSize; //核心池的大小（即线程池中的线程数目大于这个参数时，提交的任务会被放进任务缓存队列）private volatile int maximumPoolSize; //线程池最大能容忍的线程数private volatile int poolSize; //线程池中当前的线程数private volatile RejectedExecutionHandler handler; //任务拒绝策略private volatile ThreadFactory threadFactory; //线程工厂，用来创建线程private int largestPoolSize; //用来记录线程池中曾经出现过的最大线程数private long completedTaskCount; //用来记录已经执行完毕的任务个数 每个变量的作用都已经标明出来了，这里要重点解释一下corePoolSize、maximumPoolSize、largestPoolSize三个变量。 如果当前线程池中的线程数目小于corePoolSize，则每来一个任务，就会创建一个线程去执行这个任务； 如果当前线程池中的线程数目&gt;=corePoolSize，则每来一个任务，会尝试将其添加到任务缓存队列当中，若添加成功，则该任务会等待空闲线程将其取出去执行；若添加失败（一般来说是任务缓存队列已满），则会尝试创建新的线程去执行这个任务； 如果当前线程池中的线程数目达到maximumPoolSize，则会采取任务拒绝策略进行处理； 如果线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止，直至线程池中的线程数目不大于corePoolSize；如果允许为核心池中的线程设置存活时间，那么核心池中的线程空闲时间超过keepAliveTime，线程也会被终止。 任务缓存及排队策略在前面我们多次提到了任务缓存队列，即workQueue，它用来存放等待执行的任务。workQueue的类型为BlockingQueue&lt;Runnable&gt;，通常可以取下面三种类型： 1）ArrayBlockingQueue：有界队列，有助于防止资源耗尽，但是可能较难调整和控制。队列大小和最大池大小可能需要相互折衷：使用大型队列和小型池可以最大限度地降低 CPU 使用率、操作系统资源和上下文切换开销，但是可能导致人工降低吞吐量。如果任务频繁阻塞（例如，如果它们是 I/O 边界），则系统可能为超过许可的更多线程安排时间。使用小型队列通常要求较大的池大小，CPU 使用率较高，但是可能遇到不可接受的调度开销，这样也会降低吞吐量。 2）LinkedBlockingQueue：无界队列，将导致在所有 corePoolSize 线程都忙时新任务在队列中等待。这样，创建的线程就不会超过 corePoolSize。（因此，maximumPoolSize 的值也就无效了。）当每个任务完全独立于其他任务，即任务执行互不影响时，适合于使用无界队列；例如，在 Web 页服务器中。这种排队可用于处理瞬态突发请求，当命令以超过队列所能处理的平均数连续到达时，此策略允许无界线程具有增长的可能性。 3）SynchronousQueue：工作队列的默认选项是 SynchronousQueue，它将任务直接提交给线程而不保持它们。在此，如果不存在可用于立即运行任务的线程，则试图把任务加入队列将失败，因此会构造一个新的线程。此策略可以避免在处理可能具有内部依赖性的请求集时出现锁。直接提交通常要求无界 maximumPoolSizes 以避免拒绝新提交的任务。当命令以超过队列所能处理的平均数连续到达时，此策略允许无界线程具有增长的可能性。 任务拒绝策略当线程池的任务缓存队列已满并且线程池中的线程数目达到maximumPoolSize，如果还有任务到来就会采取任务拒绝策略，通常有以下四种策略： 1234ThreadPoolExecutor.AbortPolicy: 丢弃任务并抛出RejectedExecutionException异常。ThreadPoolExecutor.DiscardPolicy： 也是丢弃任务，但是不抛出异常。ThreadPoolExecutor.DiscardOldestPolicy： 丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy： 由调用线程处理该任务 线程池的关闭ThreadPoolExecutor提供了两个方法，用于线程池的关闭，分别是shutdown()和shutdownNow()： shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务。 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务。 线程池容量的动态调整ThreadPoolExecutor提供了动态调整线程池容量大小的方法：setCorePoolSize()和setMaximumPoolSize()： setCorePoolSize：设置核心池大小。 setMaximumPoolSize：设置线程池最大能创建的线程数目大小。 使用Executors创建线程池Executors提供的工厂方法，可以创建以下四种类型线程池： newFixedThreadPool：该方法将用于创建一个固定大小的线程池（此时corePoolSize = maxPoolSize），每提交一个任务就创建一个线程池，直到线程池达到最大数量，线程池的规模在此后不会发生任何变化； newCachedThreadPool：该方法创建了一个可缓存的线程池，（此时corePoolSize = 0，maxPoolSize = Integer.MAX_VALUE），空闲线程超过60秒就会被自动回收，该线程池存在的风险是，如果服务器应用达到请求高峰期时，会不断创建新的线程，直到内存耗尽； newSingleThreadExecutor：该方法创建了一个单线程的线程池，该线程池按照任务在队列中的顺序串行执行（如：FIFO、LIFO、优先级）； newScheduledThreadPool：该方法创建了一个固定长度的线程池，可以以延迟或者定时的方式执行任务； 实例12345678910111213141516171819202122232425262728293031323334public class Test &#123; public static void main(String[] args) &#123; ThreadFactory factory = new ThreadFactoryBuilder().setNameFormat(\"test\").build(); ThreadPoolExecutor executor = new ThreadPoolExecutor(2, 5, 200, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(5),factory,new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 1; i &lt;= 10; i++) &#123; MyTask myTask = new MyTask(i); executor.execute(myTask); System.out.println(\"线程池中线程数目：\" + executor.getPoolSize() + \"，队列中等待执行的任务数目：\" + executor.getQueue().size() + \"，已执行玩别的任务数目：\" + executor.getCompletedTaskCount()); &#125; executor.shutdown(); &#125;&#125;class MyTask implements Runnable &#123; private int taskNum; public MyTask(int num) &#123; this.taskNum = num; &#125; @Override public void run() &#123; System.out.println(\"正在执行task \" + taskNum); try &#123; Thread.currentThread().sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"task \" + taskNum + \"执行完毕\"); &#125;&#125; 执行结果： 合理配置线程池大小一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1 如果是IO密集型任务，参考值可以设置为2*NCPU 这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。 参考Java并发编程：线程池的使用","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"}]},{"title":"Spring+webSocket","slug":"Spring-webSocket","date":"2018-03-16T06:00:11.000Z","updated":"2018-03-17T04:30:47.275Z","comments":true,"path":"2018/03/16/Spring-webSocket/","link":"","permalink":"https://sunny0715.github.io/2018/03/16/Spring-webSocket/","excerpt":"引言websocket 是 HTML5新增加特性之一，目的是浏览器与服务端建立全双工的通信方式，解决 HTTP请求-响应带来过多的资源消耗，同时对特殊场景应用提供了全新的实现方式，比如聊天、股票交易、游戏等对对实时性要求较高的行业领域。","text":"引言websocket 是 HTML5新增加特性之一，目的是浏览器与服务端建立全双工的通信方式，解决 HTTP请求-响应带来过多的资源消耗，同时对特殊场景应用提供了全新的实现方式，比如聊天、股票交易、游戏等对对实时性要求较高的行业领域。 STOMPSTOMP(Simple Text-Orientated Messaging Protocol) 面向消息的简单文本协议。 WebSocket是一个消息架构，不强制使用任何特定的消息协议，它依赖于应用层解释消息的含义； 与处在应用层的HTTP不同，WebSocket处在TCP上非常薄的一层，会将字节流转换为文本/二进制消息，因此，对于实际应用来说，WebSocket的通信形式层级过低，因此，可以在 WebSocket 之上使用 STOMP协议，来为浏览器 和 server间的 通信增加适当的消息语义。 如何理解 STOMP 与 WebSocket 的关系：1) HTTP协议解决了 web 浏览器发起请求以及 web 服务器响应请求的细节，假设 HTTP 协议 并不存在，只能使用 TCP 套接字来 编写 web 应用，你可能认为这是一件疯狂的事情；2) 直接使用 WebSocket（SockJS） 就很类似于 使用 TCP 套接字来编写 web 应用，因为没有高层协议，就需要我们定义应用间所发送消息的语义，还需要确保连接的两端都能遵循这些语义；3) 同 HTTP 在 TCP 套接字上添加请求-响应模型层一样，STOMP 在 WebSocket 之上提供了一个基于帧的线路格式层，用来定义消息语义； Spring+websocket添加依赖需要添加spring-websocket和spring-messaging依赖，注意和spring-core的版本保持一致。 1234567891011&lt;!-- spring-websocket --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-websocket&lt;/artifactId&gt; &lt;version&gt;4.1.9.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-messaging&lt;/artifactId&gt; &lt;version&gt;4.1.9.RELEASE&lt;/version&gt;&lt;/dependency&gt; 服务端代码服务端的初始化，只需要两个类：WebsocketConfig（stomp节点配置）和WebSocketController。 12345678910111213141516171819202122232425262728import org.springframework.context.annotation.Configuration;import org.springframework.messaging.simp.config.MessageBrokerRegistry;import org.springframework.web.socket.config.annotation.AbstractWebSocketMessageBrokerConfigurer;import org.springframework.web.socket.config.annotation.EnableWebSocketMessageBroker;import org.springframework.web.socket.config.annotation.StompEndpointRegistry;/** * 通过EnableWebSocketMessageBroker 开启使用STOMP协议来传输基于代理(message broker)的消息,此时浏览器支持使用@MessageMapping 就像支持@RequestMapping一样。 */@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketConfig extends AbstractWebSocketMessageBrokerConfigurer &#123; @Override public void registerStompEndpoints(StompEndpointRegistry registry) &#123; //endPoint 注册协议节点,并映射指定的URl //注册一个名字为\"endpointChat\" 的endpoint,并指定 SockJS协议，客户端就可以通过这个端点来进行连接；withSockJS作用是添加SockJS支持。 registry.addEndpoint(\"/endpointChat\").withSockJS(); &#125; @Override public void configureMessageBroker(MessageBrokerRegistry registry) &#123; //配置消息代理(message broker)，定义了两个客户端订阅地址的前缀信息，也就是客户端接收服务端发送消息的前缀信息 //点对点式增加一个/queue 消息代理 registry.enableSimpleBroker(\"/queue\", \"/topic\"); //定义了服务端接收地址的前缀，也即客户端给服务端发消息的地址前缀 //registry.setApplicationDestinationPrefixes(“/user”); &#125;&#125; 对以上代码分析： EnableWebSocketMessageBroker 注解表明： 这个配置类不仅配置了 WebSocket，还配置了基于代理的 STOMP 消息； 它复写了 registerStompEndpoints() 方法：添加一个服务端点，来接收客户端的连接。将 “/endpointChat” 路径注册为 STOMP 端点。这个路径与之前发送和接收消息的目的路径有所不同， 这是一个端点，客户端在订阅或发布消息到目的地址前，要连接该端点，即用户发送请求 ：URL=’/127.0.0.1:8080/endpointChat’ 与 STOMP server 进行连接，之后再转发到订阅URL； 它复写了 configureMessageBroker() 方法：配置了一个 简单的消息代理，通俗一点讲就是设置消息连接请求的各种规范信息。 12345678910111213141516171819202122232425262728293031import org.springframework.messaging.handler.annotation.MessageMapping;import org.springframework.messaging.simp.SimpMessagingTemplate;import org.springframework.stereotype.Controller;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import org.springframework.beans.factory.annotation.Autowired;import com.thinkgem.jeesite.modules.sys.utils.UserUtils;@Controller@RequestMapping(\"/websocket\")public class WebsocketController &#123; @Autowired private SimpMessagingTemplate template; @MessageMapping(\"/sendMsg\") public void roomMessage() &#123; // 多线程配置推送消息 ThreadPoolExecutor executor = new ThreadPoolExecutor(1, 1, 0, TimeUnit.MILLISECONDS,new LinkedBlockingQueue&lt;Runnable&gt;()); executor.execute(new Runnable() &#123; @Override public void run() &#123; template.convertAndSendToUser(userId, \"/queue/notifications\",\"新消息：这是websocked测试消息\");// 一对一发送，发送特定的客户端 //template.convertAndSend(\"/topic/getResponse\",\"新消息：这是websocked测试消息\");//广播消息 &#125; &#125;); executor.shutdown(); &#125;&#125; template.convertAndSendToUser(user, dest, message) 这个方法官方给出的解释是 Convert the given Object to serialized form, possibly using a MessageConverter, wrap it as a message and send it to the given destination. 意思就是“将给定的对象进行序列化，使用 ‘MessageConverter’ 进行包装转化成一条消息，发送到指定的目标”，通俗点讲就是我们使用这个方法进行消息的转发发送。 客户端实现首先引用 sockjs.js 和 stomp.js 12345678910111213141516171819202122232425262728&lt;script src=\"/js/common/sockjs.min.js\"&gt;&lt;script src=\"/js/common/stomp.min.js\"&gt;&lt;script type=\"text/javascript\"&gt; $(function() &#123; connect(); &#125;); function connect() &#123; // TOMP客户端要想接收来自服务器推送的消息，必须先订阅相应的URL，即发送一个SUBSCRIBE帧，然后才能不断接收来自服务器的推送消息； var sock = new SockJS(\"http://localhost:8080/endpointChat\"); var stomp = Stomp.over(sock); stomp.connect('guest', 'guest', function(frame) &#123; /**订阅了/user/queue/notifications 发送的消息,这里与在控制器convertAndSendToUser 定义的地址保持一致 * 这里多用了一个/user,并且这个user 是必须的,使用user才会发送消息到指定的用户。 * */ stomp.subscribe(\"/user/queue/notifications\", handleNotification); stomp.subscribe('/topic/getResponse', function(response) &#123; //订阅/topic/getResponse 目标发送的消息。这个是在控制器的@SendTo中定义的。 console.info(response.body); &#125;); //向服务端发送消息 stomp.send(\"URL\", &#123;&#125;, JSON.stringify(message)); //订阅服务器发送来的消息 function handleNotification(message) &#123; console.info(message.body); &#125; &#125;&lt;/script&gt; 利用 stomp的connect(login, passcode, connectCallback, errorCallback, vhost) 方法建立连接，值得注意的是不同版本的 stomp.js 的 connect() 函数的参数会有所不同； 利用 stomp的subscribe(destination, callback, headers) 方法可以订阅服务器发送来的消息，destination 表示服务器发送消息地址；通过 event 的 body 获取消息内容； 利用 stompClient 的send(destination, headers, body) 方法可以向服务端发送消息，第一个参数为发送消息地址，最后一个参数是发送消息的 json 串； 测试在客户端请求/websocket/sendMsg后会有如下效果： 参考： Spring Framework Reference Documentation websocket+spring spring websocket + stomp 实现广播通信和一对一通信","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"spring","slug":"spring","permalink":"https://sunny0715.github.io/tags/spring/"}]},{"title":"谈谈敏捷开发","slug":"谈谈敏捷开发","date":"2018-03-15T01:34:03.000Z","updated":"2018-03-15T02:35:38.089Z","comments":true,"path":"2018/03/15/谈谈敏捷开发/","link":"","permalink":"https://sunny0715.github.io/2018/03/15/谈谈敏捷开发/","excerpt":"引言敏捷开发是一种从上个世纪九十年代开始逐渐引起广泛关注的一些新型软件开发方法，是一种应对快速变化的需求的一种软件开发能力。常言道：“天下武功，唯快不破”，此言用于形容“敏捷”的威力相当合适。敏捷意思为快，但敏捷思想不仅仅求快，它更多强调“多快好省”，产出得多，产出得快，产出得好，同时节省各种成本，既经济又实用。它们更强调程序员团队与业务专家之间的紧密协作、面对面的沟通（认为比书面的文档更有效）、频繁交付新的软件版本、紧凑而自我组织型的团队、能够很好地适应需求变化的代码编写和团队组织方法，也更注重软件开发过程中人的作用。","text":"引言敏捷开发是一种从上个世纪九十年代开始逐渐引起广泛关注的一些新型软件开发方法，是一种应对快速变化的需求的一种软件开发能力。常言道：“天下武功，唯快不破”，此言用于形容“敏捷”的威力相当合适。敏捷意思为快，但敏捷思想不仅仅求快，它更多强调“多快好省”，产出得多，产出得快，产出得好，同时节省各种成本，既经济又实用。它们更强调程序员团队与业务专家之间的紧密协作、面对面的沟通（认为比书面的文档更有效）、频繁交付新的软件版本、紧凑而自我组织型的团队、能够很好地适应需求变化的代码编写和团队组织方法，也更注重软件开发过程中人的作用。 敏捷开发宣言和原则敏捷宣言的价值观 个体和互动 高于 流程和工具 工作的软件 高于 详尽的文档 客户合作 高于 合同谈判 响应变化 高于 遵循计划 也就是说，尽管右项有其价值，我们更重视左项的价值。 敏捷宣言的原则 我们最重要的目标，是通过持续不断地及早交付有价值的软件使客户满意。 欣然面对需求变化，即使在开发后期也一样。为了客户的竞争优势，敏捷过程掌控变化。 经常地交付可工作的软件，相隔几星期或一两个月，倾向于采取较短的周期。 业务人员和开发人员必须相互合作，项目中的每一天都不例外。 激发个体的斗志，以他们为核心搭建项目。提供所需的环境和支援，辅以信任，从而达成目标。 不论团队内外，传递信息效果最好效率也最高的方式是面对面的交谈。 可工作的软件是进度的首要度量标准。 敏捷过程倡导可持续开发。责任人、开发人员和用户要能够共同维持其步调稳定延续。 坚持不懈地追求技术卓越和良好设计，敏捷能力由此增强。 以简洁为本，它是极力减少不必要工作量的艺术。 最好的架构、需求和设计出自自组织团队。 团队定期地反思如何能提高成效，并依此调整自身的举止表现。 敏捷宣言中这些价值观和原则，所谓入门容易精通难，在旁人或者入门者看来也许略显空洞，所言无物；然而，敏捷精髓的实践者们一直坚信着这些理念，觉得它们字字珠玑，所言极是。不管我们是走敏捷的哪个流派，都不妨在实践敏捷的过程中，回过头来看看这些价值观和原则，想必会常看常新，大有裨益。 Scrum核心 团队密切协作敏捷开发中，最核心的就是人。 不是说每个人坐在自己的格子间里，各自独立开发，定期向领导汇报工作就完事了，敏捷开发需要做到几个关键点： 定期会面。通过定期高效会议让开发人员保持紧张有序的工作状态。 及时告知项目进展。鼓励遇到问题一定要及时告知，让所有利益相关者都能够及时了解项目的最新进展。 知识共享。把知识分享出来，可以提升整个团队的开发能力，是对团队的一种投资。 代码共享。把代码集中在版本管理工具之中，团队任何人都有访问权限。 代码审查。针对代码的每个改动，都需要相关人员做代码审查。 不断反馈和调整敏捷开发与传统瀑布式开发一个最大的不同就是，并非一次定终身。 软件开发不是线性过程，它存在很多的不确定性，有一定的动态波动，所以需要不断的反馈、调整，快速响应变化。 需求调整。在产品落地前，你是不可能制定出完美需求方案的，客户也并不一定能够清晰的知道他想要的软件产品到底是什么样子。所以，要做好需求不断变化的心理准备，也要有快速响应需求变化的能力。 功能调整。收集客户反馈、用户反响，来不断调整和优化软件功能。 代码重构。在开发的过程中，需要不断的重构代码，保持代码清晰、内聚、整洁。 保持软件可用传统软件开发的方式是，等代码编写完毕，所有功能都完成之后，再集中测试和上线，在这之前，软件都是用户不可用的。 在信息快速发展的今天，这显然太落伍了，可能等你软件开发个一年半载，外面都变天了。 敏捷开发，另一大特色，就是保持软件一直可用，在最小可用版本基础之上，不断做功能迭代，不断发布新的版本。 通俗点讲就是，先做一个简化版本出来，让用户一直有软件可用，而后再逐步添加更多的功能，「小步快跑」式的做开发，而非一步到位。 这样做的好处，也是为了能够不断收集用户反馈与需求，及时调整开发方向。 短迭代 增量发布这一条是上一条的延伸。 所谓「迭代」就是，重复下一个开发周期，此刻你可以想象一下 while loop。 用迭代的方法在前一版本之上逐步开发新的功能，发布新的版本，即：增量发布。 迭代周期不宜过长，一年半年显然就太久了，尽量要缩短迭代周期，保持开发过程稳步前进。 但短迭代并不意味着太过频繁，每天或者每周迭代一次的话，很可能会过犹不及，半个月到一个月应该是一个不错的时间节点。当然这只是参考建议，具体的迭代周期还应根据真实情况量力而行。 提早集成 不断集成「集成」的含义就是，把软件的各个模块，新旧代码统一整合在一起，能够正确编译、运行，并且能够通过一系列的单元测试。 敏捷开发要求开发人员，不要临到软件发布或者交付的当天才开始集成，也不要很久才集成一次，尽可能的做到提早继承、频繁集成。理论上讲，每添加进一些新的代码，最好都要做一次集成。 通过提早集成、不断集成，能够尽早发现代码中的问题，保持软件的状态一直是可用的。 自动化集成、测试与部署敏捷的另一个关键点是，通过技术手段把集成、测试与部署这些非常耗时的操作自动化。 至于为什么要自动化，如果你是一个人开发一个几千行的程序，那确实没必要自动化，确切的说，也没必要应用什么敏捷开发。 针对开发大型软件的团队而言，编译、测试过程有可能都非常的耗时，编译有时会花上半天的时间，测试可能会持续好几天。而且是多人协作共同开发，如果纯手动的话，你想象一下如果两个人想前后脚提交代码，岂不是要等上好几天的时间，等前面那个人集成结束之后，后面那个人才可以开始集成？ 把集成、测试与部署自动化的好处就是，把这些耗时的纯体力劳动扔给机器去做，它做完了，只要返回你一个最终结果就好了，而且两个人同时 Check In 时也不会受到影响。 这副图涵盖了自动化集成、测试以及部署的流程。 开发者 Check In 之后的所有工作将都交给机器去做，它们都有专门的工具 ，包括集成工具、测试工具，当然这些工具可以使用第三方的，也可以自己开发。 一般情况下，这一系列的工作都跟版本管理工具绑定在一起，你只要 Check In 就会触发集成、测试，甚至还有部署。而你，只要专心的等待结果就好。 总结在流程上，敏捷最大的特色是迭代式开发。敏捷开发可以用一句话概括：拥抱变化，轻量文档，团队合作，多个短的冲刺周期 参考文章： https://www.zybuluo.com/yishuailuo/note/672154?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://sunny0715.github.io/tags/随笔/"}]},{"title":"Sping定时任务","slug":"Sping定时任务","date":"2018-03-07T09:09:22.000Z","updated":"2018-03-13T05:46:53.196Z","comments":true,"path":"2018/03/07/Sping定时任务/","link":"","permalink":"https://sunny0715.github.io/2018/03/07/Sping定时任务/","excerpt":"引言在企业开发中，经常会遇到时间任务调度的需求，比如每天凌晨生成前天报表、数据汇总等动态配置是否开启定时的任务。在Java领域中，定时任务的开源工具也非常多，小到一个Timer类，大到Quartz框架。在Spring中最常见的定时任务方式属Spring schedule注解的方式和利用Quartz动态管理定时任务。总体来说，个人比较喜欢的还是Quartz，功能强大而且使用方便。","text":"引言在企业开发中，经常会遇到时间任务调度的需求，比如每天凌晨生成前天报表、数据汇总等动态配置是否开启定时的任务。在Java领域中，定时任务的开源工具也非常多，小到一个Timer类，大到Quartz框架。在Spring中最常见的定时任务方式属Spring schedule注解的方式和利用Quartz动态管理定时任务。总体来说，个人比较喜欢的还是Quartz，功能强大而且使用方便。 Spring-@scheduled对于较简单的任务可以使用Spring内置的定时任务方法@scheduled注解进行配置达到自己的需求。 spring配置文件配置spring项目的基础文件spring.xml 123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;beans xmlns:task=\"http://www.springframework.org/schema/task\" xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.1.xsd\"&gt; &lt;!-- 开启定时任务 spring的定时任务默认是单线程，多个任务执行起来时间会有问题，所以这里配置了线程池--&gt; &lt;task:executor id=\"executor\" pool-size=\"5\" /&gt; &lt;task:scheduler id=\"scheduler\" pool-size=\"10\" /&gt; &lt;task:annotation-driven executor=\"executor\" scheduler=\"scheduler\" /&gt; &lt;/beans&gt; Task任务类定义了一个任务类ATask，里面有两个定时任务aTask和bTask。编写java业务代码，需要在类声明上边添加@Component注解，并在需要定时任务执行的方法声明上添加@Scheduled注解以及cron表达式和相关的参数。 12345678910111213141516// 定时器的任务方法不能有返回值@Componentpublic class ATask &#123; @Scheduled(cron = \"0/10 * * * * ? \") // 每10秒执行一次 public void aTask() &#123; DateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); System.out.println(sdf.format(DateTime.now().toDate()) + \"*********A任务每10秒执行一次进入测试\"); &#125; @Scheduled(cron = \"0/5 * * * * ? \") // 每5秒执行一次 public void bTask() &#123; DateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); System.out.println(sdf.format(DateTime.now().toDate()) + \"*********B任务每5秒执行一次进入测试\"); &#125;&#125; 运行结果启动项目会发现定时任务已经开启。 Spring-Quartz@scheduled固然可以实现定时任务，但是仔细想想并不灵活，任务随着应用的启动而执行，并不能动态的进行管理，很是不方便，然而Quartz很好的解决了这一问题。 引入依赖12345&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt; 任务管理类QuartzManager123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151public class QuartzManager &#123; private static SchedulerFactory schedulerFactory = new StdSchedulerFactory(); /** * @Description: 添加一个定时任务 * * @param jobName * 任务名 * @param jobGroupName * 任务组名 * @param triggerName * 触发器名 * @param triggerGroupName * 触发器组名 * @param jobClass * 任务 * @param cron * 时间设置，参考quartz说明文档 */ @SuppressWarnings(&#123; \"unchecked\", \"rawtypes\" &#125;) public static void addJob(String jobName, String jobGroupName, String triggerName, String triggerGroupName, Class jobClass, String cron) &#123; try &#123; Scheduler sched = schedulerFactory.getScheduler(); // 任务名，任务组，任务执行类 JobDetail jobDetail = JobBuilder.newJob(jobClass).withIdentity(jobName, jobGroupName).build(); // 触发器 TriggerBuilder&lt;Trigger&gt; triggerBuilder = TriggerBuilder.newTrigger(); // 触发器名,触发器组 triggerBuilder.withIdentity(triggerName, triggerGroupName); triggerBuilder.startNow(); // 触发器时间设定 triggerBuilder.withSchedule(CronScheduleBuilder.cronSchedule(cron)); // 创建Trigger对象 CronTrigger trigger = (CronTrigger) triggerBuilder.build(); // 调度容器设置JobDetail和Trigger sched.scheduleJob(jobDetail, trigger); // 启动 if (!sched.isShutdown()) &#123; sched.start(); &#125; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * @Description: 修改一个任务的触发时间 * * @param jobName * @param jobGroupName * @param triggerName * 触发器名 * @param triggerGroupName * 触发器组名 * @param cron * 时间设置，参考quartz说明文档 */ public static void modifyJobTime(String jobName, String jobGroupName, String triggerName, String triggerGroupName, String cron) &#123; try &#123; Scheduler sched = schedulerFactory.getScheduler(); TriggerKey triggerKey = TriggerKey.triggerKey(triggerName, triggerGroupName); CronTrigger trigger = (CronTrigger) sched.getTrigger(triggerKey); if (trigger == null) &#123; return; &#125; String oldTime = trigger.getCronExpression(); if (!oldTime.equalsIgnoreCase(cron)) &#123; /** 方式一 ：调用 rescheduleJob 开始 */ // 触发器 TriggerBuilder&lt;Trigger&gt; triggerBuilder = TriggerBuilder.newTrigger(); // 触发器名,触发器组 triggerBuilder.withIdentity(triggerName, triggerGroupName); triggerBuilder.startNow(); // 触发器时间设定 triggerBuilder.withSchedule(CronScheduleBuilder.cronSchedule(cron)); // 创建Trigger对象 trigger = (CronTrigger) triggerBuilder.build(); // 方式一 ：修改一个任务的触发时间 sched.rescheduleJob(triggerKey, trigger); /** 方式一 ：调用 rescheduleJob 结束 */ /** 方式二：先删除，然后在创建一个新的Job */ // JobDetail jobDetail = // sched.getJobDetail(JobKey.jobKey(jobName, jobGroupName)); // Class&lt;? extends Job&gt; jobClass = jobDetail.getJobClass(); // removeJob(jobName, jobGroupName, triggerName, // triggerGroupName); // addJob(jobName, jobGroupName, triggerName, triggerGroupName, // jobClass, cron); /** 方式二 ：先删除，然后在创建一个新的Job */ &#125; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * @Description: 移除一个任务 * * @param jobName * @param jobGroupName * @param triggerName * @param triggerGroupName */ public static void removeJob(String jobName, String jobGroupName, String triggerName, String triggerGroupName) &#123; try &#123; Scheduler sched = schedulerFactory.getScheduler(); TriggerKey triggerKey = TriggerKey.triggerKey(triggerName, triggerGroupName); sched.pauseTrigger(triggerKey);// 停止触发器 sched.unscheduleJob(triggerKey);// 移除触发器 sched.deleteJob(JobKey.jobKey(jobName, jobGroupName));// 删除任务 &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * @Description:启动所有定时任务 */ public static void startJobs() &#123; try &#123; Scheduler sched = schedulerFactory.getScheduler(); sched.start(); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * @Description:关闭所有定时任务 */ public static void shutdownJobs() &#123; try &#123; Scheduler sched = schedulerFactory.getScheduler(); if (!sched.isShutdown()) &#123; sched.shutdown(); &#125; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 任务执行业务这里做一个简单的演示，只实现Job接口打印当前时间。 1234567public class MyJob implements Job&#123; public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException &#123; DateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); System.out.println(sdf.format(DateTime.now().toDate())); &#125;&#125; 测试动态定时任务新建QuartzTest.Java 测试类 123456789101112131415161718192021222324public class QuartzTest &#123; public static String JOB_NAME = \"动态任务调度\"; public static String TRIGGER_NAME = \"动态任务触发器\"; public static String JOB_GROUP_NAME = \"XLXXCC_JOB_GROUP\"; public static String TRIGGER_GROUP_NAME = \"XLXXCC_JOB_GROUP\"; public static void main(String[] args) &#123; try &#123; System.out.println(\"【系统启动】开始(每1秒输出一次)...\"); QuartzManager.addJob(JOB_NAME, JOB_GROUP_NAME, TRIGGER_NAME, TRIGGER_GROUP_NAME, MyJob.class,\"0/1 * * * * ?\"); Thread.sleep(5000); System.out.println(\"【修改时间】开始(每5秒输出一次)...\"); QuartzManager.modifyJobTime(JOB_NAME, JOB_GROUP_NAME, TRIGGER_NAME, TRIGGER_GROUP_NAME, \"0/5 * * * * ?\"); Thread.sleep(15000); System.out.println(\"【移除定时】开始...\"); QuartzManager.removeJob(JOB_NAME, JOB_GROUP_NAME, TRIGGER_NAME, TRIGGER_GROUP_NAME); System.out.println(\"【移除定时】成功\"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出如下： 总结通过以上测试可以明显的看出两者的优劣，Quartz足够灵活强大，单Spring scheduled 在简单任务下也是一个不错的选择。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"spring","slug":"spring","permalink":"https://sunny0715.github.io/tags/spring/"}]},{"title":"大话数据库连接池","slug":"大话数据库连接池","date":"2018-02-06T09:50:49.000Z","updated":"2018-03-20T06:22:26.399Z","comments":true,"path":"2018/02/06/大话数据库连接池/","link":"","permalink":"https://sunny0715.github.io/2018/02/06/大话数据库连接池/","excerpt":"引言数据库连接池在Java数据库相关中间件产品群中，应该算是最底层最基础的一类产品，作为企业应用开发必不可少的组件，无数开发者们贡献了一个又一个的优秀产品，它们有的随着时代发展，功成身退，有的还在不断迭代，老而弥坚，更有新生代产品，或性能无敌、或功能全面。接下来就聊一聊 “那些年，我们用过的数据库连接池。”","text":"引言数据库连接池在Java数据库相关中间件产品群中，应该算是最底层最基础的一类产品，作为企业应用开发必不可少的组件，无数开发者们贡献了一个又一个的优秀产品，它们有的随着时代发展，功成身退，有的还在不断迭代，老而弥坚，更有新生代产品，或性能无敌、或功能全面。接下来就聊一聊 “那些年，我们用过的数据库连接池。” 第一、二代连接池区分一个数据库连接池是属于第一代产品还是代二代产品有一个最重要的特征就是看它在架构和设计时采用的线程模型，因为这直接影响的是并发环境下存取数据库连接的性能。 一般来讲采用单线程同步的架构设计都属于第一代连接池，二采用多线程异步架构的则属于第二代。比较有代表性的就是Apache Commons DBCP，在1.x版本中，一直延续着单线程设计模式，到2.x才采用多线程模型。 用版本发布时间来辨别区分两代产品，则一个偷懒的好方法。以下是这些常见数据库连接池最新版本的发布时间： 数据库连接池 最新版本 发布时间 C3P0 c3p0-0.9.5.2 on 9 Dec 2015 DBCP 2.2.0 27 December 2017 Druid 0.11.0 Dec 4 2017 HikariCP 2.7.6 2018-01-14 从表中可以看出，C3P0已经很久没有更新了。DBCP更新速度很慢，基本处于不活跃状态，而Druid和HikariCP处于活跃状态的更新中，这就是我们说的二代产品了。 二代产品对一代产品的超越是颠覆性的，除了一些“历史原因”，你很难再找到第二条理由说服自己不选择二代产品，但任何成功都不是偶然的，二代产品的成功很大程度上得益于前代产品们打下的基础，站在巨人的肩膀上，新一代的连接池的设计师们将这一项“工具化”的产品，推向了极致。其中，最具代表性的两款产品是： HikariCP Druid 彻底死掉的C3P0C3P0是我使用的第一款数据库连接池，在很长一段时间内，它一直是Java领域内数据库连接池的代名词，当年盛极一时的Hibernate都将其作为内置的数据库连接池，可以业内对它的稳定性还是认可的。C3P0功能简单易用，稳定性好这是它的优点，但是性能上的缺点却让它彻底被打入冷宫。C3P0的性能很差，差到即便是同时代的产品相比它也是垫底的，更不用和Druid、HikariCP等相比了。正常来讲，有问题很正常，改就是了，但c3p0最致命的问题就是架构设计过于复杂，让重构变成了一项不可能完成的任务。随着国内互联网大潮的涌起，性能有硬伤的c3p0彻底的退出了历史舞台。 咸鱼翻身的DBCPDBCP（DataBase Connection Pool）属于Apache顶级项目Commons中的核心子项目（最早在Jakarta Commons里就有）,在Apache的生态圈中的影响里十分广泛，比如最为大家所熟知的Tomcat就在内部集成了DBCP，实现JPA规范的OpenJPA，也是默认集成DBCP的。但DBCP并不是独立实现连接池功能的，它内部依赖于Commons中的另一个子项目Pool，连接池最核心的“池”，就是由Pool组件提供的，因此，DBCP的性能实际上就是Pool的性能，DBCP和Pool的依赖关系如下表： Apache Commons DBCP Apache Commons Pool v1.2.2 v1.3 v1.3 v1.5.4 v1.4 v1.5.4 v2.0.x v2.2 v2.1.x v2.4.2 v2.2.x v2.5.0 可以看到，因为核心功能依赖于Pool，所以DBCP本身只能做小版本的更新，真正大版本的更迭则完全依托于pool。有很长一段时间，pool都还是停留在1.x版本，这直接导致DBCP也更新乏力。很多依赖DBCP的应用在遇到性能瓶颈之后，别无选择，只能将其替换掉，DBCP忠实的拥趸tomcat就在其tomcat 7.0版本中，自己重新设计开发出了一套连接池（Tomcat JDBC Pool）。好在，在2013年事情终于迎来转机，13年9月Commons-Pool 2.0版本发布，14年2月份，DBCP也终于迎来了自己的2.0版本，基于新的线程模型全新设计的“池”让DBCP重焕青春，虽然和新一代的连接池相比仍有一定差距，但差距并不大，DBCP2.x版本已经稳稳达到了和新一代产品同级别的性能指标（见下图）。 DBCP终于靠Pool咸鱼翻身，打了一个漂亮的翻身仗，但长时间的等待已经完全消磨了用户的耐心，与新一代的产品项目相比，DBCP没有任何优势，试问，谁会在有选择的前提下，去选择那个并不优秀的呢？也许，现在还选择DBCP2的唯一理由，就是情怀吧。 性能无敌的HikariCPHikariCP号称“性能杀手”（It’s Faster），它的表现究竟如何呢，先来看下官网提供的数据： 不光性能强劲，稳定性也不差： 那它是怎么做到如此强劲的呢？官网给出的说明如下： 字节码精简：优化代码，直到编译后的字节码最少，这样，CPU缓存可以加载更多的程序代码； 优化代理和拦截器：减少代码，例如HikariCP的Statement proxy只有100行代码； 自定义数组类型（FastStatementList）代替ArrayList：避免每次get()调用都要进行range check，避免调用remove()时的从头到尾的扫描； 自定义集合类型（ConcurrentBag）：提高并发读写的效率； 其他缺陷的优化，比如对于耗时超过一个CPU时间片的方法调用的研究（但没说具体怎么优化）。 可以看到，上述这几点优化，和现在能找到的资料来看，HakariCP在性能上的优势应该是得到共识的，再加上它自身小巧的身形，在当前的“云时代、微服务”的背景下，HakariCP一定会得到更多人的青睐。 功能全面的Druid近几年，阿里在开源项目上动作频频，除了有像fastJson、dubbo这类项目，更有像AliSQL这类的大型软件，今天说的Druid，就是阿里众多优秀开源项目中的一个。它除了提供性能卓越的连接池功能外，还集成了SQL监控，黑名单拦截等功能，用它自己的话说，Druid是“为监控而生”。借助于阿里这个平台的号召力，产品一经发布就赢得了大批用户的拥趸，从用户使用的反馈来看，Druid也确实没让用户失望。 相较于其他产品，Druid另一个比较大的优势，就是中文文档比较全面（毕竟是国人的项目么），在github的wiki页面，列举了日常使用中可能遇到的问题，对一个新用户来讲，上面提供的内容已经足够指导它完成产品的配置和使用了。 下图为Druid自己提供的性能测试数据： 现在项目开发中，我还是比较倾向于使用Durid，它不仅仅是一个数据库连接池，它还包含一个ProxyDriver，一系列内置的JDBC组件库，一个SQL Parser。 Druid 相对于其他数据库连接池的优点 强大的监控特性，通过Druid提供的监控功能，可以清楚知道连接池和SQL的工作情况。 a. 监控SQL的执行时间、ResultSet持有时间、返回行数、更新行数、错误次数、错误堆栈信息； b. SQL执行的耗时区间分布。什么是耗时区间分布呢？比如说，某个SQL执行了1000次，其中0~1毫秒区间50次，1~10毫秒800次，10~100毫秒100次，100~1000毫秒30次，1~10秒15次，10秒以上5次。通过耗时区间分布，能够非常清楚知道SQL的执行耗时情况； c. 监控连接池的物理连接创建和销毁次数、逻辑连接的申请和关闭次数、非空等待次数、PSCache命中率等。 方便扩展。Druid提供了Filter-Chain模式的扩展API，可以自己编写Filter拦截JDBC中的任何方法，可以在上面做任何事情，比如说性能监控、SQL审计、用户名密码加密、日志等等。 Druid集合了开源和商业数据库连接池的优秀特性，并结合阿里巴巴大规模苛刻生产环境的使用经验进行优化。 总结时至今日，虽然每个应用（需要RDBMS的）都离不开连接池，但在实际使用的时候，连接池已经可以做到“隐形”了。也就是说在通常情况下，连接池完成项目初始化配置之后，就再不需要再做任何改动了。不论你是选择Druid或是HikariCP，甚至是DBCP，它们都足够稳定且高效！之前讨论了很多关于连接池的性能的问题，但这些性能上的差异，是相较于其他连接池而言的，对整个系统应用来说，第二代连接池在使用过程中体会到的差别是微乎其微的，基本上不存在因为连接池的自身的配饰和使用导致系统性能下降的情况，除非是在单点应用的数据库负载足够高的时候（压力测试的时候），但即便是如此，通用的优化的方式也是单点改集群，而不是在单点的连接池上死扣。 参考 数据库连接池性能比对 大话数据库连接池 c3p0,Druid,Tomcat Jdbc Pool,dbcp2,proxool数据源性能比较","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"tool","slug":"tool","permalink":"https://sunny0715.github.io/tags/tool/"}]},{"title":"Nginx配置文件详解","slug":"Nginx配置文件详解","date":"2018-01-24T06:12:52.000Z","updated":"2018-03-13T05:48:20.900Z","comments":true,"path":"2018/01/24/Nginx配置文件详解/","link":"","permalink":"https://sunny0715.github.io/2018/01/24/Nginx配置文件详解/","excerpt":"引言之前介绍了Linux下安装Nginx，Nginx 专为性能优化而开发，性能是其最重要的考量,实现上非常注重效率 。它支持内核 Poll 模型，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。","text":"引言之前介绍了Linux下安装Nginx，Nginx 专为性能优化而开发，性能是其最重要的考量,实现上非常注重效率 。它支持内核 Poll 模型，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。 Nginx特点Nginx 具有很高的稳定性。其它 HTTP 服务器，当遇到访问的峰值，或者有人恶意发起慢速连接时，也很可能会导致服务器物理内存耗尽频繁交换，失去响应，只能重启服务器。例如当前 apache 一旦上到 200 个以上进程，web响应速度就明显非常缓慢了。而 Nginx 采取了分阶段资源分配技术，使得它的 CPU 与内存占用率非常低。Nginx 官方表示保持 10,000 个没有活动的连接，它只占 2.5M 内存，所以类似 DOS 这样的攻击对 Nginx 来说基本上是毫无用处的。就稳定性而言,Nginx 比 lighthttpd 更胜一筹。 Nginx 支持热部署。它的启动特别容易, 并且几乎可以做到 7*24 不间断运行，即使运行数个月也不需要重新启动。你还能够在不间断服务的情况下，对软件版本进行进行升级。 Nginx的用处说了这么多Nginx的优点，Nginx在开发中最常用作反向代理服务器，但是Nginx的用处可不止这一点。 Nginx配置虚拟主机虚拟主机是一种特殊的软硬件技术，它可以将网络上的每一台计算机分成多个虚拟主机，每个虚拟主机可以独立对外提供www服务，这样就可以实现一台主机对外提供多个web服务，每个虚拟主机之间是独立的，互不影响的。 1、 基于ip的虚拟主机 2、基于端口的虚拟主机 3、基于域名的虚拟主机 Nginx反向代理通常的代理服务器，只用于代理内部网络对Internet的连接请求，客户机必须指定代理服务器,并将本来要直接发送到Web服务器上的http请求发送到代理服务器中由代理服务器向Internet上的web服务器发起请求，最终达到客户机上网的目的。 ​ 而反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 Nginx配置详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265#用户user nginx ;#工作进程，根据硬件调整，大于等于cpu核数worker_processes 8;#错误日志error_log logs/nginx_error.log crit;#pid放置的位置pid logs/nginx.pid;#指定进程可以打开的最大描述符worker_rlimit_nofile 204800;这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。events&#123; #使用epoll的I/O 模型 use epoll; 补充说明: 与apache相类，nginx针对不同的操作系统，有不同的事件模型 A）标准事件模型 Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll B）高效事件模型 Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 Epoll:使用于Linux内核2.6版本及以后的系统。 /dev/poll：使用于Solaris 7 11/99+, HP/UX 11.22+ (eventport), IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 Eventport：使用于Solaris 10. 为了防止出现内核崩溃的问题， 有必要安装安全补丁 #工作进程的最大连接数量，根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行 worker_connections 204800; 每个进程允许的最多连接数， 理论上每台nginx服务器的最大连接数为worker_processes*worker_connections #keepalive超时时间。 keepalive_timeout 60; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 open_file_cache_min_uses 1;&#125;#设定http服务器，利用它的反向代理功能提供负载均衡支持http&#123; #设定mime类型,类型由mime.type文件定义 include mime.types; default_type application/octet-stream; log_format main '$host $status [$time_local] $remote_addr [$time_local] $request_uri ''\"$http_referer\" \"$http_user_agent\" \"$http_x_forwarded_for\" ''$bytes_sent $request_time $sent_http_x_cache_hit';log_format log404 '$status [$time_local] $remote_addr $host$request_uri $sent_http_location';$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址；$remote_user：用来记录客户端用户名称；$time_local： 用来记录访问时间与时区；$request： 用来记录请求的url与http协议；$status： 用来记录请求状态；成功是200，$body_bytes_s ent ：记录发送给客户端文件主体内容大小；$http_referer：用来记录从那个页面链接访问过来的；$http_user_agent：记录客户毒啊浏览器的相关信息；通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址； #用了log_format指令设置了日志格式之后，需要用access_log指令指定日志文件的存放路径； # access_log /usr/local/nginx/logs/access_log main; access_log /dev/null; #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小，这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 128k; #客户请求头缓冲大小 n#ginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取如果设置过小HTTP头/Cookie过大 会报400 错误nginx 400 bad request求行如果超过buffer，就会报HTTP 414错误(URI Too Long)nginx接受最长的HTTP头部大小必须比其中一个buffer大，否则就会报400的 large_client_header_buffers 8 128k;HTTP错误(Bad Request)。#使用字段:http, server, location 这个指令指定缓存是否启用,如果启用,将记录文件以下信息: ·打开的文件描述符,大小信息和修改时间. ·存在的目录信息. ·在搜索文件过程中的错误信息 --没有这个文件,无法正确读取,参考open_file_cache_errors指令选项:·max -指定缓存的最大数目,如果缓存溢出,最长使用过的文件(LRU)将被移除#例: open_file_cache max=1000 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 2; open_file_cache_errors on; open_file_cache max 102400 #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件是记录cache错误. open_file_cache_errors #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如 果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid #设定通过nginx上传文件的大小 client_max_body_size 300m; #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，#对于普通应用，必须设为on。#如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #后端服务器连接的超时时间_发起握手等候响应超时时间 proxy_connect_timeout 90; #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_read_timeout 180; #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 proxy_send_timeout 180; #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 proxy_buffer_size 256k; #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_buffers 8 256k; proxy_busy_buffers_size 256k; #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 proxy_temp_file_write_size 256k; #proxy_temp_path和proxy_cache_path指定的路径必须在同一分区 proxy_temp_path /data0/proxy_temp_dir; #设置内存缓存空间大小为200MB，1天没有被访问的内容自动清除，硬盘缓存空间大小为30GB。 proxy_cache_path /data0/proxy_cache_dir levels=1:2 keys_zone=cache_one:200m inactive=1d max_size=30g; client_header_timeout 5; client_body_timeout 5; send_timeout 5; #keepalive超时时间。 keepalive_timeout 120; #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。#无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 client_body_buffer_size 512k; #表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_intercept_errors on; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; upstream img_relay &#123; server 127.0.0.1:8027; server 127.0.0.1:8028; server 127.0.0.1:8029; hash $request_uri; &#125; #如果请求为img_relay:80,则交给名称为img_relay的Nginx集群来处理 server&#123; listen 80; server_name img_relay;# limit_req zone=req_one burst=5 nodelay; location ~ .*.jsp$ &#123; proxy_ignore_client_abort on; proxy_pass http://img_relay; #http:// + upstream名称 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_connect_timeout 60; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_read_timeout 600; #连接成功后，后端服务器响应时间(代理接收超时) proxy_send_timeout 60; #后端服务器数据回传时间(代理发送超时) proxy_buffer_size 128k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 32 128k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_busy_buffers_size 128k; #高负荷下缓冲大小（proxy_buffers*2） &#125; &#125;nginx的upstream目前支持4种方式的分配1、轮询（默认）每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 2、weight指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。例如：upstream bakend &#123;server 192.168.0.14 weight=10;server 192.168.0.15 weight=10;&#125;3、ip_hash每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。例如：upstream bakend &#123;ip_hash;server 192.168.0.14:88;server 192.168.0.15:80;&#125;4、fair（第三方）按后端服务器的响应时间来分配请求，响应时间短的优先分配。upstream backend &#123;server server1;server server2;fair;&#125;5、url_hash（第三方）按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法upstream backend &#123; server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32;&#125;tips:upstream bakend&#123;#定义负载均衡设备的Ip及设备状态 ip_hash; server 127.0.0.1:9090 down; server 127.0.0.1:8080 weight=2; server 127.0.0.1:6060; server 127.0.0.1:7070 backup;&#125;在需要使用负载均衡的server中增加proxy_pass http://bakend/;每个设备的状态设置为:1.down表示单前的server暂时不参与负载2.weight默认为1.weight越大，负载的权重就越大。3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误4.fail_timeout:max_fails次失败后，暂停的时间。5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。nginx支持同时设置多组的负载均衡，用来给不用的server来使用。client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debugclient_body_temp_path设置记录文件的目录 可以设置最多3层目录location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://sunny0715.github.io/tags/nginx/"}]},{"title":"HttpClient后台跨域","slug":"HttpClient后台跨域","date":"2018-01-20T01:48:24.000Z","updated":"2018-03-13T05:56:22.588Z","comments":true,"path":"2018/01/20/HttpClient后台跨域/","link":"","permalink":"https://sunny0715.github.io/2018/01/20/HttpClient后台跨域/","excerpt":"引言跨域可以说是一个经常遇到的问题，最近在联调一个身份证识别接口，该接口由python语言编写，Java语言调用，刚开始采用了CORS（Cross-Origin Resource Sharing）跨域，在IE8上一直出现兼容性问题，固定的思维容易出现错误，自己一直想着前端Ajax跨域而忽略了后台HttpClient的跨域，最后还是用HttpClient顺利解决问题，避免了浏览器跨域带来的兼容性问题。","text":"引言跨域可以说是一个经常遇到的问题，最近在联调一个身份证识别接口，该接口由python语言编写，Java语言调用，刚开始采用了CORS（Cross-Origin Resource Sharing）跨域，在IE8上一直出现兼容性问题，固定的思维容易出现错误，自己一直想着前端Ajax跨域而忽略了后台HttpClient的跨域，最后还是用HttpClient顺利解决问题，避免了浏览器跨域带来的兼容性问题。 HttpClient VS Jsonp之前的博客有说过Jsonp的跨域方式，jsonp的核心则是动态添加&lt;script&gt;标签来调用服务器提供的js脚本。相比于HttpClient，Jsonp有两个很大的缺点： 1、它只能发送get请求，如果发送post请求会造成无法解析获取不到数据的问题。 2、如果返回的数据没有经过配置相应的编码文件来处理，拿到的数据可能会是一堆乱码。 问题总是能解决，HttpClient则没那么多约束，HttpClient封装了http协议的jar包，基本的请求方法get、post、put、 delete都能实现，当然得在web.xml文件中配置相应的filter拦截器拦截请求后再设好编码，一般返回的参数都是Json字符串，而我们只需要导入Jackson或者fastJson或者别的jar包来解析这对象把他转换成你所需要的数据即可。 整合Spring添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt;&lt;/dependency&gt; 封装方法新建HttpClientUtil工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114public class HttpClientUtil &#123; public static String doGet(String url, Map&lt;String, String&gt; param) &#123; // 创建Httpclient对象 CloseableHttpClient httpclient = HttpClients.createDefault(); String resultString = \"\"; CloseableHttpResponse response = null; try &#123; // 创建uri URIBuilder builder = new URIBuilder(url); if (param != null) &#123; for (String key : param.keySet()) &#123; builder.addParameter(key, param.get(key)); &#125; &#125; URI uri = builder.build(); // 创建http GET请求 HttpGet httpGet = new HttpGet(uri); // 执行请求 response = httpclient.execute(httpGet); // 判断返回状态是否为200 if (response.getStatusLine().getStatusCode() == 200) &#123; resultString = EntityUtils.toString(response.getEntity(), \"UTF-8\"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (response != null) &#123; response.close(); &#125; httpclient.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return resultString; &#125; public static String doGet(String url) &#123; return doGet(url, null); &#125; public static String doPost(String url, Map&lt;String, String&gt; param) &#123; // 创建Httpclient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); CloseableHttpResponse response = null; String resultString = \"\"; try &#123; // 创建Http Post请求 HttpPost httpPost = new HttpPost(url); // 创建参数列表 if (param != null) &#123; List&lt;NameValuePair&gt; paramList = new ArrayList&lt;&gt;(); for (String key : param.keySet()) &#123; paramList.add(new BasicNameValuePair(key, param.get(key))); &#125; // 模拟表单 UrlEncodedFormEntity entity = new UrlEncodedFormEntity(paramList); httpPost.setEntity(entity); &#125; // 执行http请求 response = httpClient.execute(httpPost); resultString = EntityUtils.toString(response.getEntity(), \"utf-8\"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; response.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; return resultString; &#125; public static String doPost(String url) &#123; return doPost(url, null); &#125; public static String doPostJson(String url, String json) &#123; // 创建Httpclient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); CloseableHttpResponse response = null; String resultString = \"\"; try &#123; // 创建Http Post请求 HttpPost httpPost = new HttpPost(url); // 创建请求内容 StringEntity entity = new StringEntity(json, ContentType.APPLICATION_JSON); httpPost.setEntity(entity); // 执行http请求 response = httpClient.execute(httpPost); resultString = EntityUtils.toString(response.getEntity(), \"utf-8\"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; response.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; return resultString; &#125;&#125; 单元测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class HttpClientTest &#123; @Test public void doGet() throws Exception &#123; //创建一个httpclient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //创建一个GET对象 HttpGet get = new HttpGet(\"http://www.sogou.com\"); //执行请求 CloseableHttpResponse response = httpClient.execute(get); //取响应的结果 int statusCode = response.getStatusLine().getStatusCode(); System.out.println(statusCode); HttpEntity entity = response.getEntity(); String string = EntityUtils.toString(entity, \"utf-8\"); System.out.println(string); //关闭httpclient response.close(); httpClient.close(); &#125; @Test public void doGetWithParam() throws Exception&#123; //创建一个httpclient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //创建一个uri对象 URIBuilder uriBuilder = new URIBuilder(\"http://www.sogou.com/web\"); uriBuilder.addParameter(\"query\", \"花千骨\"); HttpGet get = new HttpGet(uriBuilder.build()); //执行请求 CloseableHttpResponse response = httpClient.execute(get); //取响应的结果 int statusCode = response.getStatusLine().getStatusCode(); System.out.println(statusCode); HttpEntity entity = response.getEntity(); String string = EntityUtils.toString(entity, \"utf-8\"); System.out.println(string); //关闭httpclient response.close(); httpClient.close(); &#125; @Test public void doPost() throws Exception &#123; CloseableHttpClient httpClient = HttpClients.createDefault(); //创建一个post对象 HttpPost post = new HttpPost(\"http://localhost:8082/httpclient/post.action\"); //执行post请求 CloseableHttpResponse response = httpClient.execute(post); String string = EntityUtils.toString(response.getEntity()); System.out.println(string); response.close(); httpClient.close(); &#125; @Test public void doPostWithParam() throws Exception&#123; CloseableHttpClient httpClient = HttpClients.createDefault(); //创建一个post对象 HttpPost post = new HttpPost(\"http://localhost:8082/httpclient/post.action\"); //创建一个Entity。模拟一个表单 List&lt;NameValuePair&gt; kvList = new ArrayList&lt;&gt;(); kvList.add(new BasicNameValuePair(\"username\", \"张三\")); kvList.add(new BasicNameValuePair(\"password\", \"123\")); //包装成一个Entity对象 StringEntity entity = new UrlEncodedFormEntity(kvList, \"utf-8\"); //设置请求的内容 post.setEntity(entity); //执行post请求 CloseableHttpResponse response = httpClient.execute(post); String string = EntityUtils.toString(response.getEntity()); System.out.println(string); response.close(); httpClient.close(); &#125;&#125; 项目实例123456789101112131415161718192021222324252627/** * 订单处理Service * &lt;p&gt;Title: OrderServiceImpl&lt;/p&gt; * &lt;p&gt;Description: &lt;/p&gt; * @version 1.0 */@Servicepublic class OrderServiceImpl implements OrderService &#123; @Value(\"$&#123;ORDER_BASE_URL&#125;\") private String ORDER_BASE_URL; @Value(\"$&#123;ORDER_CREATE_URL&#125;\") private String ORDER_CREATE_URL; @Override public String createOrder(Order order) &#123; //调用order的服务提交订单。 String json = HttpClientUtil.doPostJson(ORDER_BASE_URL + ORDER_CREATE_URL, JsonUtils.objectToJson(order)); //把json转换成taotaoResult TaotaoResult taotaoResult = TaotaoResult.format(json); if (taotaoResult.getStatus() == 200) &#123; Object orderId = taotaoResult.getData(); return orderId.toString(); &#125; return \"\"; &#125;&#125; 总结HttpClient与Jsonp能够轻易的解决跨域问题，从而得到自己想要的数据(来自不同IP，协议，端口)，唯一的不同点是，HttpClient是在后台Java代码中进行跨域访问，而Jsonp是在前台js中进行跨域访问。跨域还有一级跨域，二级跨域，更多内容值得研究。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"}]},{"title":"说说JSON和JSONP","slug":"说说JSON和JSONP","date":"2018-01-16T06:41:09.000Z","updated":"2018-03-13T05:54:27.615Z","comments":true,"path":"2018/01/16/说说JSON和JSONP/","link":"","permalink":"https://sunny0715.github.io/2018/01/16/说说JSON和JSONP/","excerpt":"前言说到AJAX就会不可避免的面临两个问题，第一个是AJAX以何种格式来交换数据？第二个是跨域的需求如何解决？这两个问题目前都有不同的解决方案，比如数据可以用自定义字符串或者用XML来描述，跨域可以通过服务器端代理来解决。 JSON和JSONP虽然只有一个字母的差别，但其实他们根本不是一回事：JSON是一种数据交换格式，而JSONP是一种依靠开发人员的聪明才智创造出的一种非官方跨域数据交互协议。我们拿最近比较火的谍战片来打个比方，JSON是地下党们用来书写和交换情报的“暗号”，而JSONP则是把用暗号书写的情报传递给自己同志时使用的接头方式。看到没？一个是描述信息的格式，一个是信息传递双方约定的方法。","text":"前言说到AJAX就会不可避免的面临两个问题，第一个是AJAX以何种格式来交换数据？第二个是跨域的需求如何解决？这两个问题目前都有不同的解决方案，比如数据可以用自定义字符串或者用XML来描述，跨域可以通过服务器端代理来解决。 JSON和JSONP虽然只有一个字母的差别，但其实他们根本不是一回事：JSON是一种数据交换格式，而JSONP是一种依靠开发人员的聪明才智创造出的一种非官方跨域数据交互协议。我们拿最近比较火的谍战片来打个比方，JSON是地下党们用来书写和交换情报的“暗号”，而JSONP则是把用暗号书写的情报传递给自己同志时使用的接头方式。看到没？一个是描述信息的格式，一个是信息传递双方约定的方法。 什么是JSON前面简单说了一下，JSON是一种基于文本的数据交换方式，或者叫做数据描述格式，你是否该选用他首先肯定要关注它所拥有的优点。 JSON的优点 基于纯文本，跨平台传递极其简单； JavaScript原生支持，后台语言几乎全部支持； 轻量级数据格式，占用字符数量极少，特别适合互联网传递； 可读性较强，虽然比不上XML那么一目了然，但在合理的依次缩进之后还是很容易识别的； 容易编写和解析，当然前提是你要知道数据结构； JSON的缺点当然也有，但在作者看来实在是无关紧要的东西，所以不再单独说明。 JSON的格式或者叫规则JSON能够以非常简单的方式来描述数据结构，XML能做的它都能做，因此在跨平台方面两者完全不分伯仲。 JSON只有两种数据类型描述符，大括号{}和方括号[]，其余英文冒号:是映射符，英文逗号,是分隔符，英文双引号””是定义符。 大括号{}用来描述一组“不同类型的无序键值对集合”（每个键值对可以理解为OOP的属性描述），方括号[]用来描述一组“相同类型的有序数据集合”（可对应OOP的数组）。 上述两种集合中若有多个子项，则通过英文逗号,进行分隔。 键值对以英文冒号:进行分隔，并且建议键名都加上英文双引号””，以便于不同语言的解析。 JSON内部常用数据类型无非就是字符串、数字、布尔、日期、null 这么几个，字符串必须用双引号引起来，其余的都不用，日期类型比较特殊，这里就不展开讲述了，只是建议如果客户端没有按日期排序功能需求的话，那么把日期时间直接作为字符串传递就好，可以省去很多麻烦。 JSON的实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// 描述一个人var person = &#123; \"Name\": \"Bob\", \"Age\": 32, \"Company\": \"IBM\", \"Engineer\": true&#125;// 获取这个人的信息var personAge = person.Age;// 描述几个人var members = [ &#123; \"Name\": \"Bob\", \"Age\": 32, \"Company\": \"IBM\", \"Engineer\": true &#125;, &#123; \"Name\": \"John\", \"Age\": 20, \"Company\": \"Oracle\", \"Engineer\": false &#125;, &#123; \"Name\": \"Henry\", \"Age\": 45, \"Company\": \"Microsoft\", \"Engineer\": false &#125;]// 读取其中John的公司名称var johnsCompany = members[1].Company;// 描述一次会议var conference = &#123; \"Conference\": \"Future Marketing\", \"Date\": \"2012-6-1\", \"Address\": \"Beijing\", \"Members\": [ &#123; \"Name\": \"Bob\", \"Age\": 32, \"Company\": \"IBM\", \"Engineer\": true &#125;, &#123; \"Name\": \"John\", \"Age\": 20, \"Company\": \"Oracle\", \"Engineer\": false &#125;, &#123; \"Name\": \"Henry\", \"Age\": 45, \"Company\": \"Microsoft\", \"Engineer\": false &#125; ]&#125;// 读取参会者Henry是否工程师var henryIsAnEngineer = conference.Members[2].Engineer; 关于JSON，就说这么多，更多细节请在开发过程中查阅资料深入学习。 什么是JSONPJSONP的产生其实网上关于JSONP的讲解有很多，但却千篇一律，而且云里雾里，对于很多刚接触的人来讲理解起来有些困难，试着用自己的方式来阐释一下这个问题。 1、一个众所周知的问题，Ajax直接请求普通文件存在跨域无权限访问的问题，甭管你是静态页面、动态网页、web服务、WCF，只要是跨域请求，一律不准； 2、不过我们又发现，Web页面上调用js文件时则不受是否跨域的影响（不仅如此，我们还发现凡是拥有”src”这个属性的标签都拥有跨域的能力，比如&lt;script&gt;、&lt;img&gt;、&lt;iframe&gt;）； 3、于是可以判断，当前阶段如果想通过纯web端（ActiveX控件、服务端代理、属于未来的HTML5之Websocket等方式不算）跨域访问数据就只有一种可能，那就是在远程服务器上设法把数据装进js格式的文件里，供客户端调用和进一步处理； 4、恰巧我们已经知道有一种叫做JSON的纯字符数据格式可以简洁的描述复杂数据，更妙的是JSON还被js原生支持，所以在客户端几乎可以随心所欲的处理这种格式的数据； 5、这样子解决方案就呼之欲出了，web客户端通过与调用脚本一模一样的方式，来调用跨域服务器上动态生成的js格式文件（一般以JSON为后缀），显而易见，服务器之所以要动态生成JSON文件，目的就在于把客户端需要的数据装入进去。 6、客户端在对JSON文件调用成功之后，也就获得了自己所需的数据，剩下的就是按照自己需求进行处理和展现了，这种获取远程数据的方式看起来非常像AJAX，但其实并不一样。 7、为了便于客户端使用数据，逐渐形成了一种非正式传输协议，人们把它称作JSONP，该协议的一个要点就是允许用户传递一个callback参数给服务端，然后服务端返回数据时会将这个callback参数作为函数名来包裹住JSON数据，这样客户端就可以随意定制自己的函数来自动处理返回数据了。 JSONP的客户端具体实现不管jQuery也好，extjs也罢，又或者是其他支持jsonp的框架，他们幕后所做的工作都是一样的，下面我来循序渐进的说明一下jsonp在客户端的实现： 1、我们知道，哪怕跨域js文件中的代码（当然指符合web脚本安全策略的），web页面也是可以无条件执行的。 远程服务器remoteserver.com根目录下有个remote.js文件代码如下： 1alert('我是远程文件'); 本地服务器localserver.com下有个jsonp.html页面代码如下： 123456789&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;script type=\"text/javascript\" src=\"http://remoteserver.com/remote.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 毫无疑问，页面将会弹出一个提示窗体，显示跨域调用成功。 2、现在我们在jsonp.html页面定义一个函数，然后在远程remote.js中传入数据进行调用。 jsonp.html页面代码如下： 1234567891011121314&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;script type=\"text/javascript\"&gt; var localHandler = function(data)&#123; alert('我是本地函数，可以被跨域的remote.js文件调用，远程js带来的数据是：' + data.result); &#125;; &lt;/script&gt; &lt;script type=\"text/javascript\" src=\"http://remoteserver.com/remote.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; remote.js文件代码如下： 1localHandler(&#123;\"result\":\"我是远程js带来的数据\"&#125;); 运行之后查看结果，页面成功弹出提示窗口，显示本地函数被跨域的远程js调用成功，并且还接收到了远程js带来的数据。很欣喜，跨域远程获取数据的目的基本实现了，但是又一个问题出现了，我怎么让远程js知道它应该调用的本地函数叫什么名字呢？毕竟是jsonp的服务者都要面对很多服务对象，而这些服务对象各自的本地函数都不相同啊？我们接着往下看。 3、聪明的开发者很容易想到，只要服务端提供的js脚本是动态生成的就行了呗，这样调用者可以传一个参数过去告诉服务端“我想要一段调用XXX函数的js代码，请你返回给我”，于是服务器就可以按照客户端的需求来生成js脚本并响应了。 看jsonp.html页面的代码： 123456789101112131415161718192021&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;script type=\"text/javascript\"&gt; // 得到航班信息查询结果后的回调函数 var flightHandler = function(data)&#123; alert('你查询的航班结果是：票价 ' + data.price + ' 元，' + '余票 ' + data.tickets + ' 张。'); &#125;; // 提供jsonp服务的url地址（不管是什么类型的地址，最终生成的返回值都是一段javascript代码） var url = \"http://flightQuery.com/jsonp/flightResult.aspx?code=CA1998&amp;callback=flightHandler\"; // 创建script标签，设置其属性 var script = document.createElement('script'); script.setAttribute('src', url); // 把script标签加入head，此时调用开始 document.getElementsByTagName('head')[0].appendChild(script); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 这次的代码变化比较大，不再直接把远程js文件写死，而是编码实现动态查询，而这也正是jsonp客户端实现的核心部分，本例中的重点也就在于如何完成jsonp调用的全过程。 我们看到调用的url中传递了一个code参数，告诉服务器我要查的是CA1998次航班的信息，而callback参数则告诉服务器，我的本地回调函数叫做flightHandler，所以请把查询结果传入这个函数中进行调用。 OK，服务器很聪明，这个叫做flightResult.aspx的页面生成了一段这样的代码提供给jsonp.html（服务端的实现这里就不演示了，与你选用的语言无关，说到底就是拼接字符串）： 12345flightHandler(&#123; \"code\": \"CA1998\", \"price\": 1780, \"tickets\": 5&#125;); 我们看到，传递给flightHandler函数的是一个json，它描述了航班的基本信息。运行一下页面，成功弹出提示窗口，jsonp的执行全过程顺利完成！ 4、到这里为止的话，相信你已经能够理解jsonp的客户端实现原理了吧？剩下的就是如何把代码封装一下，以便于与用户界面交互，从而实现多次和重复调用。 什么？你用的是jQuery，想知道jQuery如何实现jsonp调用？好吧，那我就好人做到底，再给你一段jQuery使用jsonp的代码（我们依然沿用上面那个航班信息查询的例子，假定返回jsonp结果不变）： 123456789101112131415161718192021222324252627&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt; &lt;html xmlns=\"http://www.w3.org/1999/xhtml\" &gt; &lt;head&gt; &lt;title&gt;Untitled Page&lt;/title&gt; &lt;script type=\"text/javascript\" src=jquery.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; jQuery(document).ready(function()&#123; $.ajax(&#123; type: \"get\", async: false, url: \"http://flightQuery.com/jsonp/flightResult.aspx?code=CA1998\", dataType: \"jsonp\", jsonp: \"callback\",//传递给请求处理程序或页面的，用以获得jsonp回调函数名的参数名(一般默认为:callback) jsonpCallback:\"flightHandler\",//自定义的jsonp回调函数名称，默认为jQuery自动生成的随机函数名，也可以写\"?\"，jQuery会自动为你处理数据 success: function(json)&#123; alert('您查询到航班信息：票价： ' + json.price + ' 元，余票： ' + json.tickets + ' 张。'); &#125;, error: function()&#123; alert('fail'); &#125; &#125;); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; 是不是有点奇怪？为什么我这次没有写flightHandler这个函数呢？而且竟然也运行成功了！哈哈，这就是jQuery的功劳了，jquery在处理jsonp类型的ajax时（还是忍不住吐槽，虽然jquery也把jsonp归入了ajax，但其实它们真的不是一回事儿），自动帮你生成回调函数并把数据取出来供success属性方法来调用，是不是很爽呀？ 总结1、ajax和jsonp这两种技术在调用方式上“看起来”很像，目的也一样，都是请求一个url，然后把服务器返回的数据进行处理，因此jquery和ext等框架都把jsonp作为ajax的一种形式进行了封装； 2、但ajax和jsonp其实本质上是不同的东西。ajax的核心是通过XmlHttpRequest获取非本页内容，而jsonp的核心则是动态添加&lt;script&gt;标签来调用服务器提供的js脚本。 3、所以说，其实ajax与jsonp的区别不在于是否跨域，ajax通过服务端代理一样可以实现跨域，jsonp本身也不排斥同域的数据的获取。 4、还有就是，jsonp是一种方式或者说非强制性协议，如同ajax一样，它也不一定非要用json格式来传递数据，如果你愿意，字符串都行，只不过这样不利于用jsonp提供公开服务。 总而言之，jsonp不是ajax的一个特例，哪怕jquery等巨头把jsonp封装进了ajax，也不能改变着一点！ 转自：说说JSON和JSONP，也许你会豁然开朗，含jQuery用例","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"前端","slug":"前端","permalink":"https://sunny0715.github.io/tags/前端/"}]},{"title":"Linux下安装Nginx","slug":"Linux下安装Nginx","date":"2018-01-11T07:16:49.000Z","updated":"2018-03-13T05:51:21.886Z","comments":true,"path":"2018/01/11/Linux下安装Nginx/","link":"","permalink":"https://sunny0715.github.io/2018/01/11/Linux下安装Nginx/","excerpt":"","text":"# Nginx安装环境Nginx是C语言开发，建议在Linxu上运行，下面操作实在Centos6.5上的安装环境。1. gcc安装nginx需要先将官网下载的源码进行编译，编译依赖gcc环境，如果没有gcc环境，需要安装gcc。安装命令：yum install gcc-c++2. PCREPCRE(Perl Compatible Regular Expressions)是一个Perl库，包括 perl 兼容的正则表达式库。nginx的http模块使用pcre来解析正则表达式，所以需要在linux上安装pcre库。安装命令：yum install -y pcre pcre-devel3. zlibzlib库提供了很多种压缩和解压缩的方式，nginx使用zlib对http包的内容进行gzip，所以需要在linux上安装zlib库。安装命令：yum install -y zlib zlib-devel4. opensslOpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及SSL协议，并提供丰富的应用程序供测试或其它目的使用。nginx不仅支持http协议，还支持https（即在ssl协议上传输http），所以需要在linux安装openssl库。安装命令：yum install -y openssl openssl-devel# 编译安装将nginx-1.8.0.tar.gz拷贝至Linux服务器后解压。12tar -zxvf nginx-1.8.0.tar.gzcd nginx-1.8.01. configure./configure –help查询详细参数。参数设置如下：注意：下边将临时文件目录指定为/var/temp/nginx，需要在/var下创建temp及nginx目录123456789101112./configure \\--prefix=/usr/local/nginx \\--pid-path=/var/run/nginx/nginx.pid \\--lock-path=/var/lock/nginx.lock \\--error-log-path=/var/log/nginx/error.log \\--http-log-path=/var/log/nginx/access.log \\--with-http_gzip_static_module \\--http-client-body-temp-path=/var/temp/nginx/client \\--http-proxy-temp-path=/var/temp/nginx/proxy \\--http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\--http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\--http-scgi-temp-path=/var/temp/nginx/scgi2. 编译安装12makemake install 启动Nginx1234cd /usr/local/nginx/sbin/./nginx// 查询nginx进程命令ps aux|grep nginx 注意：执行./nginx启动nginx，这里可以-c指定加载的nginx配置文件，如下： 1./nginx -c /usr/soft/nginx-1.8.0/conf/nginx.conf 如果不指定-c，nginx在启动时默认加载/usr/local/nginx/conf/nginx.conf文件，此文件的地址也可以在编译安装nginx时指定./configure的参数（–conf-path= 指向配置文件（nginx.conf）） 重启Nginx 先停止再启动（建议使用） 对nginx进行重启相当于先停止nginx再启动nginx，即先执行停止命令再执行启动命令。 12./nginx -s quit./nginx 重新加载配置文件 当nginx的配置文件nginx.conf修改后，要想让配置生效需要重启nginx，使用-s reload不用先停止nginx再启动nginx即可将配置信息在nginx中生效。 1./nginx -s reload 测试nginx安装成功，启动nginx，即可访问虚拟机上的nginx 到这说明nginx上安装成功。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://sunny0715.github.io/tags/linux/"},{"name":"nginx","slug":"nginx","permalink":"https://sunny0715.github.io/tags/nginx/"}]},{"title":"Linux下安装JDK与MySQL","slug":"Linux下安装JDK与MySQL","date":"2018-01-09T11:53:24.000Z","updated":"2018-03-20T06:18:45.762Z","comments":true,"path":"2018/01/09/Linux下安装JDK与MySQL/","link":"","permalink":"https://sunny0715.github.io/2018/01/09/Linux下安装JDK与MySQL/","excerpt":"引言重温记录下Linux环境下JDK和MySQL的安装。","text":"引言重温记录下Linux环境下JDK和MySQL的安装。 JDK的安装下载解压下载JDK压缩包，下载目录：http://www.oracle.com/technetwork/java/javase/downloads/index.html 解压 1tar -xvzf jdk-8u152-linux-x64.tar.gz 配置环境变量以root用户使用以下命令进入配置环境变量的profile文件。 1vim /etc/profile 在文件末尾加入以下内容并保存（注意修改JDK路径）。 1234# set java environment export JAVA_HOME=/usr/soft/jdk1.8.0_152export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 在命令行使用以下命令使环境变量生效。 1source /etc/profile 切换JDK版本当Linux中安装多个JDK时切换进行版本切换。 查看选择所有JDK。 1alternatives --config java 给jdk1.8.0_152设置序列号，输入以下命令（注意修改JDK目录）。 1alternatives --install /usr/bin/java java /usr/soft/jdk1.8.0_152 4 输入以下命令，选择JDK对应的数字，切换JDK版本。 1alternatives --config java MySQL的安装与卸载yum安装从Oracle官方网站下载Linux系统对应的MySQL的yum源包。地址：https://dev.mysql.com/downloads/repo/yum/ 把yum源包上传到linux，依次执行以下命令进行安装。 12yum localinstall mysql-community-release-el6-5.noarch.rpmyum install mysql-server 安装完成后启动MySQL 1service mysqld start 给root用户设置密码 1/usr/bin/mysqladmin -u root password &apos;root&apos; 进入MySQL后进行远程连接授权 1GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION; 卸载查看MySQL的安装路径 1whereis mysql 查看mysql的安装包 1rpm -qa|grep mysql 卸载 1yum remove mysql 若卸载不完全，则要逐个卸载 1234rpm -qa|grep mysqlyum remove mysql-community-release-el6-5.noarchyum remove mysql-community-common-5.6.38-2.el6.x86_64yum remove mysql-community-libs-5.6.38-2.el6.x86_64 删除mysql的数据库文件 1rm -rf /var/lib/mysql/ 安装包离线安装下载MySQL离线安装包：https://dev.mysql.com/downloads/mysql/ 123456mv mysql-5.6.38-linux-glibc2.12-x86_64.tar.gz /usr/local/cd /usr/local/// 解压MySQL安装包tar -zxvf mysql-5.6.38-linux-glibc2.12-x86_64.tar.gz// 重命名mv mysql-5.6.38-linux-glibc2.12-x86_64 mysql 检查MySQL组和用户是否存在，如无创建 12345cat /etc/group | grep mysqlcat /etc/passwd | grep mysql// 如果没有则创建。useradd -r参数表示mysql用户是系统用户，不可用于登录系统groupadd mysqluseradd -r -g mysql mysql 分配用户和组 123456cd mysql// 更改mysql目录所属的用户(用户为mysql)chown -R mysql ../mysql/// -R是递归的意思，就是把mysql目录下的全部文件和子目录都设置为mysql用户和mysql组。chgrp -R mysql ../mysql/// 上面的做法是为了把mysql降权，以限定只能访问属于mysql用户的文件。 安装及初始化数据库（创建系统数据库的表） 1./scripts/mysql_install_db --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/ 配置MySQL数据库 12345678// 复制配置文件cp -a ./support-files/my-default.cnf /etc/my.cnf// 更改配置文件信息vi /etc/my.cnf// 加入以下内容# These are commonly set, remove the # and set as required.basedir = /usr/local/mysqldatadir = /usr/local/mysql/data 修改MySQL密码 123456// 启动MySQL./support-files/mysql.server start// 修改密码./bin/mysqladmin -u root -h localhost.localdomain password 'root'// 进入MySQL./bin/mysql -h127.0.0.1 -uroot -proot 增加远程登录权限 12grant all privileges on *.* to root@&apos;%&apos; identified by &apos;root&apos;;flush privileges; 将MySQL加入Service系统服务 123456// 先退出MySQLcp support-files/mysql.server /etc/init.d/mysqldchkconfig --add mysqldchkconfig mysqld onservice mysqld restartservice mysqld status 到这里MySQL就配置完成了，剩下的就是优化MySQL，配置/etc/my.cnf啦！","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://sunny0715.github.io/tags/mysql/"},{"name":"linux","slug":"linux","permalink":"https://sunny0715.github.io/tags/linux/"}]},{"title":"Linux下Tomcat的安装与优化","slug":"Linux下Tomcat的安装与优化","date":"2018-01-05T11:04:11.000Z","updated":"2018-03-13T05:50:48.918Z","comments":true,"path":"2018/01/05/Linux下Tomcat的安装与优化/","link":"","permalink":"https://sunny0715.github.io/2018/01/05/Linux下Tomcat的安装与优化/","excerpt":"引言Linux系统已经搁置很久了，之前有在Ubuntu系统上开发过，但是Linux已经很久没有用了。现在公司把项目部署在Linux系统上，又要把Linux相关知识温习一下。这篇博客温习一下Linux下Tomcat的部署与优化，大部分的操作与在windows上相同。","text":"引言Linux系统已经搁置很久了，之前有在Ubuntu系统上开发过，但是Linux已经很久没有用了。现在公司把项目部署在Linux系统上，又要把Linux相关知识温习一下。这篇博客温习一下Linux下Tomcat的部署与优化，大部分的操作与在windows上相同。 Tomcat的安装首先下载Tomcat的压缩包（apache-tomcat-7.0.82.tar.gz），下载地址为：https://tomcat.apache.org/download-70.cgi 将压缩包放到Linux预定目录下，执行tar的解压缩命令 12cd /usr/soft/tar -zxvf apache-tomcat-7.0.82.tar.gz 进入到apache-tomcat-7.0.82.tar.gz的bin目录下执行./startup.sh 命令即可启动Tomcat。 Tomcat的优化默认情况下Tomcat的配置适合开发模式或者比较小的系统应用，当访问量稍微多的时候比如1000人同时在线做一些频繁的业务操作的时候，可能性能方面就会存在问题，所以有必要在生产环境下对Tomcat做一些优化。 之前几篇文章也提到了Tomcat相关参数的设置与优化，Windows操作系统与Linux操作系统大同小异。 APR模式Tomcat 常用运行模式有3种，分别为 BIO，NIO，APR。生产环境建议用APR，从操作系统级别来解决异步的IO问题，大幅度的提高性能。Linux下需要另安装配置APR。 下载APR模式需要下载apr-1.6.3.tar.gz和apr-util-1.6.1.tar.gz两个文件，下载地址为：http://apr.apache.org/download.cgi 安装将连个文件放到合适的位置然后进行安装操作。 apr的安装依次执行，将安装路径设为/usr/local/apr 12345tar -zxvf apr-1.6.3.tar.gzcd apr-1.6.3.tar.gz./configure --prefix=/usr/local/aprmakemake install apr-util的安装12345tar -zxvf apr-util-1.6.1.tar.gzcd apr-util-1.6.1.tar.gz./configure --with-apr=/usr/local/apr/bin/apr-1-configmakemake install 安装tomcat-nativetomcat-native.tar.gz是Tomcat自带的压缩包，该文件在tomcat的bin目录下。 系统要先安装好JDK，我的JDK的安装目录为：/usr/soft/jdk1.8.0_152 123456cd /usr/soft/apache-tomcat-7.0.82/bin/tar -zxvf tomcat-native.tar.gzcd tomcat-native-1.2.14-src/java/org/apache/tomcat/jni/./configure --with-apr=/usr/local/apr/bin/apr-1-config --with-java-home=/usr/soft/jdk1.8.0_152makemake install 配置 编辑tomcat目录下文件bin/catalina.sh加载apr，在任意地方加入下面一行 1CATALINA_OPTS=\"$CATALINA_OPTS -Djava.library.path=/usr/local/apr/lib\" 编辑bin/catalina.sh配置JVM运行参数，注意引号不要忘记。 1JAVA_OPTS=&quot;-server -Xmx4g -Xms4g -Xmn1g -XX:PermSize=512M -XX:MaxPermSize=521M -XX:+DisableExplicitGC -XX:SurvivorRatio=3 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/soft/apache-tomcat-7.0.82 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:CMSInitiatingOccupancyFraction=65 -XX:+UseCMSInitiatingOccupancyOnly -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+UseCMSCompactAtFullCollection -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:/usr/soft/jdk1.8.0_152/log/gc.log -Djava.awt.headless=true&quot; 编辑conf/server.xml使用apr运行模式 12345&lt;Connector port=&quot;8080&quot; protocol=&quot;org.apache.coyote.http11.Http11AprProtocol&quot; connectionTimeout=&quot;20000&quot; maxThreads=&quot;1000&quot; minSpareThreads=&quot;100&quot; maxSpareThreads=&quot;200&quot; acceptCount=&quot;900&quot; enableLookups=&quot;false&quot; compression=&quot;on&quot; compressionMinSize=&quot;1024&quot; compressableMimeType=&quot;text/html,text/xml,text/css,text/javascript&quot; redirectPort=&quot;8443&quot; URIEncoding=&quot;UTF-8&quot; maxHttpHeaderSize=&quot;8192&quot;/&gt; 启动Tomcat 启动tomcat，查看tomcat日志文件，若出现如下信息则表明安装配置成功。 123456一月 05, 2018 2:03:09 下午 org.apache.coyote.AbstractProtocol init信息: Initializing ProtocolHandler [&quot;http-apr-8080&quot;]一月 05, 2018 2:03:09 下午 org.apache.coyote.AbstractProtocol init信息:: Initializing ProtocolHandler [&quot;ajp-apr-8009&quot;]一月 05, 2018 2:03:09 下午 org.apache.catalina.startup.Catalina load信息:: Initialization processed in 1471 ms 结语性能的影响因素是多方面的，互相影响，首先是系统本身没问题，数据库的响应没问题，web容器顺畅，硬件顺畅，网络带宽足够，再使用一些小工具进行检测，只有在大量用户在实际的生产环境中使用系统，才能发现问题，找到问题的根源到底是哪一块引发的性能瓶颈，调整一下自然一切都变得顺畅。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"tomcat","slug":"tomcat","permalink":"https://sunny0715.github.io/tags/tomcat/"},{"name":"linux","slug":"linux","permalink":"https://sunny0715.github.io/tags/linux/"}]},{"title":"回顾过去 展望未来","slug":"回顾过去-展望未来","date":"2018-01-02T01:53:54.000Z","updated":"2018-03-13T05:53:51.648Z","comments":true,"path":"2018/01/02/回顾过去-展望未来/","link":"","permalink":"https://sunny0715.github.io/2018/01/02/回顾过去-展望未来/","excerpt":"时间过的可真的是快，转眼间到了2018年，今年的元旦没有出去跨年，而是在家里静静的等待着2018年的到来，不知道是欣喜还是忧郁。2017年我的年度目标只实现了90%，并没有完全达到期望。","text":"时间过的可真的是快，转眼间到了2018年，今年的元旦没有出去跨年，而是在家里静静的等待着2018年的到来，不知道是欣喜还是忧郁。2017年我的年度目标只实现了90%，并没有完全达到期望。 自己最近一年来特别不想熬夜，一熬夜第二天就感觉特别的累。这一年自己也学到了很多的东西，虽然感觉很累，但是觉得很开心。公司放假的安排也出来了，腊月二十四，放假立马就回家，又到了一年抢票的时间。几多欢喜几多愁啊。2018也是自己的本命年，加油！加油！","categories":[{"name":"life","slug":"life","permalink":"https://sunny0715.github.io/categories/life/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://sunny0715.github.io/tags/随笔/"}]},{"title":"详解Tomcat连接池与连接数","slug":"详解Tomcat连接池与连接数","date":"2017-12-28T06:56:06.000Z","updated":"2018-03-13T05:55:18.252Z","comments":true,"path":"2017/12/28/详解Tomcat连接池与连接数/","link":"","permalink":"https://sunny0715.github.io/2017/12/28/详解Tomcat连接池与连接数/","excerpt":"引言在使用Tomcat时，经常会遇到连接数、线程数之类的配置，然后自己就去谷歌、百度，没有真正理解Tomcat配置的作用及当前业务环境、服务器配置等情况下Tomcat最优配置。 12345&lt;Connector port=\"8080\" protocol=\"org.apache.coyote.http11.Http11AprProtocol\" connectionTimeout=\"20000\" maxThreads=\"1000\" minSpareThreads=\"100\" maxSpareThreads=\"200\" acceptCount=\"900\" enableLookups=\"false\" compression=\"on\" compressionMinSize=\"1024\" compressableMimeType=\"text/html,text/xml,text/css,text/javascript\" redirectPort=\"8443\" URIEncoding=\"UTF-8\" maxHttpHeaderSize=\"8192\"/&gt;","text":"引言在使用Tomcat时，经常会遇到连接数、线程数之类的配置，然后自己就去谷歌、百度，没有真正理解Tomcat配置的作用及当前业务环境、服务器配置等情况下Tomcat最优配置。 12345&lt;Connector port=\"8080\" protocol=\"org.apache.coyote.http11.Http11AprProtocol\" connectionTimeout=\"20000\" maxThreads=\"1000\" minSpareThreads=\"100\" maxSpareThreads=\"200\" acceptCount=\"900\" enableLookups=\"false\" compression=\"on\" compressionMinSize=\"1024\" compressableMimeType=\"text/html,text/xml,text/css,text/javascript\" redirectPort=\"8443\" URIEncoding=\"UTF-8\" maxHttpHeaderSize=\"8192\"/&gt; Tomcat连接器（Connector）上一篇文章说到过Tomcat的配置文件server.xml ：Connector的主要功能，是接收连接请求，创建Request和Response对象用于和请求端交换数据；然后分配线程让Engine（也就是Servlet容器）来处理这个请求，并把产生的Request和Response对象传给Engine。当Engine处理完请求后，也会通过Connector将响应返回给客户端。 可以说，Servlet容器处理请求，是需要Connector进行调度和控制的，Connector是Tomcat处理请求的主干，因此Connector的配置和使用对Tomcat的性能有着重要的影响。这篇文将从Connector入手，讨论一些与Connector有关的重要问题，包括NIO/BIO模式、线程池、连接数等。 根据协议的不同，Connector可以分为HTTP Connector、AJP Connector等，在这篇文章我们只讨论HTTP Connector。 BIO、NIO、APRConnector的protocalConnector在处理HTTP请求时，会使用不同的protocal。不同的Tomcat版本支持的protocal不同，其中最典型的protocol包括BIO、NIO和APR（Tomcat7中支持这3种，Tomcat8增加了对NIO2的支持，而到了Tomcat8.5和Tomcat9.0，则去掉了对BIO的支持）。 BIO是Blocking IO，顾名思义是阻塞的IO；NIO是Non-blocking IO，则是非阻塞的IO。而APR是Apache Portable Runtime，是Apache可移植运行库，利用本地库可以实现高可扩展性、高性能；Apr是在Tomcat上运行高并发应用的首选模式，但是需要安装apr、apr-utils、tomcat-native等包。 指定protocalConnector使用哪种protocol，可以通过&lt;connector&gt;元素中的protocol属性进行指定，也可以使用默认值。 指定的protocol取值及对应的协议如下： HTTP/1.1：默认值，使用的协议与Tomcat版本有关 org.apache.coyote.http11.Http11Protocol：BIO org.apache.coyote.http11.Http11NioProtocol：NIO org.apache.coyote.http11.Http11Nio2Protocol：NIO2 org.apache.coyote.http11.Http11AprProtocol：APR 如果没有指定protocol，则使用默认值HTTP/1.1，其含义如下：在Tomcat7中，自动选取使用BIO或APR（如果找到APR需要的本地库，则使用APR，否则使用BIO）；在Tomcat8中，自动选取使用NIO或APR（如果找到APR需要的本地库，则使用APR，否则使用NIO）。 BIO和NIO的不同无论是BIO，还是NIO，Connector处理请求的大致流程是一样的： 在accept队列中接收连接（当客户端向服务器发送请求时，如果客户端与OS完成三次握手建立了连接，则OS将该连接放入accept队列）；在连接中获取请求的数据，生成request；调用servlet容器处理请求；返回response。为了便于后面的说明，首先明确一下连接与请求的关系：连接是TCP层面的（传输层），对应socket；请求是HTTP层面的（应用层），必须依赖于TCP的连接实现；一个TCP连接中可能传输多个HTTP请求。 在BIO实现的Connector中，处理请求的主要实体是JIoEndpoint对象。JIoEndpoint维护了Acceptor和Worker：Acceptor接收socket，然后从Worker线程池中找出空闲的线程处理socket，如果worker线程池没有空闲线程，则Acceptor将阻塞。其中Worker是Tomcat自带的线程池，如果通过配置了其他线程池，原理与Worker类似。 在NIO实现的Connector中，处理请求的主要实体是NIoEndpoint对象。NIoEndpoint中除了包含Acceptor和Worker外，还是用了Poller，处理流程如下图所示 Acceptor接收socket后，不是直接使用Worker中的线程处理请求，而是先将请求发送给了Poller，而Poller是实现NIO的关键。Acceptor向Poller发送请求通过队列实现，使用了典型的生产者-消费者模式。在Poller中，维护了一个Selector对象；当Poller从队列中取出socket后，注册到该Selector中；然后通过遍历Selector，找出其中可读的socket，并使用Worker中的线程处理相应请求。与BIO类似，Worker也可以被自定义的线程池代替。 通过上述过程可以看出，在NIoEndpoint处理请求的过程中，无论是Acceptor接收socket，还是线程处理请求，使用的仍然是阻塞方式；但在“读取socket并交给Worker中的线程”的这个过程中，使用非阻塞的NIO实现，这是NIO模式与BIO模式的最主要区别（其他区别对性能影响较小，暂时略去不提）。而这个区别，在并发量较大的情形下可以带来Tomcat效率的显著提升： 目前大多数HTTP请求使用的是长连接（HTTP/1.1默认keep-alive为true），而长连接意味着，一个TCP的socket在当前请求结束后，如果没有新的请求到来，socket不会立马释放，而是等timeout后再释放。如果使用BIO，“读取socket并交给Worker中的线程”这个过程是阻塞的，也就意味着在socket等待下一个请求或等待释放的过程中，处理这个socket的工作线程会一直被占用，无法释放；因此Tomcat可以同时处理的socket数目不能超过最大线程数，性能受到了极大限制。而使用NIO，“读取socket并交给Worker中的线程”这个过程是非阻塞的，当socket在等待下一个请求或等待释放时，并不会占用工作线程，因此Tomcat可以同时处理的socket数目远大于最大线程数，并发性能大大提高。 acceptCount、maxConnections、maxThreads参数Tomcat处理请求的过程：在accept队列中接收连接（当客户端向服务器发送请求时，如果客户端与OS完成三次握手建立了连接，则OS将该连接放入accept队列）；在连接中获取请求的数据，生成request；调用servlet容器处理请求；返回response。 相对应的，Connector中的几个参数功能如下： acceptCountaccept队列的长度；当accept队列中连接的个数达到acceptCount时，队列满，进来的请求一律被拒绝。默认值是100。 maxConnectionsTomcat在任意时刻接收和处理的最大连接数。当Tomcat接收的连接数达到maxConnections时，Acceptor线程不会读取accept队列中的连接；这时accept队列中的线程会一直阻塞着，直到Tomcat接收的连接数小于maxConnections。如果设置为-1，则连接数不受限制。 默认值与连接器使用的协议有关：NIO的默认值是10000，APR/native的默认值是8192，而BIO的默认值为maxThreads（如果配置了Executor，则默认值是Executor的maxThreads）。 在windows下，APR/native的maxConnections值会自动调整为设置值以下最大的1024的整数倍；如设置为2000，则最大值实际是1024。 maxThreads请求处理线程的最大数量。默认值是200（Tomcat7和8都是的）。如果该Connector绑定了Executor，这个值会被忽略，因为该Connector将使用绑定的Executor，而不是内置的线程池来执行任务。 maxThreads规定的是最大的线程数目，并不是实际running的CPU数量；实际上，maxThreads的大小比CPU核心数量要大得多。这是因为，处理请求的线程真正用于计算的时间可能很少，大多数时间可能在阻塞，如等待数据库返回数据、等待硬盘读写数据等。因此，在某一时刻，只有少数的线程真正的在使用物理CPU，大多数线程都在等待；因此线程数远大于物理核心数才是合理的。 换句话说，Tomcat通过使用比CPU核心数量多得多的线程数，可以使CPU忙碌起来，大大提高CPU的利用率。 参数设置 maxThreads的设置既与应用的特点有关，也与服务器的CPU核心数量有关。通过前面介绍可以知道，maxThreads数量应该远大于CPU核心数量；而且CPU核心数越大，maxThreads应该越大；应用中CPU越不密集（IO越密集），maxThreads应该越大，以便能够充分利用CPU。当然，maxThreads的值并不是越大越好，如果maxThreads过大，那么CPU会花费大量的时间用于线程的切换，整体效率会降低。 maxConnections的设置与Tomcat的运行模式有关。如果tomcat使用的是BIO，那么maxConnections的值应该与maxThreads一致；如果tomcat使用的是NIO，那么类似于Tomcat的默认值，maxConnections值应该远大于maxThreads。 通过前面的介绍可以知道，虽然tomcat同时可以处理的连接数目是maxConnections，但服务器中可以同时接收的连接数为maxConnections+acceptCount 。acceptCount的设置，与应用在连接过高情况下希望做出什么反应有关系。如果设置过大，后面进入的请求等待时间会很长；如果设置过小，后面进入的请求立马返回connection refused。 线程池ExecutorExecutor元素代表Tomcat中的线程池，可以由其他组件共享使用；要使用该线程池，组件需要通过Executor属性指定该线程池。 Executor是Service元素的内嵌元素。一般来说，使用线程池的是Connector组件；为了使Connector能使用线程池，Executor元素应该放在Connector前面。Executor与Connector的配置举例如下： 123&lt;Executor name=\"tomcatThreadPool\" namePrefix =\"catalina-exec-\" maxThreads=\"150\" minSpareThreads=\"4\" /&gt;&lt;Connector executor=\"tomcatThreadPool\" port=\"8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" acceptCount=\"1000\" /&gt; Executor的主要属性包括： name：该线程池的标记 maxThreads：线程池中最大活跃线程数，默认值200（Tomcat7和8都是） minSpareThreads：线程池中保持的最小线程数，最小值是25 maxIdleTime：线程空闲的最大时间，当空闲超过该值时关闭线程（除非线程数小于minSpareThreads），单位是ms，默认值60000（1分钟） daemon：是否后台线程，默认值true threadPriority：线程优先级，默认值5 namePrefix：线程名字的前缀，线程池中线程名字为：namePrefix+线程编号 查看当前状态上面介绍了Tomcat连接数、线程数的概念以及如何设置，下面说明如何查看服务器中的连接数和线程数。 查看服务器的状态，大致分为两种方案： jconsole工具现成的工具，如JDK自带的jconsole工具可以方便的查看线程信息（此外还可以查看CPU、内存、类、JVM基本信息等），Tomcat自带的manager，收费工具New Relic等。下图是jconsole查看线程信息的界面： Linux命令查看假设Tomcat接收http请求的端口是8083，则可以使用如下语句查看连接情况： 1netstat –nat|grep 8080 结果如下所示： 可以看出，有一个连接处于listen状态，监听请求；除此之外，还有6个已经建立的连接（ESTABLISHED）和0个等待关闭的连接（CLOSE_WAIT）。 原文： https://mp.weixin.qq.com/s?__biz=MjM5NzMyMjAwMA==&amp;mid=2651479428&amp;idx=1&amp;sn=791fed1205da057aba77655aaac9d841&amp;chksm=bd2531fb8a52b8ed0066a8efc76d031ffb6e0d2099fb342129c307f78b4bf0581cbf3bbcb058&amp;mpshare=1&amp;scene=1&amp;srcid=1114Osu1mhmfSobleuByFbEC#rd","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"tomcat","slug":"tomcat","permalink":"https://sunny0715.github.io/tags/tomcat/"}]},{"title":"Tomcat入门","slug":"tomcat入门","date":"2017-12-20T06:35:32.000Z","updated":"2018-03-13T05:52:58.071Z","comments":true,"path":"2017/12/20/tomcat入门/","link":"","permalink":"https://sunny0715.github.io/2017/12/20/tomcat入门/","excerpt":"引言Tomcat 服务器是一个免费的开放源代码的Web 应用服务器，属于轻量级应用服务器。说是经常用到，也只是熟悉，还没没有真正达到了解其中的原理和其中配置的意义，最近也找了一些书籍来看，先入门。","text":"引言Tomcat 服务器是一个免费的开放源代码的Web 应用服务器，属于轻量级应用服务器。说是经常用到，也只是熟悉，还没没有真正达到了解其中的原理和其中配置的意义，最近也找了一些书籍来看，先入门。 Tomcat简介Tomcat的下载包解压之后的目录 Tomcat根目录在Tomcat中叫&lt;CATALINA_HOME&gt; &lt;CATALINA_HOME&gt;/bin：存放各种平台下启动和关闭Tomcat的脚本文件。其中有个是catalina.bat，打开这个windows配置文件，在非注释行加入JDK路径,例如 : SET JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_141，其中对JDK的优化也在catalina.bat中配置，保存后就配置好Tomcat环境了。 startup.bat是windows下启动Tomcat的脚本文件，shutdown.bat是关闭Tomcat的脚本文件。 &lt;CATALINA_HOME&gt;/conf：存放不同的配置文件（如：server.xml和web.xml） server.xml文件：该文件用于配置和server相关的信息，比如tomcat启动的端口号、配置host主机、配置Context，接下来会重点讲述。 web.xml文件：部署描述文件，这个web.xml中描述了一些默认的servlet，部署每个webapp时，都会调用这个文件，配置该web应用的默认servlet。 tomcat-users.xml文件：配置tomcat的用户密码与权限。 context.xml：定义web应用的默认行为。&lt;CATALINA_HOME&gt;/lib：存放Tomcat运行需要的库文件（Jars）；&lt;CATALINA_HOME&gt;/logs：存放Tomcat执行时的log文件；&lt;CATALINA_HOME&gt;/temp： 存放Tomcat运行时产生的文件，如缓存等；&lt;CATALINA_HOME&gt;/webapps：Tomcat的主要Web发布目录（包括应用程序示例）； &lt;CATALINA_HOME&gt;/work：存放jsp编译后产生的class文件； 【Tomcat的启动过程】Tomcat 先根据/conf/server.xml 下的配置启动Server，再加载Service，对于与Engine相匹配的Host，每个Host 下面都有一个或多个Context。 注意：Context 既可配置在server.xml 下，也可配置成一单独的文件，放在conf\\Catalina\\localhost 下，简称应用配置文件。 Web Application 对应一个Context，每个Web Application 由一个或多个Servlet 组成。当一个Web Application 被初始化的时候，它将用自己的ClassLoader 对象载入部署配置文件web.xml 中定义的每个Servlet 类：它首先载入在$CATALINA_HOME/conf/web.xml中部署的Servlet 类，然后载入在自己的Web Application 根目录下WEB-INF/web.xml 中部署的Servlet 类。 web.xml 文件有两部分：Servlet 类定义和Servlet 映射定义。每个被载入的Servlet 类都有一个名字，且被填入该Context 的映射表(mapping table)中，和某种URL 路径对应。当该Context 获得请求时，将查询mapping table，找到被请求的Servlet，并执行以获得请求响应。 Tomcat一个server实例12345678910111213141516171819202122232425262728293031&lt;Server port=\"8005\" shutdown=\"SHUTDOWN\"&gt; &lt;Listener className=\"org.apache.catalina.startup.VersionLoggerListener\" /&gt; &lt;Listener className=\"org.apache.catalina.core.AprLifecycleListener\" SSLEngine=\"on\" /&gt; &lt;Listener className=\"org.apache.catalina.core.JasperListener\" /&gt; &lt;Listener className=\"org.apache.catalina.core.JreMemoryLeakPreventionListener\" /&gt; &lt;Listener className=\"org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\" /&gt; &lt;Listener className=\"org.apache.catalina.core.ThreadLocalLeakPreventionListener\" /&gt; &lt;GlobalNamingResources&gt; &lt;Resource name=\"UserDatabase\" auth=\"Container\" type=\"org.apache.catalina.UserDatabase\" description=\"User database that can be updated and saved\" factory=\"org.apache.catalina.users.MemoryUserDatabaseFactory\" pathname=\"conf/tomcat-users.xml\" /&gt; &lt;/GlobalNamingResources&gt; &lt;Service name=\"Catalina\"&gt; &lt;Connector port=\"8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt; &lt;Connector port=\"8009\" protocol=\"AJP/1.3\" redirectPort=\"8443\" /&gt; &lt;Engine name=\"Catalina\" defaultHost=\"localhost\"&gt; &lt;Realm className=\"org.apache.catalina.realm.LockOutRealm\"&gt; &lt;Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/&gt; &lt;/Realm&gt; &lt;Host name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"&gt; &lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log.\" suffix=\".txt\" pattern=\"%h %l %u %t &amp;quot;%r&amp;quot; %s %b\" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; server.xml文档的元素分类和整体结构整体结构server.xml的整体结构如下： 1234567891011&lt;Server&gt; &lt;Service&gt; &lt;Connector/&gt; &lt;Connector/&gt; &lt;Engine&gt; &lt;Host&gt; &lt;Context/&gt;&lt;!-- 现在常常使用自动部署，不推荐配置Context元素，Context小节有详细说明 --&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; 该结构中只给出了Tomcat的核心组件，除了核心组件外，Tomcat还有一些其他组件，下面介绍一下组件的分类。 元素分类server.xml文件中的元素可以分为以下4类： （1）顶层元素：&lt;Server&gt;和&lt;Service&gt; &lt;Server&gt;元素是整个配置文件的根元素，&lt;Service&gt;元素则代表一个Engine元素以及一组与之相连的Connector元素。 （2）连接器：&lt;Connector&gt; &lt;Connector&gt;代表了外部客户端发送请求到特定Service的接口；同时也是外部客户端从特定Service接收响应的接口。 （3）容器：&lt;Engine&gt;&lt;Host&gt;&lt;Context&gt; 容器的功能是处理Connector接收进来的请求，并产生相应的响应。Engine、Host和Context都是容器，但它们不是平行的关系，而是父子关系：Engine包含Host，Host包含Context。一个Engine组件可以处理Service中的所有请求，一个Host组件可以处理发向一个特定虚拟主机的所有请求，一个Context组件可以处理一个特定Web应用的所有请求。 （4）内嵌组件：可以内嵌到容器中的组件。实际上，Server、Service、Connector、Engine、Host和Context是最重要的最核心的Tomcat组件，其他组件都可以归为内嵌组件。 核心组件1、Server Server元素在最顶层，代表整个Tomcat容器，因此它必须是server.xml中唯一一个最外层的元素。一个Server元素中可以有一个或多个Service元素。 在第一部分的例子中，在最外层有一个&lt;Server&gt;元素，shutdown属性表示关闭Server的指令；port属性表示Server接收shutdown指令的端口号，设为-1可以禁掉该端口。 Server的主要任务，就是提供一个接口让客户端能够访问到这个Service集合，同时维护它所包含的所有的Service的声明周期，包括如何初始化、如何结束服务、如何找到客户端要访问的Service。 2、Service Service的作用，是在Connector和Engine外面包了一层，把它们组装在一起，对外提供服务。一个Service可以包含多个Connector，但是只能包含一个Engine；其中Connector的作用是从客户端接收请求，Engine的作用是处理接收进来的请求。 在第一部分的例子中，Server中包含一个名称为“Catalina”的Service。实际上，Tomcat可以提供多个Service，不同的Service监听不同的端口。 3、Connector Connector的主要功能，是接收连接请求，创建Request和Response对象用于和请求端交换数据；然后分配线程让Engine来处理这个请求，并把产生的Request和Response对象传给Engine。 通过配置Connector，可以控制请求Service的协议及端口号。在第一部分的例子中，Service包含两个Connector： &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; &lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; （1）通过配置第1个Connector，客户端可以通过8080端口号使用http协议访问Tomcat。其中，protocol属性规定了请求的协议，port规定了请求的端口号，redirectPort表示当强制要求https而请求是http时，重定向至端口号为8443的Connector，connectionTimeout表示连接的超时时间。 在这个例子中，Tomcat监听HTTP请求，使用的是8080端口，而不是正式的80端口；实际上，在正式的生产环境中，Tomcat也常常监听8080端口，而不是80端口。这是因为在生产环境中，很少将Tomcat直接对外开放接收请求，而是在Tomcat和客户端之间加一层代理服务器(如nginx)，用于请求的转发、负载均衡、处理静态文件等；通过代理服务器访问Tomcat时，是在局域网中，因此一般仍使用8080端口。 （2）通过配置第2个Connector，客户端可以通过8009端口号使用AJP协议访问Tomcat。AJP协议负责和其他的HTTP服务器(如Apache)建立连接；在把Tomcat与其他HTTP服务器集成时，就需要用到这个连接器。之所以使用Tomcat和其他服务器集成，是因为Tomcat可以用作Servlet/JSP容器，但是对静态资源的处理速度较慢，不如Apache和IIS等HTTP服务器；因此常常将Tomcat与Apache等集成，前者作Servlet容器，后者处理静态资源，而AJP协议便负责Tomcat和Apache的连接。Tomcat与Apache等集成的原理如下图： 4、Engine Engine组件在Service组件中有且只有一个；Engine是Service组件中的请求处理组件。Engine组件从一个或多个Connector中接收请求并处理，并将完成的响应返回给Connector，最终传递给客户端。 前面已经提到过，Engine、Host和Context都是容器，但它们不是平行的关系，而是父子关系：Engine包含Host，Host包含Context。 在第一部分的例子中，Engine的配置语句如下： &lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot;&gt; 其中，name属性用于日志和错误信息，在整个Server中应该唯一。defaultHost属性指定了默认的host名称，当发往本机的请求指定的host名称不存在时，一律使用defaultHost指定的host进行处理；因此，defaultHost的值，必须与Engine中的一个Host组件的name属性值匹配。 5、Host （1）Engine与Host Host是Engine的子容器。Engine组件中可以内嵌1个或多个Host组件，每个Host组件代表Engine中的一个虚拟主机。Host组件至少有一个，且其中一个的name必须与Engine组件的defaultHost属性相匹配。 （2）Host的作用 Host虚拟主机的作用，是运行多个Web应用（一个Context代表一个Web应用），并负责安装、展开、启动和结束每个Web应用。 Host组件代表的虚拟主机，对应了服务器中一个网络名实体(如”www.test.com”，或IP地址”116.25.25.25”)；为了使用户可以通过网络名连接Tomcat服务器，这个名字应该在DNS服务器上注册。 客户端通常使用主机名来标识它们希望连接的服务器；该主机名也会包含在HTTP请求头中。Tomcat从HTTP头中提取出主机名，寻找名称匹配的主机。如果没有匹配，请求将发送至默认主机。因此默认主机不需要是在DNS服务器中注册的网络名，因为任何与所有Host名称不匹配的请求，都会路由至默认主机。 （3）Host的配置 在第一部分的例子中，Host的配置如下： &lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt; 下面对其中配置的属性进行说明： name属性指定虚拟主机的主机名，一个Engine中有且仅有一个Host组件的name属性与Engine组件的defaultHost属性相匹配；一般情况下，主机名需要是在DNS服务器中注册的网络名，但是Engine指定的defaultHost不需要，原因在前面已经说明。 unpackWARs指定了是否将代表Web应用的WAR文件解压；如果为true，通过解压后的文件结构运行该Web应用，如果为false，直接使用WAR文件运行Web应用。 Host的autoDeploy和appBase属性，与Host内Web应用的自动部署有关；此外，本例中没有出现的xmlBase和deployOnStartup属性，也与Web应用的自动部署有关。 6、Context （1）Context的作用 Context元素代表在特定虚拟主机上运行的一个Web应用。在后文中，提到Context、应用或Web应用，它们指代的都是Web应用。每个Web应用基于WAR文件，或WAR文件解压后对应的目录（这里称为应用目录）。 Context是Host的子容器，每个Host中可以定义任意多的Context元素。 在第一部分的例子中，可以看到server.xml配置文件中并没有出现Context元素的配置。这是因为，Tomcat开启了自动部署，Web应用没有在server.xml中配置静态部署，而是由Tomcat通过特定的规则自动部署。下面介绍一下Tomcat自动部署Web应用的机制。 （2）Web应用自动部署 Host的配置 要开启Web应用的自动部署，需要配置所在的虚拟主机；配置的方式就是前面提到的Host元素的deployOnStartup和autoDeploy属性。如果deployOnStartup和autoDeploy设置为true，则tomcat启动自动部署：当检测到新的Web应用或Web应用的更新时，会触发应用的部署(或重新部署)。二者的主要区别在于，deployOnStartup为true时，Tomcat在启动时检查Web应用，且检测到的所有Web应用视作新应用；autoDeploy为true时，Tomcat在运行时定期检查新的Web应用或Web应用的更新。除此之外，二者的处理相似。 通过配置deployOnStartup和autoDeploy可以开启虚拟主机自动部署Web应用；实际上，自动部署依赖于检查是否有新的或更改过的Web应用，而Host元素的appBase和xmlBase设置了检查Web应用更新的目录。 其中，appBase属性指定Web应用所在的目录，默认值是webapps，这是一个相对路径，代表Tomcat根目录下webapps文件夹。 xmlBase属性指定Web应用的XML配置文件所在的目录，默认值为conf/&lt;engine_name&gt;/&lt;host_name&gt;，例如第一部分的例子中，主机localhost的xmlBase的默认值是$TOMCAT_HOME/conf/Catalina/localhost。 检查Web应用更新 一个Web应用可能包括以下文件：XML配置文件，WAR包，以及一个应用目录(该目录包含Web应用的文件结构)；其中XML配置文件位于xmlBase指定的目录，WAR包和应用目录位于appBase指定的目录。 Tomcat按照如下的顺序进行扫描，来检查应用更新： A、扫描虚拟主机指定的xmlBase下的XML配置文件 B、扫描虚拟主机指定的appBase下的WAR文件 C、扫描虚拟主机指定的appBase下的应用目录 &lt;Context&gt;元素的配置 Context元素最重要的属性是docBase和path，此外reloadable属性也比较常用。 docBase指定了该Web应用使用的WAR包路径，或应用目录。需要注意的是，在自动部署场景下(配置文件位于xmlBase中)，docBase不在appBase目录中，才需要指定；如果docBase指定的WAR包或应用目录就在docBase中，则不需要指定，因为Tomcat会自动扫描appBase中的WAR包和应用目录，指定了反而会造成问题。 path指定了访问该Web应用的上下文路径，当请求到来时，Tomcat根据Web应用的 path属性与URI的匹配程度来选择Web应用处理相应请求。例如，Web应用app1的path属性是”/app1”，Web应用app2的path属性是”/app2”，那么请求/app1/index.html会交由app1来处理；而请求/app2/index.html会交由app2来处理。如果一个Context元素的path属性为””，那么这个Context是虚拟主机的默认Web应用；当请求的uri与所有的path都不匹配时，使用该默认Web应用来处理。 但是，需要注意的是，在自动部署场景下(配置文件位于xmlBase中)，不能指定path属性，path属性由配置文件的文件名、WAR文件的文件名或应用目录的名称自动推导出来。如扫描Web应用时，发现了xmlBase目录下的app1.xml，或appBase目录下的app1.WAR或app1应用目录，则该Web应用的path属性是”app1”。如果名称不是app1而是ROOT，则该Web应用是虚拟主机默认的Web应用，此时path属性推导为””。 reloadable属性指示tomcat是否在运行时监控在WEB-INF/classes和WEB-INF/lib目录下class文件的改动。如果值为true，那么当class文件改动时，会触发Web应用的重新加载。在开发环境下，reloadable设置为true便于调试；但是在生产环境中设置为true会给服务器带来性能压力，因此reloadable参数的默认值为false。 下面来看自动部署时，xmlBase下的XML配置文件app1.xml的例子： &lt;Context docBase=&quot;D:\\Program Files\\app1.war&quot; reloadable=&quot;true&quot;/&gt; 在该例子中，docBase位于Host的appBase目录之外；path属性没有指定，而是根据app1.xml自动推导为”app1”；由于是在开发环境下，因此reloadable设置为true，便于开发调试。 自动部署举例 最典型的自动部署，就是当我们安装完Tomcat后，$TOMCAT_HOME/webapps目录下有如下文件夹： 当我们启动Tomcat后，可以使用http://localhost:8080/来访问Tomcat，其实访问的就是ROOT对应的Web应用；我们也可以通过http://localhost:8080/docs来访问docs应用，同理我们可以访问examples/host-manager/manager这几个Web应用。 （3）server.xml中静态部署Web应用 除了自动部署，我们也可以在server.xml中通过&lt;context&gt;元素静态部署Web应用。静态部署与自动部署是可以共存的。在实际应用中，并不推荐使用静态部署，因为server.xml 是不可动态重加载的资源，服务器一旦启动了以后，要修改这个文件，就得重启服务器才能重新加载。而自动部署可以在Tomcat运行时通过定期的扫描来实现，不需要重启服务器。 server.xml中使用Context元素配置Web应用，Context元素应该位于Host元素中。举例如下： &lt;Context path=&quot;/&quot; docBase=&quot;D:\\Program Files \\app1.war&quot; reloadable=&quot;true&quot;/&gt; docBase：静态部署时，docBase可以在appBase目录下，也可以不在；本例中，docBase不在appBase目录下。 path：静态部署时，可以显式指定path属性，但是仍然受到了严格的限制：只有当自动部署完全关闭(deployOnStartup和autoDeploy都为false)或docBase不在appBase中时，才可以设置path属性。在本例中，docBase不在appBase中，因此path属性可以设置。 reloadable属性的用法与自动部署时相同。 核心组件的关联1、整体关系 核心组件之间的整体关系，在上一部分有所介绍，这里总结一下： Server元素在最顶层，代表整个Tomcat容器；一个Server元素中可以有一个或多个Service元素。 Service在Connector和Engine外面包了一层，把它们组装在一起，对外提供服务。一个Service可以包含多个Connector，但是只能包含一个Engine；Connector接收请求，Engine处理请求。 Engine、Host和Context都是容器，且 Engine包含Host，Host包含Context。每个Host组件代表Engine中的一个虚拟主机；每个Context组件代表在特定Host上运行的一个Web应用。 2、如何确定请求由谁处理？ 当请求被发送到Tomcat所在的主机时，如何确定最终哪个Web应用来处理该请求呢？ （1）根据协议和端口号选定Service和Engine Service中的Connector组件可以接收特定端口的请求，因此，当Tomcat启动时，Service组件就会监听特定的端口。在第一部分的例子中，Catalina这个Service监听了8080端口（基于HTTP协议）和8009端口（基于AJP协议）。当请求进来时，Tomcat便可以根据协议和端口号选定处理请求的Service；Service一旦选定，Engine也就确定。 通过在Server中配置多个Service，可以实现通过不同的端口号来访问同一台机器上部署的不同应用。 （2）根据域名或IP地址选定Host Service确定后，Tomcat在Service中寻找名称与域名/IP地址匹配的Host处理该请求。如果没有找到，则使用Engine中指定的defaultHost来处理该请求。在第一部分的例子中，由于只有一个Host（name属性为localhost），因此该Service/Engine的所有请求都交给该Host处理。 （3）根据URI选定Context/Web应用 这一点在Context一节有详细的说明：Tomcat根据应用的 path属性与URI的匹配程度来选择Web应用处理相应请求，这里不再赘述。 （4）举例 以请求http://localhost:8080/app1/index.html为例，首先通过协议和端口号（http和8080）选定Service；然后通过主机名（localhost）选定Host；然后通过uri（/app1/index.html）选定Web应用。 3、如何配置多个服务 通过在Server中配置多个Service服务，可以实现通过不同的端口号来访问同一台机器上部署的不同Web应用。 在server.xml中配置多服务的方法非常简单，分为以下几步： （1）复制&lt;Service&gt;元素，放在当前&lt;Service&gt;后面。 （2）修改端口号：根据需要监听的端口号修改&lt;Connector&gt;元素的port属性；必须确保该端口没有被其他进程占用，否则Tomcat启动时会报错，而无法通过该端口访问Web应用。 以Win7为例，可以用如下方法找出某个端口是否被其他进程占用：netstat -aon|findstr “8081″发现8081端口被PID为2064的进程占用，tasklist |findstr “2064″发现该进程为FrameworkService.exe(这是McAfee杀毒软件的进程)。 （3）修改Service和Engine的name属性 （4）修改Host的appBase属性（如webapps2） （5）Web应用仍然使用自动部署 （6）将要部署的Web应用(WAR包或应用目录)拷贝到新的appBase下。 以第一部分的server.xml为例，多个Service的配置如下： 1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version='1.0' encoding='utf-8'?&gt;&lt;Server port=\"8005\" shutdown=\"SHUTDOWN\"&gt; &lt;Listener className=\"org.apache.catalina.startup.VersionLoggerListener\" /&gt; &lt;Listener className=\"org.apache.catalina.core.AprLifecycleListener\" SSLEngine=\"on\" /&gt; &lt;Listener className=\"org.apache.catalina.core.JasperListener\" /&gt; &lt;Listener className=\"org.apache.catalina.core.JreMemoryLeakPreventionListener\" /&gt; &lt;Listener className=\"org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\" /&gt; &lt;Listener className=\"org.apache.catalina.core.ThreadLocalLeakPreventionListener\" /&gt; &lt;GlobalNamingResources&gt; &lt;Resource name=\"UserDatabase\" auth=\"Container\" type=\"org.apache.catalina.UserDatabase\" description=\"User database that can be updated and saved\" factory=\"org.apache.catalina.users.MemoryUserDatabaseFactory\" pathname=\"conf/tomcat-users.xml\" /&gt; &lt;/GlobalNamingResources&gt; &lt;Service name=\"Catalina\"&gt; &lt;Connector port=\"8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt; &lt;Connector port=\"8009\" protocol=\"AJP/1.3\" redirectPort=\"8443\" /&gt; &lt;Engine name=\"Catalina\" defaultHost=\"localhost\"&gt; &lt;Realm className=\"org.apache.catalina.realm.LockOutRealm\"&gt; &lt;Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/&gt; &lt;/Realm&gt; &lt;Host name=\"localhost\" appBase=\"/opt/project/webapps\" unpackWARs=\"true\" autoDeploy=\"true\"&gt; &lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log.\" suffix=\".txt\" pattern=\"%h %l %u %t &amp;quot;%r&amp;quot; %s %b\" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt; &lt;Service name=\"Catalina2\"&gt; &lt;Connector port=\"8084\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt; &lt;Connector port=\"8010\" protocol=\"AJP/1.3\" redirectPort=\"8443\" /&gt; &lt;Engine name=\"Catalina2\" defaultHost=\"localhost\"&gt; &lt;Realm className=\"org.apache.catalina.realm.LockOutRealm\"&gt; &lt;Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/&gt; &lt;/Realm&gt; &lt;Host name=\"localhost\" appBase=\"/opt/project/webapps2\" unpackWARs=\"true\" autoDeploy=\"true\"&gt; &lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log.\" suffix=\".txt\" pattern=\"%h %l %u %t &amp;quot;%r&amp;quot; %s %b\" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; 再将原webapps下的docs目录拷贝到webapps2中，则通过如下两个接口都可以访问docs应用： http://localhost:8080/docs/ http://localhost:8084/docs/ 其他组件除核心组件外，server.xml中还可以配置很多其他组件。下面只介绍第一部分例子中出现的组件，如果要了解更多内容，可以查看Tomcat官方文档。 1、Listener 123456&lt;Listener className=\"org.apache.catalina.startup.VersionLoggerListener\" /&gt;&lt;Listener className=\"org.apache.catalina.core.AprLifecycleListener\" SSLEngine=\"on\" /&gt;&lt;Listener className=\"org.apache.catalina.core.JasperListener\" /&gt;&lt;Listener className=\"org.apache.catalina.core.JreMemoryLeakPreventionListener\" /&gt;&lt;Listener className=\"org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\" /&gt;&lt;Listener className=\"org.apache.catalina.core.ThreadLocalLeakPreventionListener\" /&gt; Listener(即监听器)定义的组件，可以在特定事件发生时执行特定的操作；被监听的事件通常是Tomcat的启动和停止。 监听器可以在Server、Engine、Host或Context中，本例中的监听器都是在Server中。实际上，本例中定义的6个监听器，都只能存在于Server组件中。监听器不允许内嵌其他组件。 监听器需要配置的最重要的属性是className，该属性规定了监听器的具体实现类，该类必须实现了org.apache.catalina.LifecycleListener接口。 下面依次介绍例子中配置的监听器： VersionLoggerListener：当Tomcat启动时，该监听器记录Tomcat、Java和操作系统的信息。该监听器必须是配置的第一个监听器。 AprLifecycleListener：Tomcat启动时，检查APR库，如果存在则加载。APR，即Apache Portable Runtime，是Apache可移植运行库，可以实现高可扩展性、高性能，以及与本地服务器技术更好的集成。 JasperListener：在Web应用启动之前初始化Jasper，Jasper是JSP引擎，把JVM不认识的JSP文件解析成java文件，然后编译成class文件供JVM使用。 JreMemoryLeakPreventionListener：与类加载器导致的内存泄露有关。 GlobalResourcesLifecycleListener：通过该监听器，初始化&lt; GlobalNamingResources&gt;标签中定义的全局JNDI资源；如果没有该监听器，任何全局资源都不能使用。&lt; GlobalNamingResources&gt;将在后文介绍。 ThreadLocalLeakPreventionListener：当Web应用因thread-local导致的内存泄露而要停止时，该监听器会触发线程池中线程的更新。当线程执行完任务被收回线程池时，活跃线程会一个一个的更新。只有当Web应用(即Context元素)的renewThreadsWhenStoppingContext属性设置为true时，该监听器才有效。 2、GlobalNamingResources与Realm 第一部分的例子中，Engine组件下定义了Realm组件： 1234&lt;Realm className=\"org.apache.catalina.realm.LockOutRealm\"&gt; &lt;Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/&gt;&lt;/Realm&gt; Realm，可以把它理解成“域”；Realm提供了一种用户密码与web应用的映射关系，从而达到角色安全管理的作用。在本例中，Realm的配置使用name为UserDatabase的资源实现。而该资源在Server元素中使用GlobalNamingResources配置： 123&lt;GlobalNamingResources&gt; &lt;Resource name=\"UserDatabase\" auth=\"Container\" type=\"org.apache.catalina.UserDatabase\" description=\"User database that can be updated and saved\" factory=\"org.apache.catalina.users.MemoryUserDatabaseFactory\" pathname=\"conf/tomcat-users.xml\" /&gt;&lt;/GlobalNamingResources&gt; 3、Valve 在第一部分的例子中，Host元素内定义了Valve组件： 1&lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log.\" suffix=\".txt\" pattern=\"%h %l %u %t &amp;quot;%r&amp;quot; %s %b\" /&gt; 单词Valve的意思是“阀门”，在Tomcat中代表了请求处理流水线上的一个组件；Valve可以与Tomcat的容器(Engine、Host或Context)关联。 不同的Valve有不同的特性，下面介绍一下本例中出现的AccessLogValve。 AccessLogValve的作用是通过日志记录其所在的容器中处理的所有请求，在本例中，Valve放在Host下，便可以记录该Host处理的所有请求。AccessLogValve记录的日志就是访问日志，每天的请求会写到一个日志文件里。AccessLogValve可以与Engine、Host或Context关联；在本例中，只有一个Engine，Engine下只有一个Host，Host下只有一个Context，因此AccessLogValve放在三个容器下的作用其实是类似的。 本例的AccessLogValve属性的配置，使用的是默认的配置；下面介绍AccessLogValve中各个属性的作用： （1）className：规定了Valve的类型，是最重要的属性；本例中，通过该属性规定了这是一个AccessLogValve。 （2）directory：指定日志存储的位置，本例中，日志存储在$TOMCAT_HOME/logs目录下。 （3）prefix：指定了日志文件的前缀。 （4）suffix：指定了日志文件的后缀。通过directory、prefix和suffix的配置，在$TOMCAT_HOME/logs目录下，可以看到如下所示的日志文件。 （5）pattern：指定记录日志的格式，本例中各项的含义如下： %h：远程主机名或IP地址；如果有nginx等反向代理服务器进行请求分发，该主机名/IP地址代表的是nginx，否则代表的是客户端。后面远程的含义与之类似，不再解释。 %l：远程逻辑用户名，一律是”-”，可以忽略。 %u：授权的远程用户名，如果没有，则是”-”。 %t：访问的时间。 %r：请求的第一行，即请求方法(get/post等)、uri、及协议。 %s：响应状态，200,404等等。 %b：响应的数据量，不包括请求头，如果为0，则是””-。 例如，下面是访问日志中的一条记录 pattern的配置中，除了上述各项，还有一个非常常用的选项是%D，含义是请求处理的时间(单位是毫秒)，对于统计分析请求的处理速度帮助很大。 开发人员可以充分利用访问日志，来分析问题、优化应用。例如，分析访问日志中各个接口被访问的比例，不仅可以为需求和运营人员提供数据支持，还可以使自己的优化有的放矢；分析访问日志中各个请求的响应状态码，可以知道服务器请求的成功率，并找出有问题的请求；分析访问日志中各个请求的响应时间，可以找出慢请求，并根据需要进行响应时间的优化。 参考文档 Tomcat官方文档 《How Tomcat Works》 《深入分析Java Web技术内幕》 详解 Tomcat 配置文件 server.xml","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"tomcat","slug":"tomcat","permalink":"https://sunny0715.github.io/tags/tomcat/"}]},{"title":"JVM类加载机制","slug":"JVM类加载机制","date":"2017-12-13T02:03:36.000Z","updated":"2018-03-13T05:50:04.484Z","comments":true,"path":"2017/12/13/JVM类加载机制/","link":"","permalink":"https://sunny0715.github.io/2017/12/13/JVM类加载机制/","excerpt":"引言之前的博客说了Java虚拟机的运行时数据区域、GC算法、垃圾回收器等知识。距离深入了解还有一段距离，包括虚拟机的类加载机制、性能调优、线程并发等等还都没有涉及到，一直在看周志明的《深入理解Java虚拟机》，越深入去读发现这本书写的真的是经典，解决了自己很多的疑惑。 JVM的类加载机制。虚拟机把描述类的数据从class文件加载到内存中，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。","text":"引言之前的博客说了Java虚拟机的运行时数据区域、GC算法、垃圾回收器等知识。距离深入了解还有一段距离，包括虚拟机的类加载机制、性能调优、线程并发等等还都没有涉及到，一直在看周志明的《深入理解Java虚拟机》，越深入去读发现这本书写的真的是经典，解决了自己很多的疑惑。 JVM的类加载机制。虚拟机把描述类的数据从class文件加载到内存中，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 类加载过程类被加载到虚拟机内存中开始，到卸载出内存为止。它的生命周期分7个阶段，加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）、卸载（Unloading）。其中验证、准备、解析三个部分统称为连接（Linking）。 加载 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。 1注意：JVM中的ClassLoader类加载器加载Class发生在此阶段。后面会有描述。 连接验证验证的目的是为了确保Class文件中的字节流包含的信息符合当前虚拟机的要求，而且不会危害虚拟机自身的安全。不同的虚拟机对类验证的实现可能会有所不同，但大致都会完成以下四个阶段的验证：文件格式的验证、元数据的验证、字节码验证和符号引用验证。 文件格式验证 主要验证字节流是否符合calss文件格式的规范，如果符合则把字节流加载到方法区中进行存储。 验证文件头、主次版本等等。 元数据验证主要对字节码描述的信息进行语义分析，保证其描述符合Java语言的要求。 类是否有父类。 是否继承了不允许被继承的类（final修饰过的类）。 如果这个类不是抽象类，是否实现其父类或接口中所有要求实现的方法。 类中的字段、方法是否与父类产生矛盾（如：覆盖父类final类型的字段，或者不符合个则的方法）。 字节码验证该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。保证被校验类的方法在运行时不会做出危害虚拟机安全的事件。 符号引用验证 符号引用中通过字符串描述的全限定名是否能找到对应的类。 在指定类中是否存在符合方法的字段描述符以及简单名称所描述的方法和字段。 符号引用中的类、字段、方法的访问性（private、protected、public、default）是否可被当前类访问。 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 1、这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。 ​ 2、这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。 假设一个类变量的定义为：public static int value = 3； 那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的putstatic指令是在程序编译后，存放于类构造器（）方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。 下表列出了Java中所有基本数据类型以及reference类型的默认零值： 12注意：只设置类中的静态变量（方法区中），不包括实例变量（堆内存中），实例变量是在对象实例化的时候初始化分配值的。 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。 类或接口的解析：判断所要转化成的直接引用是对数组类型，还是普通的对象类型的引用，从而进行不同的解析。 字段解析：对字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会按照继承关系从上往下递归搜索该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束。 类方法解析：对类方法的解析与对字段解析的搜索步骤差不多，只是多了判断该方法所处的是类还是接口的步骤，而且对类方法的匹配搜索，是先搜索父类，再搜索接口。 接口方法解析：与类方法解析步骤类似，只是接口不会有父类，因此，只递归向上搜索父接口就行了。 初始化初始化是类加载过程的最后一步，到了此阶段，才真正开始执行类中定义的Java程序代码。在准备阶段，类变量已经被赋过一次系统要求的初始值，而在初始化阶段，则是根据程序员通过程序指定的主观计划去初始化类变量和其他资源，或者可以从另一个角度来表达：初始化阶段是执行类构造器()方法的过程。 执行类构造器。 初始化静态变量、静态块中的数据等（一个类加载器只会初始化一次）。 子类的调用前保证父类的被调用。 12注意：&lt;clinit&gt;是线程安全的，执行&lt;clinit&gt;的线程需要先获取锁才能进行初始化操作，保证只有一个线程能执行&lt;clinit&gt;(利用此特性可以实现线程安全的懒汉单例模式)。如果在一个类的&lt;clinit&gt;方法中有耗时很长的操作，那就可能造成多个线程阻塞，在实际应用中这种阻塞往往是很隐蔽的。 类加载器类被加载进虚拟机是由类加载器（ClassLoader）来完成的。类加载器虽然只用于实现类的加载动作，但它在Java程序中起到的作用却远远不限于类的加载阶段。对于任意一个类，都需要由它的类加载器和这个类本身一同确定其在就Java虚拟机中的唯一性，也就是说，即使两个类来源于同一个Class文件，只要加载它们的类加载器不同，那这两个类就必定不相等。这里的“相等”包括了代表类的Class对象的equals（）、isAssignableFrom（）、isInstance（）等方法的返回结果，也包括了使用instanceof关键字对对象所属关系的判定结果。只有在两个类被同一个类加载器加载的前提下，比较才有意义。否则，即使两个类来自同一个class文件，被同一个JVM加载，但是加载它们的类加载器不同，则这两个类就不相等。这就相当于两个命名空间中的等价类LoaderA::C和LoaderB::C。 类加载器的种类从Java虚拟机的角度来分的话，ClassLoader分为启动类加载器（Bootstrap ClassLoader）和其它的加载器。其中Bootstrap ClassLoader负责加载Java的核心类，该类加载器使用C++语言实现，属于虚拟机自身的一部分。而其它类加载器独立于JVM外部，并且全部继承自抽象类java.lang.ClassLoader。 站在Java开发人员的角度来分的话，ClassLoader分为： 启动类加载器（Bootstrap ClassLoader） 负责加载JAVA_HOME\\lib目录中并且能被虚拟机识别的类库到JVM内存中（如rt.jar，所有的java.*开头的类均被Bootstrap ClassLoader加载），如果名称不符合的类库即使放在lib目录中也不会被加载。该类加载器无法被Java程序直接引用。 扩展类加载器（Extension ClassLoader） 该加载器主要是负责加载JDK\\jre\\lib\\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类）。该加载器可以被开发者直接使用。 应用程序类加载器（Application ClassLoader） 该类加载器也称为系统类加载器（System ClassLoader），它负责加载用户类路径（Classpath）上所指定的类库，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 除此之外，还有自定义的类加载器，它们之间的层次关系被称为类加载器的双亲委派模型。该模型要求除了顶层的启动类加载器外，其余的类加载器都应该有自己的父类加载器，而这种父子关系一般通过组合（Composition）关系来实现，而不是通过继承（Inheritance）。 双亲委派模型 如上图所示的类加载器之间的这种层次关系，就称为类加载器的双亲委派模型（Parent Delegation Model）。该模型要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过组合（Composition）关系来复用父加载器的代码。 双亲委派模型的工作过程为：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的加载器都是如此，因此所有的类加载请求都会传给顶层的启动类加载器，只有当父加载器反馈自己无法完成该加载请求（该加载器的搜索范围中没有找到对应的类）时，子加载器才会尝试自己去加载。 使用这种模型来组织类加载器之间的关系的好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系。例如java.lang.Object类，无论哪个类加载器去加载该类，最终都是由启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。否则的话，如果不使用该模型的话，如果用户自定义一个java.lang.Object类且存放在classpath中，那么系统中将会出现多个Object类，应用程序也会变得很混乱。如果我们自定义一个rt.jar中已有类的同名Java类，会发现JVM可以正常编译，但该类永远无法被加载运行。 在rt.jar包中的java.lang.ClassLoader类中，我们可以查看类加载实现过程的代码，具体源码如下： 123456789101112131415161718192021222324252627282930protected synchronized Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; // First, check if the class has already been loaded //首先检查请求的类是否已经被加载过 Class c = findLoadedClass(name); if (c == null) &#123; try &#123; //委派父类加载器加载 if (parent != null) &#123; c = parent.loadClass(name, false); //委派启动类加载器加载 &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; //父类加载器无法完成类加载请求 &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; //本身类加载器进行类加载 if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; 通过上面代码可以看出，双亲委派模型是通过loadClass()方法来实现的，根据代码以及代码中的注释可以很清楚地了解整个过程其实非常简单：先检查是否已经被加载过，如果没有则调用父加载器的loadClass()方法，如果父加载器为空则默认使用启动类加载器作为父加载器。如果父类加载器加载失败，则先抛出ClassNotFoundException，然后再调用自己的findClass()方法进行加载。 注意，双亲委派模型是Java设计者推荐给开发者的类加载器的实现方式，并不是强制规定的。大多数的类加载器都遵循这个模型，但是JDK中也有较大规模破坏双亲模型的情况，例如线程上下文类加载器（Thread Context ClassLoader）的出现。 总结整个类加载过程中，除了在加载阶段用户应用程序可以自定义类加载器参与之外，其余所有的动作完全由虚拟机主导和控制。到了初始化才开始执行类中定义的Java程序代码，但这里的执行代码只是个开端，它仅限于（）方法。类加载过程中主要是将Class文件（准确地讲，应该是类的二进制字节流）加载到虚拟机内存中，真正执行字节码的操作，在加载完成后才真正开始。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"https://sunny0715.github.io/tags/jvm/"}]},{"title":"UML工具-PowerDesigner设计数据库","slug":"UML工具-PowerDesigner设计数据库","date":"2017-12-06T07:47:06.000Z","updated":"2018-03-13T05:53:11.889Z","comments":true,"path":"2017/12/06/UML工具-PowerDesigner设计数据库/","link":"","permalink":"https://sunny0715.github.io/2017/12/06/UML工具-PowerDesigner设计数据库/","excerpt":"引言在数据库的开发设计中，PowerDesiger（PD）是一个较为常用的UML工具。PowerDesiger为各类数据模型提供了直观的符号表示，不仅使设计人员能更方便、更快捷地使非计算机专业技术人员展示数据库设计和应用系统设计，使系统设计人员与使用系统的业务人员更易于相互理解和交流，同时也使项目组内的交流更为直观、准确，更便于协调工作，从而加速系统的设计和开发过程。PowerDesiger设计完成后的数据库可直接生成SQL语句。","text":"引言在数据库的开发设计中，PowerDesiger（PD）是一个较为常用的UML工具。PowerDesiger为各类数据模型提供了直观的符号表示，不仅使设计人员能更方便、更快捷地使非计算机专业技术人员展示数据库设计和应用系统设计，使系统设计人员与使用系统的业务人员更易于相互理解和交流，同时也使项目组内的交流更为直观、准确，更便于协调工作，从而加速系统的设计和开发过程。PowerDesiger设计完成后的数据库可直接生成SQL语句。 使用ODBC连接MySQL准备工作PowerDesigner本身是32位的程序（特别重要），故不管在32位或者64位操作系统中，都需要安装32位的MySQL Connector /ODBC。 MySQL Connector /ODBC下载地址：https://dev.mysql.com/downloads/connector/odbc/ 连接数据库 安装完ODBC之后，打开PowerDesigner，新建一个Model，File—&gt;New Model 选择工具栏中的Database—&gt; Update Model from Database，如下图 打开配置对话框，选择[Using a data source]，点击输入框后的图标 配置ODBC数据源 说明：这里提供了ANSI和Unicode两种字符集版本的Driver，Unicode提供更丰富的字符集，一般推荐使用Unicode。 点击完成，配置连接信息。 123456789说明：Data Source Name：指定当前配置的ODBC数据源名称，可随意填写。Description：指定ODBC数据源的描述信息，可根据用途随意填写。TCP/IP Server：采用TCP/IP协议连接服务器，如果是本地填写localhost或者127.0.0.1（根据实际MySQL用户情况选择），如果是远程服务器则填写相应IP地址即可。Port：默认3306，根据实际MySQL的端口设置填写。lNamed Pipe：命名管道方式连接，只适用于widows下的本地连接。连接性能比TCP/IP方式更高，更安全。请按照MySQL的配置文件my.ini中的socket参数指定的值填写，如果没有设置则默认为MySQL（但是目前为止这种方式我还没有测试成功）。User：数据库用户名。Password：数据库密码。Database：数据库中的database。 信息输入完之后可以选择Test测试配置是否正确，点击OK就结束了配置。 到这里已经可以连接数据库了。 设计数据库如果在已有的数据库上需要设计和修改，先取消所有表，再选择需要设计或修改的数据库，选择表，点击ok。 连接后的UML如下，可以新建和修改表 同时可对表进行主外键设计，现在主外键已经很少用到了。 双击表之间的连接线，点击Joins 点击【确定】按钮，即可如我们所愿： 生成建表语句点击Database—&gt;Generate Database 点击【确定】按钮之后，可以在桌面上找到shiro.sql这样的一个文件，打开，即可看到建表语句： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/*==============================================================*//* DBMS name: MySQL 5.0 *//* Created on: 2017.12.6 17:22:25 *//*==============================================================*/drop table if exists shiro.u_permission;drop table if exists shiro.u_role;drop table if exists shiro.u_role_permission;drop table if exists shiro.u_user;drop table if exists shiro.u_user_role;/*==============================================================*//* User: shiro *//*==============================================================*/create user shiro;/*==============================================================*//* Table: u_permission *//*==============================================================*/create table shiro.u_permission( id bigint(20) not null auto_increment, url national varchar(256) comment 'url地址', name national varchar(64) comment 'url描述', primary key (id));/*==============================================================*//* Table: u_role *//*==============================================================*/create table shiro.u_role( id bigint(20) not null auto_increment, name national varchar(32) comment '角色名称', type national varchar(10) comment '角色类型', primary key (id));/*==============================================================*//* Table: u_role_permission *//*==============================================================*/create table shiro.u_role_permission( rid bigint(20) comment '角色ID', pid bigint(20) comment '权限ID');/*==============================================================*//* Table: u_user *//*==============================================================*/create table shiro.u_user( id bigint(20) not null auto_increment, nickname national varchar(20) comment '用户昵称', email national varchar(128) comment '邮箱|登录帐号', pswd national varchar(32) comment '密码', create_time datetime comment '创建时间', last_login_time datetime comment '最后登录时间', status bigint(1) default 1 comment '1:有效，0:禁止登录', primary key (id));/*==============================================================*//* Table: u_user_role *//*==============================================================*/create table shiro.u_user_role( uid bigint(20) comment '用户ID', rid bigint(20) comment '角色ID');alter table shiro.u_user_role add constraint FK_Reference_1 foreign key (uid) references shiro.u_user (id) on delete restrict on update restrict; 得到SQL语句后可直接导入到数据库。由此我们设计数据库已经完成。 总结这里只是简单介绍了PowerDesigner进行数据库模型设计，自动生成SQL语句等功能。PowerDesigner还有很多技巧和功能在摸索中。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"tool","slug":"tool","permalink":"https://sunny0715.github.io/tags/tool/"},{"name":"mysql","slug":"mysql","permalink":"https://sunny0715.github.io/tags/mysql/"}]},{"title":"Java String intern方法","slug":"Java-String-intern方法","date":"2017-12-01T02:27:19.000Z","updated":"2018-03-13T05:48:34.428Z","comments":true,"path":"2017/12/01/Java-String-intern方法/","link":"","permalink":"https://sunny0715.github.io/2017/12/01/Java-String-intern方法/","excerpt":"引言String类我们经常使用，但是它的intern()方法之前还真的不太了解，通过谷歌百度一番之后终于搞明白了。 intern()方法设计的初衷，就是重用String对象，以节省内存消耗。","text":"引言String类我们经常使用，但是它的intern()方法之前还真的不太了解，通过谷歌百度一番之后终于搞明白了。 intern()方法设计的初衷，就是重用String对象，以节省内存消耗。 案例123String str1 = new String(\"rainbow\") + new String(\"horse\");System.out.println(str1.intern() == str1);System.out.println(str1 == \"rainbowhorse\"); 在JDK1.7下输出结果为： 12truetrue 再将上面的例子加上一行代码： 1234String str2 = \"rainbowhorse\"; //新加的一行代码，其余不变 String str1 = new String(\"rainbow\") + new String(\"horse\");System.out.println(str1.intern() == str1);System.out.println(str1 == \"rainbowhorse\"); 再运行，结果为： 12falsefalse 在JVM运行时数据区中的方法区有一个常量池，但是发现在JDK1.6以后常量池被放置在了堆空间，因此常量池位置的不同影响到了String的intern()方法的表现。 为什么使用intern()方法就如引言所说的，intern()方法设计的初衷，就是重用String对象，以节省内存消耗。下面通过例子来说明： 12345678910111213141516171819202122public class Test &#123; static final int MAX = 100000; static final String[] arr = new String[MAX]; public static void main(String[] args) throws Exception &#123; // 为长度为10的Integer数组随机赋值 Integer[] sample = new Integer[10]; Random random = new Random(1000); for (int i = 0; i &lt; sample.length; i++) &#123; sample[i] = random.nextInt(); &#125; // 记录程序开始时间 long t = System.currentTimeMillis(); // 使用/不使用intern方法为10万个String赋值，值来自于Integer数组的10个数 for (int i = 0; i &lt; MAX; i++) &#123; arr[i] = new String(String.valueOf(sample[i % sample.length])); // arr[i] = new String(String.valueOf(sample[i % sample.length])).intern(); &#125; System.out.println((System.currentTimeMillis() - t) + \"ms\"); System.gc(); &#125;&#125; 这个主要是为了证明使用intern()比不使用intern()消耗的内存更少。 先定义一个长度为10的Integer数组，并随机为其赋值，在通过for循环为长度为10万的String对象依次赋值，这些值都来自于Integer数组。两种情况分别运行，可通过Window —&gt; Preferences –&gt; Java –&gt; Installed JREs设置JVM启动参数为-agentlib:hprof=heap=dump,format=b，将程序运行完后的hprof置于工程目录下。再通过MAT插件查看该hprof文件。 不使用intern()方法 使用intern()方法 从运行结果来看，不使用intern()的情况下，程序生成了101762个String对象，而使用了intern()方法时，程序仅生成了1772个String对象。证明了intern()节省内存的结论。 但是会发现使用了intern()方法后程序运行时间有所增加。这是因为程序中每次都是用了new String后又进行intern()操作的耗时时间，但是不使用intern()占用内存空间导致GC的时间是要远远大于这点时间的。 深入理解intern()方法JDK1.7后，常量池被放入到堆空间中，这导致intern()函数的功能不同。这点很重要。 看看下面代码，这个例子是网上流传较广的一个例子，我也是照抄过来的。 123456789String s = new String(\"1\"); s.intern(); String s2 = \"1\"; System.out.println(s == s2); String s3 = new String(\"1\") + new String(\"1\"); s3.intern(); String s4 = \"11\"; System.out.println(s3 == s4); 输出结果为： 12JDK1.6以及以下：false false JDK1.7以及以上：false true 再分别调整上面代码2、3行，7、8行的顺序： 123456789String s = new String(\"1\"); String s2 = \"1\"; s.intern(); System.out.println(s == s2); String s3 = new String(\"1\") + new String(\"1\"); String s4 = \"11\"; s3.intern(); System.out.println(s3 == s4); 输出结果为： 12JDK1.6以及以下：false false JDK1.7以及以上：false false JDK1.6 在JDK1.6中所有的输出结果都是 false，因为JDK1.6以及以前版本中，常量池是放在 Perm 区（属于方法区）中的，Perm区是和堆区完全分开的。 使用引号声明的字符串都是会直接在字符串常量池中生成的，而new 出来的String对象是放在堆空间中的。所以两者的内存地址肯定是不相同的，即使调用了intern()方法也是不影响的。 intern()方法在JDK1.6中的作用是：比如String s = new String(“rainbowhorse”)，再调用s.intern()，此时返回值还是字符串”rainbowhorse”，表面上看起来好像这个方法没什么用处。但实际上，在JDK1.6中它做了个小动作：检查字符串池里是否存在”rainbowhorse”这么一个字符串，如果存在，就返回池里的字符串；如果不存在，该方法把”rainbowhorse”添加到字符串池中，然后再返回它的引用。 JDK1.7例一分析123456789String s = new String(\"1\"); s.intern(); String s2 = \"1\"; System.out.println(s == s2); String s3 = new String(\"1\") + new String(\"1\"); s3.intern(); String s4 = \"11\"; System.out.println(s3 == s4); String s = newString(“1”)，生成了常量池中的“1” 和堆空间中的字符串对象。 s.intern()，这一行的作用是s对象去常量池中寻找后发现”1”已经存在于常量池中了。 String s2 = “1”，这行代码是生成一个s2的引用指向常量池中的“1”对象。 结果就是 s 和 s2 的引用地址明显不同。因此返回了false。 String s3 = new String(“1”) + newString(“1”)，这行代码在字符串常量池中生成“1” ，并在堆空间中生成s3引用指向的对象（内容为”11”）。注意此时常量池中是没有 “11”对象的。 s3.intern()，这一行代码，是将 s3中的“11”字符串放入 String 常量池中，此时常量池中不存在“11”字符串，JDK1.6的做法是直接在常量池中生成一个 “11” 的对象。 但是在JDK1.7中，常量池中不需要再存储一份对象了，可以直接存储堆中的引用。这份引用直接指向 s3 引用的对象，也就是说s3.intern() ==s3会返回true。 String s4 = “11”， 这一行代码会直接去常量池中创建，但是发现已经有这个对象了，此时也就是指向 s3 引用对象的一个引用。因此s3 == s4返回了true。 例二分析123456789String s = new String(\"1\"); String s2 = \"1\"; s.intern(); System.out.println(s == s2); String s3 = new String(\"1\") + new String(\"1\"); String s4 = \"11\"; s3.intern(); System.out.println(s3 == s4); String s = newString(“1”)，生成了常量池中的“1” 和堆空间中的字符串对象。 String s2 = “1”，这行代码是生成一个s2的引用指向常量池中的“1”对象，但是发现已经存在了，那么就直接指向了它。 s.intern()，这一行在这里就没什么实际作用了。因为”1”已经存在了。 结果就是 s 和 s2 的引用地址明显不同。因此返回了false。 String s3 = new String(“1”) + newString(“1”)，这行代码在字符串常量池中生成“1” ，并在堆空间中生成s3引用指向的对象（内容为”11”）。注意此时常量池中是没有 “11”对象的。 String s4 = “11”， 这一行代码会直接去生成常量池中的”11”。 s3.intern()，这一行在这里就没什么实际作用了。因为”11”已经存在了。 结果就是 s3 和 s4 的引用地址明显不同。因此返回了false。 总结从JDK 1.7后，HotSpot 将常量池从永久代移到了元空间，正因为如此，JDK 1.7 后的intern方法在实现上发生了比较大的改变，JDK 1.7后，intern方法还是会先去查询常量池中是否有已经存在，如果存在，则返回常量池中的引用，这一点与之前没有区别，区别在于，如果在常量池找不到对应的字符串，则不会再将字符串拷贝到常量池，而只是在常量池中生成一个对原字符串的引用。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"}]},{"title":"JVM入门","slug":"JVM入门","date":"2017-11-20T07:27:10.000Z","updated":"2018-03-13T05:50:16.365Z","comments":true,"path":"2017/11/20/JVM入门/","link":"","permalink":"https://sunny0715.github.io/2017/11/20/JVM入门/","excerpt":"引言JVM（Java Virtual Machine）Java 虚拟机是整个 Java 平台的基石，是 Java 系统实现硬件无关与操作系统无关的关键部分，是保障用户机器免于恶意代码损害的屏障。Java开发人员不需要了解JVM是如何工作的，但是，了解 JVM 有助于我们更好的开发java 程序。近些天一直在看周志明的《深入理解Java虚拟机》这本书，这本书写的堪称经典，对于JVM的学习非常有帮助。","text":"引言JVM（Java Virtual Machine）Java 虚拟机是整个 Java 平台的基石，是 Java 系统实现硬件无关与操作系统无关的关键部分，是保障用户机器免于恶意代码损害的屏障。Java开发人员不需要了解JVM是如何工作的，但是，了解 JVM 有助于我们更好的开发java 程序。近些天一直在看周志明的《深入理解Java虚拟机》这本书，这本书写的堪称经典，对于JVM的学习非常有帮助。 运行时数据区域JVM将内存主要划分为：方法区、虚拟机栈、本地方法栈、堆、程序计数器。JVM运行时数据区如下： 程序计数器(线程私有)程序计数器是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于Java虚拟机的虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，一个处理器都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，所以程序计数器是私有空间。 此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 Java虚拟机栈(线程私有)生命周期与线程相同。“栈”就是虚拟机栈，或者说是虚拟机栈中局部变量表部分。 局部变量表存放了编译期可知的基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）和returnAddress（指向了一条字节码指令的地址）。其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用一个。 这个区域可能出现的两种异常： 一种是StackOverflowError，当前线程请求的栈深度大于虚拟机所允许的深度时，会抛出这个异常。制造这种异常很简单：将一个函数反复递归自己，最终会出现栈溢出错误（StackOverflowError）。 另一种异常是OutOfMemoryError异常，当虚拟机栈可以动态扩展时（当前大部分虚拟机都可以），如果无法申请足够多的内存就会抛出OutOfMemoryError， 本地方法栈本地方法栈与虚拟机所发挥的作用很相似，他们的区别在于虚拟机栈为执行Java代码方法服务，而本地方法栈是为Native方法服务。与虚拟机栈一样，本地方法栈也会抛出StackOverflowError和OutOfMemoryError异常。 Java堆(线程共享区域)Java堆是Java虚拟机所管理内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此区域内存的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。在Java虚拟机规范中的描述就是：所有对象实例及数组都要在堆上分配。随着JIT编译器的发展，所有对象在堆上分配渐渐变得不那么“绝对”了。 Java堆是垃圾收集器管理的主要区域。由于现在的收集器基本上采用的都是分代收集算法，所有Java堆可以细分为：新生代和老年代。在细致分就是把新生代分为：Eden空间、From Survivor空间、To Survivor空间。 Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像磁盘空间一样。 当堆无法再扩展时，会抛出OutOfMemoryError异常。 方法区(线程共享区域)方法区存放的是类信息、常量、静态变量、即时编译器编译后的代码等数据。方法区是各个线程共享区域，我们在写Java代码时，每个线程度可以访问同一个类的静态变量对象。由于使用反射机制的原因，虚拟机很难推测那个类信息不再使用，因此这块区域的回收很难。 1运行时常量池是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 另外，对这块区域主要是针对常量池回收，值得注意的是JDK1.7已经把常量池转移到堆里面了。同样，当方法区无法满足内存分配需求时，会抛出OutOfMemoryError。 GC算法Java与C++之间有一堵由内存动态分配和垃圾收集技术所围成的“高墙”，墙外面的人想进去，墙里面的人却想出来。 标记-清除算法(Mark-Sweep)最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。 它的不足主要有两个： 效率问题，标记和清除两个过程效率都不高； 空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法(Copy)为了解决效率问题，复制算法是将内存分为大小相同的两块，每次只使用其中一块。当这块内存用完了，就将还存活的对象复制到另一块内存上面。然后再把已经使用过的内存一次清理掉。这使得每次只对半个区域进行垃圾回收，内存分配时也不用考虑内存碎片情况。 但是，这代价实在是让人无法接受，需要牺牲一般的内存空间。 研究发现，大部分对象(70%~95%)都是“朝生夕死”，所以不需要安装1:1比例划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden空间和一块Survivor空间，默认比例为Eden：Survivor=8:1。新生代区域就是这么划分，每次实例在Eden和一块Survivor中分配，回收时，将存活的对象复制到剩下的另一块Survivor。这样只有10%的内存会被浪费，但是带来的效率却很高。 当剩下的Survivor内存不足时，可以去老年代内存进行分配担保。如何理解分配担保呢，其实就是，内存不足时，去老年代内存空间分配，然后等新生代内存缓过来了之后，把内存归还给老年代，保持新生代中的Eden：Survivor=8:1.另外，两个Survivor分别有自己的名称：From Survivor、To Survivor。二者身份经常调换，即有时这块内存与Eden一起参与分配，有时是另一块。因为他们之间经常相互复制。 标记整理(Mark-Compact)复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会降低。更关键的是，如果不想浪费50%的空间，就需要有额外打的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 标记整理算法很简单，就是先标记需要回收的对象，然后把所有存活的对象移动到内存的一端，最后直接清理掉边界意外的内存。这样的好处是避免了内存碎片。 分代收集算法当前商业虚拟机的垃圾收集都采用“分代收集”算法，这种算法只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 在新生代中，每次垃圾收集时都发现有大批对象死去（70%-95%），只有少量存活，那就采用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高，没有额外的空间对它进行分配担保，就必须使用“标记-清除”或“标记-整理”算法来进行回收。 HotSpot算法实现枚举根节点可达性分析判断jvm对象是否存活。GCRoots的对象做为起点，从起点开始向下搜索，搜索走过路径叫引用链，当一个对象到GCRoots没有引用链时，判断对象死亡。在jvm中，做为GCRoots的对象： 虚拟机栈(栈桢中的本地变量表)中的引用的对象; 方法区中的类静态属性引用的对象; 方法区中的常量引用的对象; 本地方法栈中JNI的引用的对象 。 从可达性分析中从GC Roots节点找引用链这个操作为例，可做为GC Roots的节点主要在全局性的引用（类如常量或类静态变量）与执行上下文（类如栈桢的本地变量表）中，现在很多应用仅仅方法区就有数百兆，如果要逐个检查这里面的应用，那么必然会逍遥很多的时间。 可达性分析对执行时间的敏感还体现在GC停顿上，因为分析工作必须要再一个能确保一致性的快照中进行这是导致GC进行时必须停顿所有Java线程（STW）的其中一个重要原因，即使在号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。 垃圾收集器如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。 图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。 概念理解 并发和并行这两个名词都是并发编程中的概念，在谈论垃圾收集器的上下文语境中，它们可以解释如下。 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。 Minor GC 和 Full GC 新生代GC（Minor GC）：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。 老年代GC（Major GC / Full GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。 吞吐量 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 Serial收集器Serial收集器是最基本、发展历史最悠久的收集器，曾经（在JDK 1.3.1之前）是虚拟机新生代收集的唯一选择。 特性：这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。Stop The World(STW) 应用场景：Serial收集器是虚拟机运行在Client模式下的默认新生代收集器。 优势：简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。 ParNew收集器 特性：ParNew收集器其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。 应用场景：ParNew收集器是许多运行在Server模式下的虚拟机中首选的新生代收集器。 很重要的原因是：除了Serial收集器外，目前只有它能与CMS收集器配合工作。在JDK 1.5时期，HotSpot推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器——CMS收集器，这款收集器是HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。不幸的是，CMS作为老年代的收集器，却无法与JDK 1.4.0中已经存在的新生代收集器Parallel Scavenge配合工作，所以在JDK 1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Serial收集器中的一个。 Serial收集器 VS ParNew收集器：ParNew收集器在单CPU的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个CPU的环境中都不能百分之百地保证可以超越Serial收集器。然而，随着可以使用的CPU的数量的增加，它对于GC时系统资源的有效利用还是很有好处的。 Parallel Scavenge收集器 特性：Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器。 应用场景：停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 对比分析： Parallel Scavenge收集器 VS CMS等收集器：Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量（Throughput）。由于与吞吐量关系密切，Parallel Scavenge收集器也经常称为“吞吐量优先”收集器。 Parallel Scavenge收集器 VS ParNew收集器：Parallel Scavenge收集器与ParNew收集器的一个重要区别是它具有自适应调节策略。 GC自适应的调节策略：Parallel Scavenge收集器有一个参数-XX:+UseAdaptiveSizePolicy。当这个参数打开之后，就不需要手工指定新生代的大小、Eden与Survivor区的比例、晋升老年代对象年龄等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomics）。 Serial Old收集器 特性：Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记－整理算法。 应用场景： Client模式Serial Old收集器的主要意义也是在于给Client模式下的虚拟机使用。 Server模式如果在Server模式下，那么它主要还有两大用途：一种用途是在JDK 1.5以及之前的版本中与Parallel Scavenge收集器搭配使用，另一种用途就是作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。 Parallel Old收集器 特性：Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。 应用场景：在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。 这个收集器是在JDK 1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old收集器外别无选择（Parallel Scavenge收集器无法与CMS收集器配合工作）。由于老年代Serial Old收集器在服务端应用性能上的“拖累”，使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew加CMS的组合“给力”。直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的应用组合。 CMS收集器 特性： CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。CMS收集器是基于“标记—清除”算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤： 初始标记（CMS initial mark）初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，需要“Stop The World”。 并发标记（CMS concurrent mark）并发标记阶段就是进行GC Roots Tracing的过程。 重新标记（CMS remark）重新标记阶段是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短，仍然需要“Stop The World”。 并发清除（CMS concurrent sweep）并发清除阶段会清除对象。 由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。 优点：CMS是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿。 缺点： CMS收集器对CPU资源非常敏感其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。CMS默认启动的回收线程数是（CPU数量+3）/ 4，也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，CMS对用户程序的影响就可能变得很大。 CMS收集器无法处理浮动垃圾CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。 由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。 CMS收集器会产生大量空间碎片CMS是一款基于“标记—清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。 空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。 G1收集器 特性：G1（Garbage-First）是一款面向服务端应用的垃圾收集器。HotSpot开发团队赋予它的使命是未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点。 并行与并发G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。 分代收集与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。 空间整合与CMS的“标记—清理”算法不同，G1从整体来看是基于“标记—整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的，但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。 可预测的停顿这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。 在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。 G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的来由）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 执行过程：G1收集器的运作大致可划分为以下几个步骤： 初始标记（Initial Marking）初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。 并发标记（Concurrent Marking）并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。 最终标记（Final Marking）最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。 筛选回收（Live Data Counting and Evacuation）筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。 CMS收集器 VS G1收集器： G1收集器几乎可以说还没有经过实际应用的考验，网络上关于G1收集器的性能测试也非常贫乏，如果现在采用的收集器没有出现任何问题，那就没有理由现在去选择G1，如果应用追求低停顿，那G1现在已经可以作为一个可尝试的选择，如果应用追求吞吐量，那么G1并不会带来什么特别的好处。 总结内存回收与垃圾收集器在很多时候都是影响系统性能、并发能力的主要因素之一，虚拟机之所以提供多种不同的收集器以及提供大量的调节参数，是因为只有根据实际应用需求，实现方式选择最优的收集方式才能获取最高的性能，没有固定收集器、参数组合，也就没有最优的调优方法，虚拟机也有没有什么必然的内存回收行为。 虽然我们是在对各个收集器进行比较，但并非为了挑选出一个最好的收集器。因为直到现在为止还没有最好的收集器出现，更加没有万能的收集器，所以我们选择的只是对具体应用最合适的收集器。这点不需要多加解释就能证明：如果有一种放之四海皆准、任何场景下都适用的完美收集器存在，那HotSpot虚拟机就没必要实现那么多不同的收集器了。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"https://sunny0715.github.io/tags/jvm/"}]},{"title":"Java生成PDF","slug":"Java生成PDF","date":"2017-11-13T07:51:53.000Z","updated":"2018-03-13T05:49:18.726Z","comments":true,"path":"2017/11/13/Java生成PDF/","link":"","permalink":"https://sunny0715.github.io/2017/11/13/Java生成PDF/","excerpt":"引言在某些业务场景中，需要提供相关的电子凭证，比如网银/支付宝中转账的电子回单，签约的电子合同、证书等。方便用户查看，下载，打印。目前常用的解决方案是，把相关数据信息，生成对应的PDF文件返回给用户。之前有写过一篇博客关于JAVA实现HTML转PDF，不同场景下的业务不同，现在需要使用PDF生成证书，这篇博客主要介绍iText的使用。 本博客项目地址：https://github.com/Sunny0715/java_pdf_demo","text":"引言在某些业务场景中，需要提供相关的电子凭证，比如网银/支付宝中转账的电子回单，签约的电子合同、证书等。方便用户查看，下载，打印。目前常用的解决方案是，把相关数据信息，生成对应的PDF文件返回给用户。之前有写过一篇博客关于JAVA实现HTML转PDF，不同场景下的业务不同，现在需要使用PDF生成证书，这篇博客主要介绍iText的使用。 本博客项目地址：https://github.com/Sunny0715/java_pdf_demo iText介绍iText是著名的开放源码的站点sourceforge一个项目，是用于生成PDF文档的一个JAVA类库。通过iText不仅可以生成PDF或rtf的文档，而且可以将XML、HTML文件转化为PDF文件。 iText 官网：http://itextpdf.com/ iText 开发文档： http://developers.itextpdf.com/developers-home iText目前有两套版本iText5和iText7。iText5应该是网上用的比较多的一个版本。iText5因为是很多开发者参与贡献代码，因此在一些规范和设计上存在不合理的地方。iText7是后来官方针对iText5的重构，两个版本差别还是挺大的。不过在实际使用中，一般用到的都比较简单，所以不用特别拘泥于使用哪个版本。比如我们在http://mvnrepository.com/中搜索iText，出来的都是iText5的依赖。 iText简单使用添加依赖 123456&lt;!-- https://mvnrepository.com/artifact/com.itextpdf/itextpdf --&gt;&lt;dependency&gt; &lt;groupId&gt;com.itextpdf&lt;/groupId&gt; &lt;artifactId&gt;itextpdf&lt;/artifactId&gt; &lt;version&gt;5.5.11&lt;/version&gt;&lt;/dependency&gt; 测试代码：JavaToPdf 12345678910111213141516171819202122232425262728293031package com.rainbowhorse.test;import com.itextpdf.text.Document;import com.itextpdf.text.DocumentException;import com.itextpdf.text.Paragraph;import com.itextpdf.text.pdf.PdfWriter;import java.io.FileNotFoundException;import java.io.FileOutputStream;/** * 不支持中文 * ClassName: JavaToPdf * @Description: TODO * @author max * @date 2017年11月13日 */public class JavaToPdf &#123; // 生成PDF路径 private static final String DEST = \"target/HelloWorld.pdf\"; public static void main(String[] args) throws FileNotFoundException, DocumentException &#123; Document document = new Document(); PdfWriter writer = PdfWriter.getInstance(document, new FileOutputStream(DEST)); document.open(); document.add(new Paragraph(\"hello world\")); document.close(); writer.close(); &#125;&#125; 运行结果 iText中文支持iText默认是不支持中文的，因此需要添加对应的中文字体,比如黑体simhei.ttf 可参考文档：http://developers.itextpdf.com/examples/font-examples/using-fonts#1227-tengwarquenya1.java 测试代码：JavaToPdfCN 12345678910111213141516171819202122232425262728293031323334353637package com.rainbowhorse.test;import com.itextpdf.text.Document;import com.itextpdf.text.DocumentException;import com.itextpdf.text.Font;import com.itextpdf.text.FontFactory;import com.itextpdf.text.Paragraph;import com.itextpdf.text.pdf.BaseFont;import com.itextpdf.text.pdf.PdfWriter;import java.io.FileNotFoundException;import java.io.FileOutputStream;/** * 支持中文 * ClassName: JavaToPdfCN * @Description: TODO * @author max * @date 2017年11月13日 */public class JavaToPdfCN &#123; // 生成PDF路径 private static final String DEST = \"target/HelloWorld_CN.pdf\"; // 中文字体（黑体） private static final String FONT = \"simhei.ttf\"; public static void main(String[] args) throws FileNotFoundException, DocumentException &#123; Document document = new Document(); PdfWriter writer = PdfWriter.getInstance(document, new FileOutputStream(DEST)); document.open(); Font font = FontFactory.getFont(FONT, BaseFont.IDENTITY_H, BaseFont.NOT_EMBEDDED); document.add(new Paragraph(\"hello world，我是rainbowhorse。\", font)); document.close(); writer.close(); &#125;&#125; 运行结果 iText-HTML渲染在一些比较复杂的PDF布局中，我们可以通过HTML去生成PDF 可参考文档：http://developers.itextpdf.com/examples/xml-worker-itext5/xml-worker-examples 添加依赖 123456&lt;!-- https://mvnrepository.com/artifact/com.itextpdf.tool/xmlworker --&gt;&lt;dependency&gt; &lt;groupId&gt;com.itextpdf.tool&lt;/groupId&gt; &lt;artifactId&gt;xmlworker&lt;/artifactId&gt; &lt;version&gt;5.5.11&lt;/version&gt;&lt;/dependency&gt; 添加模板：template.html 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\" /&gt;&lt;title&gt;Title&lt;/title&gt;&lt;style&gt;body &#123; font-family: SimHei;&#125;.red &#123; color: red;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class=\"red\"&gt;你好，rainbowhorse&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 测试代码：JavaToPdfHtml 123456789101112131415161718192021222324252627282930313233343536373839404142package com.rainbowhorse.test;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.nio.charset.Charset;import com.itextpdf.text.Document;import com.itextpdf.text.DocumentException;import com.itextpdf.text.pdf.PdfWriter;import com.itextpdf.tool.xml.XMLWorkerFontProvider;import com.itextpdf.tool.xml.XMLWorkerHelper;import com.rainbowhorse.test.util.PathUtil;/** * HTML转PDF * ClassName: JavaToPdfHtml * @Description: TODO * @author max * @date 2017年11月13日 */public class JavaToPdfHtml &#123; // 生成PDF路径 private static final String DEST = \"target/HelloWorld_CN_HTML.pdf\"; // 模板路径 private static final String HTML = PathUtil.getCurrentPath() + \"/template.html\"; // 中文字体（黑体） private static final String FONT = \"simhei.ttf\"; public static void main(String[] args) throws IOException, DocumentException &#123; Document document = new Document(); PdfWriter writer = PdfWriter.getInstance(document, new FileOutputStream(DEST)); document.open(); XMLWorkerFontProvider fontImp = new XMLWorkerFontProvider(XMLWorkerFontProvider.DONTLOOKFORFONTS); fontImp.register(FONT); XMLWorkerHelper.getInstance().parseXHtml(writer, document, new FileInputStream(HTML), null, Charset.forName(\"UTF-8\"), fontImp); document.close(); &#125;&#125; 运行结果 注意： HTML中必须使用标准的语法，标签一定需要闭合。 HTML中如果有中文，需要在样式中添加对应字体的样式。 iText-HTML-Freemarker渲染在实际使用中，HTML内容都是动态渲染的，因此我们需要加入模板引擎支持，可以使用FreeMarker/Velocity，这里使用FreeMarker举例。 添加FreeMarke依赖 123456&lt;!-- https://mvnrepository.com/artifact/org.freemarker/freemarker --&gt;&lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.19&lt;/version&gt;&lt;/dependency&gt; 添加模板：template_freemarker.html 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\" /&gt;&lt;title&gt;Title&lt;/title&gt;&lt;style&gt;body &#123; font-family: SimHei;&#125;.blue &#123; color: blue;&#125;.pos &#123; position: absolute; left: 100px; top: 150px&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class=\"blue pos\"&gt;你好，$&#123;name&#125;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 测试代码：JavaToPdfHtmlFreeMarker 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package com.rainbowhorse.test;import java.io.ByteArrayInputStream;import java.io.File;import java.io.FileOutputStream;import java.io.IOException;import java.io.StringWriter;import java.io.Writer;import java.nio.charset.Charset;import java.util.HashMap;import java.util.Map;import com.itextpdf.text.Document;import com.itextpdf.text.DocumentException;import com.itextpdf.text.pdf.PdfWriter;import com.itextpdf.tool.xml.XMLWorkerFontProvider;import com.itextpdf.tool.xml.XMLWorkerHelper;import com.rainbowhorse.test.util.PathUtil;import freemarker.template.Configuration;import freemarker.template.Template;/** * FreeMarker模板的HTML转PDF * ClassName: JavaToPdfHtmlFreeMarker * @Description: TODO * @author max * @date 2017年11月13日 */public class JavaToPdfHtmlFreeMarker &#123; // 生成PDF路径 private static final String DEST = \"target/HelloWorld_CN_HTML_FREEMARKER.pdf\"; // 模板路径 private static final String HTML = \"template_freemarker.html\"; // 中文字体（黑体） private static final String FONT = \"simhei.ttf\"; private static Configuration freemarkerCfg = null; static &#123; freemarkerCfg = new Configuration(); // freemarker的模板目录 try &#123; freemarkerCfg.setDirectoryForTemplateLoading(new File(PathUtil.getCurrentPath())); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws IOException, DocumentException &#123; Map&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;(16); data.put(\"name\", \"rainbowhorse\"); String content = JavaToPdfHtmlFreeMarker.freeMarkerRender(data, HTML); JavaToPdfHtmlFreeMarker.createPdf(content, DEST); &#125; public static void createPdf(String content, String dest) throws IOException, DocumentException &#123; Document document = new Document(); PdfWriter writer = PdfWriter.getInstance(document, new FileOutputStream(dest)); document.open(); XMLWorkerFontProvider fontImp = new XMLWorkerFontProvider(XMLWorkerFontProvider.DONTLOOKFORFONTS); fontImp.register(FONT); XMLWorkerHelper.getInstance().parseXHtml(writer, document, new ByteArrayInputStream(content.getBytes()), null, Charset.forName(\"UTF-8\"), fontImp); document.close(); &#125; /** * freemarker渲染html */ public static String freeMarkerRender(Map&lt;String, Object&gt; data, String htmlTmp) &#123; Writer out = new StringWriter(); try &#123; // 获取模板,并设置编码方式 Template template = freemarkerCfg.getTemplate(htmlTmp); template.setEncoding(\"UTF-8\"); // 合并数据模型与模板 template.process(data, out); // 将合并后的数据和模板写入到流中，这里使用的字符流 out.flush(); return out.toString(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; out.close(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; return null; &#125;&#125; 运行结果 目前为止，我们已经实现了iText通过HTML模板生成PDF的功能，但是实际应用中，我们发现iText并不能对高级的CSS样式进行解析，比如CSS中的position属性等，因此我们要引入新的组件。 Flying Saucer-CSS高级特性支持Flying Saucer is a pure-Java library for rendering arbitrary well-formed XML (or XHTML) using CSS 2.1 for layout and formatting, output to Swing panels, PDF, and images. Flying Saucer是基于iText的，支持对CSS高级特性的解析。 添加依赖 12345678910111213&lt;!-- https://mvnrepository.com/artifact/org.xhtmlrenderer/flying-saucer-pdf --&gt;&lt;dependency&gt; &lt;groupId&gt;org.xhtmlrenderer&lt;/groupId&gt; &lt;artifactId&gt;flying-saucer-pdf&lt;/artifactId&gt; &lt;version&gt;9.1.5&lt;/version&gt;&lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.xhtmlrenderer/flying-saucer-pdf-itext5 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.xhtmlrenderer&lt;/groupId&gt; &lt;artifactId&gt;flying-saucer-pdf-itext5&lt;/artifactId&gt; &lt;version&gt;9.1.5&lt;/version&gt;&lt;/dependency&gt; 添加模板：template_freemarker_fs.html 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\" /&gt;&lt;title&gt;Title&lt;/title&gt;&lt;style&gt; @page &#123; size:297mm 230mm; @top-left&#123; content:element(header-left); &#125;; @top-right &#123; content: element(header-right) &#125;; @bottom-left &#123; content: element(footer-left) &#125;; @bottom-right &#123; content: element(footer-right) &#125;; &#125;body &#123; font-family: SimHei;&#125;.color &#123; color: green;&#125;.pos &#123; position: absolute; left: 200px; top: 200px; width: 200px; font-size: 20px;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;img src=\"logo.jpg\" /&gt; &lt;div class=\"color pos\"&gt;你好，$&#123;name&#125;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 测试代码：JavaToPdfHtmlFreeMarker： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package com.rainbowhorse.test.flyingsaucer;import java.io.File;import java.io.FileOutputStream;import java.io.IOException;import java.io.StringWriter;import java.io.Writer;import java.util.HashMap;import java.util.Map;import org.xhtmlrenderer.pdf.ITextFontResolver;import org.xhtmlrenderer.pdf.ITextRenderer;import com.itextpdf.text.DocumentException;import com.itextpdf.text.pdf.BaseFont;import com.rainbowhorse.test.util.PathUtil;import freemarker.template.Configuration;import freemarker.template.Template;/** * FreeMarker模板的HTML转PDF Flying Saucer * ClassName: JavaToPdfHtmlFreeMarker * @Description: TODO * @author max * @date 2017年11月13日 */public class JavaToPdfHtmlFreeMarker &#123; // 生成PDF路径 private static final String DEST = \"target/HelloWorld_CN_HTML_FREEMARKER_FS.pdf\"; // 模板路径 private static final String HTML = \"template_freemarker_fs.html\"; // 中文字体（黑体） private static final String FONT = \"simhei.ttf\"; // 图片路径 private static final String LOGO_PATH = \"file:/\" + PathUtil.getCurrentPath() + \"/\"; private static Configuration freemarkerCfg = null; static &#123; freemarkerCfg = new Configuration(); // freemarker的模板目录 try &#123; freemarkerCfg.setDirectoryForTemplateLoading(new File(PathUtil.getCurrentPath())); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws IOException, DocumentException, com.lowagie.text.DocumentException &#123; Map&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;(16); data.put(\"name\", \"rainbowhorse\"); String content = JavaToPdfHtmlFreeMarker.freeMarkerRender(data, HTML); JavaToPdfHtmlFreeMarker.createPdf(content, DEST); &#125; /** * freemarker渲染html */ public static String freeMarkerRender(Map&lt;String, Object&gt; data, String htmlTmp) &#123; Writer out = new StringWriter(); try &#123; // 获取模板,并设置编码方式 Template template = freemarkerCfg.getTemplate(htmlTmp); template.setEncoding(\"UTF-8\"); // 合并数据模型与模板 template.process(data, out); // 将合并后的数据和模板写入到流中，这里使用的字符流 out.flush(); return out.toString(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; out.close(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; return null; &#125; public static void createPdf(String content, String dest) throws IOException, DocumentException, com.lowagie.text.DocumentException &#123; ITextRenderer render = new ITextRenderer(); ITextFontResolver fontResolver = render.getFontResolver(); fontResolver.addFont(FONT, BaseFont.IDENTITY_H, BaseFont.NOT_EMBEDDED); // 解析html生成pdf render.setDocumentFromString(content); // 解决图片相对路径的问题 render.getSharedContext().setBaseURL(LOGO_PATH); render.layout(); render.createPDF(new FileOutputStream(dest)); &#125;&#125; 运行结果 在某些场景下，HTML中的静态资源是在本地，我们可以使用render.getSharedContext().setBaseURL()加载文件资源,注意资源URL需要使用文件协议 “file://”。 对于生成的pdf页面大小，可以用css的@page属性设置。 PDF转图片在某些场景中，我们可能只需要返回图片格式的电子凭证，我们可以使用Jpedal组件，把PDF转成图片。 添加依赖 123456&lt;!-- https://mvnrepository.com/artifact/org.jpedal/jpedal-lgpl --&gt;&lt;dependency&gt; &lt;groupId&gt;org.jpedal&lt;/groupId&gt; &lt;artifactId&gt;jpedal-lgpl&lt;/artifactId&gt; &lt;version&gt;4.74b27&lt;/version&gt;&lt;/dependency&gt; 测试代码：JavaToPdfImgHtmlFreeMarker 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159package com.rainbowhorse.test.flyingsaucer;import java.awt.image.BufferedImage;import java.io.ByteArrayOutputStream;import java.io.File;import java.io.FileOutputStream;import java.io.IOException;import java.io.StringWriter;import java.io.Writer;import java.util.HashMap;import java.util.Map;import javax.imageio.ImageIO;import org.jpedal.PdfDecoder;import org.jpedal.exception.PdfException;import org.jpedal.fonts.FontMappings;import org.xhtmlrenderer.pdf.ITextFontResolver;import org.xhtmlrenderer.pdf.ITextRenderer;import com.itextpdf.text.DocumentException;import com.itextpdf.text.pdf.BaseFont;import com.rainbowhorse.test.util.PathUtil;import freemarker.template.Configuration;import freemarker.template.Template;/** * Jpedal把pdf转成图片 * ClassName: JavaToPdfImgHtmlFreeMarker * @Description: TODO * @author max * @date 2017年11月13日 */public class JavaToPdfImgHtmlFreeMarker &#123; private static final String DEST = \"target/HelloWorld_CN_HTML_FREEMARKER_FS_IMG.png\"; private static final String HTML = \"template_freemarker_fs.html\"; private static final String FONT = \"simhei.ttf\"; private static final String LOGO_PATH = \"file://\" + PathUtil.getCurrentPath() + \"/logo.png\"; private static final String IMG_EXT = \"png\"; private static Configuration freemarkerCfg = null; static &#123; freemarkerCfg = new Configuration(); // freemarker的模板目录 try &#123; freemarkerCfg.setDirectoryForTemplateLoading(new File(PathUtil.getCurrentPath())); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws IOException, DocumentException, com.lowagie.text.DocumentException &#123; Map&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;(16); data.put(\"name\", \"rainbowhorse\"); String content = JavaToPdfImgHtmlFreeMarker.freeMarkerRender(data, HTML); ByteArrayOutputStream pdfStream = JavaToPdfImgHtmlFreeMarker.createPdf(content); ByteArrayOutputStream imgSteam = JavaToPdfImgHtmlFreeMarker.pdfToImg(pdfStream.toByteArray(), 2, 1, IMG_EXT); FileOutputStream fileStream = new FileOutputStream(new File(DEST)); fileStream.write(imgSteam.toByteArray()); fileStream.close(); &#125; /** * freemarker渲染html */ public static String freeMarkerRender(Map&lt;String, Object&gt; data, String htmlTmp) &#123; Writer out = new StringWriter(); try &#123; // 获取模板,并设置编码方式 Template template = freemarkerCfg.getTemplate(htmlTmp); template.setEncoding(\"UTF-8\"); // 合并数据模型与模板 template.process(data, out); // 将合并后的数据和模板写入到流中，这里使用的字符流 out.flush(); return out.toString(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; out.close(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; return null; &#125; /** * 根据模板生成pdf文件流 */ public static ByteArrayOutputStream createPdf(String content) &#123; ByteArrayOutputStream outStream = new ByteArrayOutputStream(); ITextRenderer render = new ITextRenderer(); ITextFontResolver fontResolver = render.getFontResolver(); try &#123; fontResolver.addFont(FONT, BaseFont.IDENTITY_H, BaseFont.NOT_EMBEDDED); &#125; catch (com.lowagie.text.DocumentException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; // 解析html生成pdf render.setDocumentFromString(content); // 解决图片相对路径的问题 render.getSharedContext().setBaseURL(LOGO_PATH); render.layout(); try &#123; render.createPDF(outStream); return outStream; &#125; catch (com.lowagie.text.DocumentException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; outStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null; &#125; /** * 根据pdf二进制文件 生成图片文件 * * @param bytes * pdf二进制 * @param scaling * 清晰度 * @param pageNum * 页数 */ public static ByteArrayOutputStream pdfToImg(byte[] bytes, float scaling, int pageNum, String formatName) &#123; // 推荐的方法打开PdfDecoder PdfDecoder pdfDecoder = new PdfDecoder(true); FontMappings.setFontReplacements(); // 修改图片的清晰度 pdfDecoder.scaling = scaling; ByteArrayOutputStream out = new ByteArrayOutputStream(); try &#123; // 打开pdf文件，生成PdfDecoder对象 pdfDecoder.openPdfArray(bytes); // bytes is byte[] array with PDF // 获取第pageNum页的pdf BufferedImage img = pdfDecoder.getPageAsImage(pageNum); ImageIO.write(img, formatName, out); &#125; catch (PdfException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return out; &#125;&#125; 输出结果 Jpedal支持将指定页PDF生成图片，pdfDecoder.scaling设置图片的分辨率(不同分辨率下文件大小不同) ，支持多种图片格式，具体更多可自行研究。 总结对于电子凭证的技术方案，总结如下: HTML模板+model数据，通过freemarker进行渲染，便于维护和修改。 渲染后的HTML流，可通过Flying Saucer组件生成HTML文件流，或者生成HTML后再转成jpg文件流。 在Web项目中，对应的文件流，可以通过ContentType设置，在线查看/下载，不需通过附件服务。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"}]},{"title":"分布式下ID生成算法 SnowFlake","slug":"分布式下ID生成算法-SnowFlake","date":"2017-10-30T06:26:21.000Z","updated":"2018-03-13T05:53:41.330Z","comments":true,"path":"2017/10/30/分布式下ID生成算法-SnowFlake/","link":"","permalink":"https://sunny0715.github.io/2017/10/30/分布式下ID生成算法-SnowFlake/","excerpt":"引言在做系统开发时，系统唯一ID是我们在设计一个系统的时候经常遇到的问题，也常常为这个问题纠结。生成ID的方法有很多，适应不同的场景、需求及性能要求。所以有些比较复杂的系统会有多个ID生成策略。在这里总结一下常用到的ID生成策略。","text":"引言在做系统开发时，系统唯一ID是我们在设计一个系统的时候经常遇到的问题，也常常为这个问题纠结。生成ID的方法有很多，适应不同的场景、需求及性能要求。所以有些比较复杂的系统会有多个ID生成策略。在这里总结一下常用到的ID生成策略。 数据库自增长序列或字段最常见的方式，利用数据库，全表中唯一。如MySQL的AUTO_INCREMENT。 优点 简单，代码方便，性能可以接受。 数字ID天然排序，对分页或者需要排序的结果很有帮助。 缺点 不同数据库语法和实现不同，数据库迁移的时候或多数据库版本支持的时候需要处理。 在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。 在性能达不到要求的情况下，比较难于扩展。 如果遇见多个系统需要合并或者涉及到数据迁移会相当痛苦。 分表分库的时候会有麻烦。 优化方案针对主库单点，如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。比如：Master1 生成的是 1, 4, 7, 10，Master2生成的是2, 5, 8, 11，Master3生成的是3, 6, 9, 12。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。 UUID常见的方式。可以利用数据库也可以利用程序生成，一般来说全球唯一。 优点 简单，代码方便。 生成ID性能非常好，基本不会有性能问题。 全球唯一，在遇见数据迁移，系统数据合并，或者数据库变更等情况下，可以从容应对。 缺点 没有排序，无法保证趋势递增。 UUID往往是使用字符串存储，查询的效率比较低。 存储空间比较大，如果是海量数据库，就需要考虑存储量的问题。 传输数据量大。 不可读。 Twitter-SnowFlake算法有些时候我们希望能使用简单一些的 ID，并且希望 ID 能够按照时间有序生成，为了解决这个问题，Twitter 发明了 SnowFlake 算法，不依赖第三方介质例如 Redis、数据库，本地生成程序生成分布式自增 ID，这个 ID 只能保证在工作组中的机器生成的 ID 唯一，不能像 UUID 那样保证时空唯一。 算法原理 除了最高位bit标记为不可用以外，其余三组bit占位均可浮动，看具体的业务需求而定。默认情况下41bit的时间戳可以支持该算法使用到2082年，10bit的工作机器id可以支持1023台机器，序列号支持1毫秒产生4095个自增序列id。 SnowFlake – 时间戳这里时间戳的细度是毫秒级，建议使用64位linux系统机器，因为有vdso，gettimeofday()在用户态就可以完成操作，减少了进入内核态的损耗。 SnowFake – 工作机器ID严格意义上来说这个bit段的使用可以是进程级，机器级的话你可以使用MAC地址来唯一标示工作机器，工作进程级可以使用IP+Path来区分工作进程。如果工作机器比较少，可以使用配置文件来设置这个id是一个不错的选择，如果机器过多配置文件的维护是一个灾难性的事情。 SnowFlake – 序列号序列号就是一系列的自增id（多线程建议使用atomic），为了处理在同一毫秒内需要给多条消息分配id，若同一毫秒把序列号用完了，则 “等待至下一毫秒”。 具体实现Sequence类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197/** * Snowflake 生成的 64 位 long 类型的 ID，结构如下:&lt;br&gt; * 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 &lt;br&gt; * 1) 01 位标识，由于 long 在 Java 中是有符号的，最高位是符号位，正数是 0，负数是 1，ID 一般使用正数，所以最高位是 0&lt;br&gt; * 2) 41 位时间截(毫秒级)，注意，41 位时间截不是存储当前时间的时间截，而是存储时间截的差值(当前时间 - 开始时间)得到的值， * 开始时间截，一般是业务开始的时间，由我们程序来指定，如 SnowflakeIdWorker 中的 startTimestamp 属性。 * 41 位的时间截，可以使用 70 年: (2^41)/(1000*60*60*24*365) = 69.7 年&lt;br&gt; * 3) 10 位的数据机器位，可以部署在 1024 个节点，包括 5 位 datacenterId 和 5 位 workerId&lt;br&gt; * 4) 12 位序列，毫秒内的计数，12 位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生 4096 个 ID 序号&lt;br&gt; * * SnowFlake 的优点是，整体上按照时间自增排序，并且整个分布式系统内不会产生 ID 碰撞(由数据中心 ID 和机器 ID 作区分)，并且效率较 高，经测试，SnowFlake 每秒能够产生约 26 万个 ID。 */public class Sequence &#123; /** 开始时间截 */ private final long twepoch = 1288834974657L; /** 机器id所占的位数 */ private final long workerIdBits = 5L; /** 数据标识id所占的位数 */ private final long datacenterIdBits = 5L; /** 支持的最大机器id，结果是31 (这个移位算法可以很快的计算出几位二进制数所能表示的最大十进制数) */ private final long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); /** 支持的最大数据标识id，结果是31 */ private final long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); /** 序列在id中占的位数 */ private final long sequenceBits = 12L; /** 机器ID向左移12位 */ private final long workerIdShift = sequenceBits; /** 数据标识id向左移17位(12+5) */ private final long datacenterIdShift = sequenceBits + workerIdBits; /** 时间截向左移22位(5+5+12) */ private final long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; /** 生成序列的掩码，这里为4095 (0b111111111111=0xfff=4095) */ private final long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /** 工作机器ID(0~31) */ private long workerId; /** 数据中心ID(0~31) */ private long datacenterId; /** 毫秒内序列(0~4095) */ private long sequence = 0L; /** 上次生成ID的时间截 */ private long lastTimestamp = -1L; public Sequence() &#123; datacenterId = getDatacenterId(maxDatacenterId); workerId = getMaxWorkerId(datacenterId, maxWorkerId); &#125; public Sequence(long workerId, long datacenterId) &#123; if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format(\"worker Id can't be greater than %d or less than 0\", maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException(String.format(\"datacenter Id can't be greater than %d or less than 0\", maxDatacenterId)); &#125; this.workerId = workerId; this.datacenterId = datacenterId; &#125; /** * 获取 maxWorkerId * @param datacenterId 数据中心id * @param maxWorkerId 机器id * @return maxWorkerId */ protected static long getMaxWorkerId(long datacenterId, long maxWorkerId) &#123; StringBuilder mpid = new StringBuilder(); mpid.append(datacenterId); String name = ManagementFactory.getRuntimeMXBean().getName(); if (name != null &amp;&amp; !\"\".equals(name)) &#123; // GET jvmPid mpid.append(name.split(\"@\")[0]); &#125; //MAC + PID 的 hashcode 获取16个低位 return (mpid.toString().hashCode() &amp; 0xffff) % (maxWorkerId + 1); &#125; /** * &lt;p&gt; * 数据标识id部分 * &lt;/p&gt; * @param maxDatacenterId * @return */ protected static long getDatacenterId(long maxDatacenterId) &#123; long id = 0L; try &#123; InetAddress ip = InetAddress.getLocalHost(); NetworkInterface network = NetworkInterface.getByInetAddress(ip); if (network == null) &#123; id = 1L; &#125; else &#123; byte[] mac = network.getHardwareAddress(); if (null != mac) &#123; id = ((0x000000FF &amp; (long) mac[mac.length - 1]) | (0x0000FF00 &amp; (((long) mac[mac.length - 2]) &lt;&lt; 8))) &gt;&gt; 6; id = id % (maxDatacenterId + 1); &#125; &#125; &#125; catch (Exception e) &#123; System.err.println(\" getDatacenterId: \" + e.getMessage()); &#125; return id; &#125; /** * 获得下一个ID (该方法是线程安全的) * * @return nextId */ public synchronized long nextId() &#123; long timestamp = timeGen(); // 如果当前时间小于上一次ID生成的时间戳，说明系统时钟回退过这个时候应当抛出异常 if (timestamp &lt; lastTimestamp) &#123;// 闰秒 long offset = lastTimestamp - timestamp; if (offset &lt;= 5) &#123; try &#123; wait(offset &lt;&lt; 1); timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; throw new RuntimeException(String.format(\"Clock moved backwards. Refusing to generate id for %d milliseconds\", offset)); &#125; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; else &#123; throw new RuntimeException(String.format(\"Clock moved backwards. Refusing to generate id for %d milliseconds\", offset)); &#125; &#125; //$NON-NLS-解决跨毫秒生成ID序列号始终为偶数的缺陷$ // 如果是同一时间生成的，则进行毫秒内序列 if (lastTimestamp == timestamp) &#123; sequence = (sequence + 1) &amp; sequenceMask; // 毫秒内序列溢出 if (sequence == 0) &#123; // 阻塞到下一个毫秒,获得新的时间戳 timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123;// 时间戳改变，毫秒内序列重置 sequence = 0L; &#125; /** // 如果是同一时间生成的，则进行毫秒内序列 if (lastTimestamp == timestamp) &#123; long old = sequence; sequence = (sequence + 1) &amp; sequenceMask; // 毫秒内序列溢出 if (sequence == old) &#123; // 阻塞到下一个毫秒,获得新的时间戳 timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123;// 时间戳改变，毫秒内序列重置 sequence = ThreadLocalRandom.current().nextLong(0, 2); &#125; **/ // 上次生成ID的时间截 lastTimestamp = timestamp; // 移位并通过或运算拼到一起组成64位的ID return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) // | (datacenterId &lt;&lt; datacenterIdShift) // | (workerId &lt;&lt; workerIdShift) // | sequence; &#125; /** * 阻塞到下一个毫秒，直到获得新的时间戳 * * @param lastTimestamp 上次生成ID的时间截 * @return 当前时间戳 */ protected long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; /** * 返回以毫秒为单位的当前时间 * * @return 当前时间(毫秒) */ protected long timeGen() &#123; return SystemClock.now(); &#125;&#125; SystemClock类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 高并发场景下System.currentTimeMillis()的性能问题的优化 * System.currentTimeMillis()的调用比new一个普通对象要耗时的多（具体耗时高出多少我还没测试过，有人说是100倍左右）&lt;p&gt; * System.currentTimeMillis()之所以慢是因为去跟系统打了一次交道&lt;p&gt; * 后台定时更新时钟，JVM退出时，线程自动回收&lt;p&gt; * 10亿：43410,206,210.72815533980582%&lt;p&gt; * 1亿：4699,29,162.0344827586207%&lt;p&gt; * 1000万：480,12,40.0%&lt;p&gt; * 100万：50,10,5.0%&lt;p&gt; */public class SystemClock &#123; private final long period; private final AtomicLong now; private SystemClock(long period) &#123; this.period = period; this.now = new AtomicLong(System.currentTimeMillis()); scheduleClockUpdating(); &#125; private static class InstanceHolder &#123; public static final SystemClock INSTANCE = new SystemClock(1); &#125; private static SystemClock instance() &#123; return InstanceHolder.INSTANCE; &#125; private void scheduleClockUpdating() &#123; ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(new ThreadFactory() &#123; public Thread newThread(Runnable runnable) &#123; Thread thread = new Thread(runnable, \"System Clock\"); thread.setDaemon(true); return thread; &#125; &#125;); scheduler.scheduleAtFixedRate(new Runnable() &#123; public void run() &#123; now.set(System.currentTimeMillis()); &#125; &#125;, period, period, TimeUnit.MILLISECONDS); &#125; private long currentTimeMillis() &#123; return now.get(); &#125; public static long now() &#123; return instance().currentTimeMillis(); &#125; public static String nowDate() &#123; return new Timestamp(instance().currentTimeMillis()).toString(); &#125;&#125; 测试12345678910111213141516171819public class IdGen &#123; private static Sequence sequence = new Sequence(); /** * 使用Sequence生成主键，利用Snowflake算法 */ public static String sequenceId() &#123; long nextId = sequence.nextId(); return String.valueOf(nextId); &#125; //测试代码 public static void main(String[] args) &#123; for (int i = 0; i &lt; 1000; i++) &#123; long id = sequenceId(); //System.out.println(Long.toBinaryString(id)); System.out.println(id); &#125;&#125; SnowFlake算法可以根据自身项目的需要进行一定的修改。比如估算未来的数据中心个数，每个数据中心的机器数以及统一毫秒可以能的并发数来调整在算法中所需要的bit数。 优点 不依赖于数据库，灵活方便，且性能优于数据库。 ID按照时间在单机上是递增的。 缺点在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。 总结在项目中SnowFlake算法生成ID是第一选择，兼具性能和灵活性。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"mysql","slug":"mysql","permalink":"https://sunny0715.github.io/tags/mysql/"}]},{"title":"Java备份和还原MySQL数据库","slug":"Java备份和还原MySQL数据库","date":"2017-09-24T09:43:10.000Z","updated":"2018-03-13T05:48:49.262Z","comments":true,"path":"2017/09/24/Java备份和还原MySQL数据库/","link":"","permalink":"https://sunny0715.github.io/2017/09/24/Java备份和还原MySQL数据库/","excerpt":"引言​ 在项目中经常会用到Java程序备份和还原MySQL数据库的内容，都是大同小异，但程序也会出现各种各样的问题（运行时异常，乱码等）。实现上都是用Runtime执行MySQL的命令行工具，然后读写IO流数据；也有可能是由于使用Java的Runtime来实现备份还原功能，而由于大家的运行时环境有差异才导致代码运行不成功。在这里记录一下自己使用的工具和方法。","text":"引言​ 在项目中经常会用到Java程序备份和还原MySQL数据库的内容，都是大同小异，但程序也会出现各种各样的问题（运行时异常，乱码等）。实现上都是用Runtime执行MySQL的命令行工具，然后读写IO流数据；也有可能是由于使用Java的Runtime来实现备份还原功能，而由于大家的运行时环境有差异才导致代码运行不成功。在这里记录一下自己使用的工具和方法。 使用MySQL自带工具备份备份使用MySQL的mysqldump命令来实现，示例代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final static Logger logger = LoggerFactory.getLogger(MySQLDatabaseBackupAndRestore.class); /** * Java代码实现MySQL数据库导出 * * @param hostIP MySQL数据库所在服务器地址IP * @param userName 进入数据库所需要的用户名 * @param password 进入数据库所需要的密码 * @param savePath 数据库导出文件保存路径 * @param fileName 数据库导出文件文件名 * @param databaseName 要导出的数据库名 * @return 返回true表示导出成功，否则返回false。 * @author maxu */ public static boolean backUpDatabase(String hostIP, String userName, String password, String databaseName, String savePath, String fileName) throws InterruptedException &#123; File saveFile = new File(savePath); if (!saveFile.exists()) &#123;// 如果目录不存在 saveFile.mkdirs();// 创建文件夹 &#125; if (!savePath.endsWith(File.separator)) &#123; savePath = savePath + File.separator; &#125; PrintWriter printWriter = null; BufferedReader bufferedReader = null; try &#123; printWriter = new PrintWriter(new OutputStreamWriter(new FileOutputStream(savePath + fileName), \"utf8\")); Process process = Runtime.getRuntime().exec(\" D:\\\\DevTools\\\\MySQL\\\\MySQL5.7\\\\bin\\\\mysqldump.exe -h\" + hostIP + \" -u\" + userName + \" -p\" + password + \" --set-charset=UTF8 \" + databaseName); InputStreamReader inputStreamReader = new InputStreamReader(process.getInputStream(), \"utf8\"); bufferedReader = new BufferedReader(inputStreamReader); String line; while ((line = bufferedReader.readLine()) != null) &#123; printWriter.println(line); &#125; printWriter.flush(); if (process.waitFor() == 0) &#123;//0 表示线程正常终止。 logger.info(\"数据库已备份到——&gt;&gt;\" + savePath); return true; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (bufferedReader != null) &#123; bufferedReader.close(); &#125; if (printWriter != null) &#123; printWriter.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return false; &#125; 还原1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 final static Logger logger = LoggerFactory.getLogger(MySQLDatabaseBackupAndRestore.class); /** * Java代码实现MySQL数据库还原 * * @param hostIP MySQL数据库所在服务器地址IP * @param userName 进入数据库所需要的用户名 * @param password 进入数据库所需要的密码 * @param path 需要还原数据库文件的路径 * @param fileName 需要还原数据库文件的名称 * @param databaseName 需要还原的数据库名称 * @return 返回true表示还原成功，否则返回false。 */ public static boolean restoreDatabase(String hostIP, String userName, String password, String databaseName, String path, String fileName) throws InterruptedException &#123; OutputStream out = null; BufferedReader br = null; PrintStream ps = null; try &#123; // 调用mysql的cmd:cmd命令在后台执行，没有命令窗口出现或者一闪而过的情况 Process process = Runtime.getRuntime().exec(\"cmd /c start /b D:\\\\DevTools\\\\MySQL\\\\MySQL5.7\\\\bin\\\\mysql -h\" + hostIP + \" -u\" + userName + \" -p\" + password + \" --default-character-set=utf8 \" + databaseName); out = process.getOutputStream();//控制台的输入信息作为输出流 StringBuffer sb = new StringBuffer(\"\"); br = new BufferedReader(new InputStreamReader(new FileInputStream(path + fileName), \"utf8\")); String outStr; String line; while ((line = br.readLine()) != null) &#123; sb.append(line + \"\\r\\n\"); &#125; outStr = sb.toString(); ps = new PrintStream(out, true, \"utf8\"); ps.write(outStr.getBytes());// OutputStreamWriter writer = new OutputStreamWriter(out, \"utf8\");// writer.write(outStr); // 注：这里如果用缓冲方式写入文件的话，会导致中文乱码，用flush()方法则可以避免// writer.flush();// writer.close(); if (process.waitFor() == 0) &#123; //0 表示线程正常终止。 return true; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; finally &#123; try &#123; if (ps != null) &#123; ps.close(); &#125; if (br != null) &#123; br.close(); &#125; if (out != null)&#123; out.close(); &#125; &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; &#125; return false;&#125; 测试 1234567891011121314151617181920212223public static void main(String[] args)&#123; //数据库备份 /*try &#123; if (backUpDatabase(\"localhost\", \"root\", \"root\", \"taotao\", \"D:/\", \"taotao.sql\")) &#123; logger.info(\"数据库成功备份！！\"); &#125; else &#123; logger.info(\"数据库备份失败！！\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;*/ //数据库恢复 try &#123; if (restoreDatabase(\"localhost\", \"root\", \"root\", \"taotao\", \"D:/\", \"taotao.sql\")) &#123; logger.info(\"数据库恢复成功！！\"); &#125; else &#123; logger.info(\"数据库恢复失败！！\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 代码下载点击：下载 Windows下bat命令工作环境 Windows Server 2003 ，MySQL安装目录 D:\\DevTools\\MySQL , WinRAR 安装目录 C:\\Program Files\\WinRAR\\WinRAR.exe 备份数据存储的路径为 D:\\数据备份，好了下面开始写DOS批处理命令了。代码如下: 123456789101112131415color 9rem ---------------------数据库备份开始-----------------------@echo offset \"Ymd=%DATE:~0,4%%DATE:~5,2%%DATE:~8,2%%TIME:~0,2%%TIME:~3,2%%TIME:~6,2%\" REM 日期格式：20170924200727 md \"D:\\%ymd%\" \"D:\\DevTools\\MySQL\\MySQL5.7\\bin\\mysqldump.exe\" --opt -Q taotao -uroot -proot &gt; D:\\%Ymd%\\taotao.sqlREM ..... 这里可以添加更多的命令，要看你有多少个数据库，其中 -Q 后面是数据库名称 -p紧跟后面是密码REM echo Winrar loading... REM \"C:\\Program Files\\WinRAR\\WinRAR.exe\" a -ep1 -r -o+ -m5 -df \"D:\\数据备份\\%Ymd%.rar\" \"D:\\数据备份\\%Ymd%\" @echo onrem ---------------------数据库备份完成-----------------------pause 把上面的命令保存为 backup.bat ，双击运行，就开始备份数据了。 第 一句是建立一个变量 %Ymd% ，通过 %date% 这个系统变量得到日期，%date:~,4% 表示取日期的前面4个字符就是年份，%%date:~5,2% 表示取日期第5个字符开始的2个字符就是月份，%date:~8,2% 这个就是日期号数，如 2017-09-24 这个日期最后得到的结果是 20170924 第二句就是使用变量 %Ymd% 的值建立一个空的文件夹。 第三句开始就是使用MySQL的命令对数据库mysql进行备份，并存储在 D:\\数据备份\\%ymd% 这个文件夹下面，这里可以有很多类似的命令，备份多个数据库。 最后就是使用 WinRAR 对备份的数据进行压缩，并存储为以 %Ymd% 变量值建立的RAR文件名，同时删除备份的 %Ymd% 目录。 如果你想让系统自动定期备份，就可以通过系统的任务计划定期执行这个命令。 但是用windows下bat命令备份有一个致命缺点：备份时数据库会暂时断开。(30M断开5s左右) 总结第二种方式的缺点太致命：备份时数据库会暂时断开。 所以第一种方式将会是我们在开发中首选的方式，因为第二种方式的缺点对用户体验的影响太大了。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"mysql","slug":"mysql","permalink":"https://sunny0715.github.io/tags/mysql/"}]},{"title":"系统中功能点的版本控制","slug":"系统中功能点的版本控制","date":"2017-09-21T10:40:41.000Z","updated":"2018-03-13T05:55:03.365Z","comments":true,"path":"2017/09/21/系统中功能点的版本控制/","link":"","permalink":"https://sunny0715.github.io/2017/09/21/系统中功能点的版本控制/","excerpt":"引言​ 开发过程中我们会使用版本控制工具，如SVN、Git等。但是我们一样会遇到一种情形：在一套试题系统中，有新建题目、编辑题目、删除题目等功能，且题目可以被多个人修改，每人修改一次即是一个版本。现在的需求就是需要记录每一次修改的详细信息，每次版本之间的差异，甚至还可以版本回滚。 ​ 例如题目编号为20170919170800000061的题目被A创建，依次被B、C、D各修改了一次，此时需要比较B和A间的差异、C和B间的差异、D和C间的差异，到最后审核阶段如果B 的版本比较符合，则需要把试题版本内容回滚到B版本作为最后的版本。","text":"引言​ 开发过程中我们会使用版本控制工具，如SVN、Git等。但是我们一样会遇到一种情形：在一套试题系统中，有新建题目、编辑题目、删除题目等功能，且题目可以被多个人修改，每人修改一次即是一个版本。现在的需求就是需要记录每一次修改的详细信息，每次版本之间的差异，甚至还可以版本回滚。 ​ 例如题目编号为20170919170800000061的题目被A创建，依次被B、C、D各修改了一次，此时需要比较B和A间的差异、C和B间的差异、D和C间的差异，到最后审核阶段如果B 的版本比较符合，则需要把试题版本内容回滚到B版本作为最后的版本。 仔细分析一下题干，我们的需求是1.比较版本的差异，2.版本的回滚。之前有考虑过两种方案： 修改时在前端进行比较，只记录版本的差异，后台只需要进行存取即可。 把所有版本信息全部存储在数据库，在请求时后台进行比较差异。 第一种方案带来的问题是没法进行版本回滚，只记录下来了差异，回滚时将会是灾难，那么第二种方案才是较合适的选择。 下图是数据库中的版本修改记录 题目的所有信息全部存储在itemJson中，比较版本间的差异即是比较版本间的itemJson，现在的目标就是要提取两个版本中itemJson中的差异。 通过从网上查找资料找到了两种比较合适的方法，值得借鉴一下。 版本差异（比较Json的方法）计较两个Json(X，Y)，其中可能情况： X和Y中均有相同字段 X中存在Y中不存在的字段 Y中存在X中不存在的字段 需要掌握： 各个字段的用处和意义 字段在Map、Json、JavaBean、List、JsonString形态之间的转换 通过Map间接比较引入Maven依赖123456789101112131415&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;23.0&lt;/version&gt;&lt;/dependency&gt; 定义静态工具类123456789101112131415//处理json字符串public static &lt;T&gt; T readJsonToObject(String jsonString, TypeReference&lt;T&gt; tr) &#123; ObjectMapper objectMapper = new ObjectMapper(); if (jsonString == null || \"\".equals(jsonString)) &#123; return null; &#125; else &#123; try &#123; return (T) objectMapper.readValue(jsonString, tr); &#125; catch (Exception e) &#123; logger.debug(\"json error:\" + e); &#125; &#125; return null;&#125; 定义Map比较的工具类​ 通过google的guava表达式中的 Maps.difference(map1,map2)方法进行比较，单此方法可比较正常的Map和String内容，对于List方式的比较，同时进行了数值和list内容顺序的比较，显然不符合我们的匹配规则，所以我们要对这个方法配合List的containAll方法进一步做封装。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public static List&lt;Map&lt;String, String&gt;&gt; compareMap(Map&lt;String, Object&gt; oldVersion, Map&lt;String, Object&gt; newVersion) &#123; MapDifference&lt;String, Object&gt; difference = Maps.difference(oldVersion, newVersion); // 获取所有不同点 Map&lt;String, MapDifference.ValueDifference&lt;Object&gt;&gt; differenceMap = difference.entriesDiffering(); List&lt;Map&lt;String, String&gt;&gt; result = new ArrayList&lt;&gt;(); Iterator diffIterator = differenceMap.entrySet().iterator(); while (diffIterator.hasNext()) &#123; Map.Entry entry = (java.util.Map.Entry) diffIterator.next(); MapDifference.ValueDifference&lt;Object&gt; valueDifference = (MapDifference.ValueDifference&lt;Object&gt;) entry .getValue(); boolean isList = valueDifference.leftValue() instanceof List &amp;&amp; valueDifference.rightValue() instanceof List; boolean isMap = valueDifference.leftValue() instanceof Map &amp;&amp; valueDifference.rightValue() instanceof Map; if (!isList &amp;&amp; !isMap) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); String fieldKey = String.valueOf(entry.getKey()); // 选择题中选项内容改变 if (oldVersion.get(\"content\") != null &amp;&amp; oldVersion.get(\"name\") != null) &#123; map.put(\"fieldName\", judgeOption(oldVersion.get(\"name\").toString())); &#125; else &#123; map.put(\"fieldName\", judgeFiledName(fieldKey)); &#125; map.put(\"fieldKey\", fieldKey); map.put(\"oldValue\", judgeFiledKey(fieldKey, valueDifference.leftValue().toString())); map.put(\"newValue\", judgeFiledKey(fieldKey, valueDifference.rightValue().toString())); result.add(map); &#125; // 处理结果是否为List,则递归执行比较规则 if (valueDifference.leftValue() instanceof List &amp;&amp; valueDifference.rightValue() instanceof List) &#123; JSONArray j = JSONArray.parseArray(JSON.toJSONString(valueDifference.leftValue())); JSONArray p = JSONArray.parseArray(JSON.toJSONString(valueDifference.rightValue())); JSONObject js = new JSONObject(); JSONObject js1 = new JSONObject(); for (int i = 0; i &lt; j.size(); i++) &#123; js.put(i + \"\", j.get(i)); &#125; for (int i = 0; i &lt; p.size(); i++) &#123; js1.put(i + \"\", p.get(i)); &#125; Map&lt;String, Object&gt; requestMap = JsonUtils.readJsonToObject(js.toString(), new TypeReference&lt;Map&lt;String, Object&gt;&gt;() &#123; &#125;); Map&lt;String, Object&gt; requestMap1 = JsonUtils.readJsonToObject(js1.toString(), new TypeReference&lt;Map&lt;String, Object&gt;&gt;() &#123; &#125;); List&lt;Map&lt;String, String&gt;&gt; m = compareMap(requestMap, requestMap1); //当修改多个选项 for (int i = 0; i &lt; m.size(); i++) &#123; result.add(compareMap(requestMap, requestMap1).get(i)); &#125; &#125; // 处理结果是否为Map,则递归执行比较规则 if (valueDifference.leftValue() instanceof Map &amp;&amp; valueDifference.rightValue() instanceof Map) &#123; result.add(compareMap((Map&lt;String, Object&gt;) valueDifference.leftValue(), (Map&lt;String, Object&gt;) valueDifference.rightValue()).get(0)); &#125; &#125; // 若A中有B中不存在的值 Map&lt;String, Object&gt; entriesOnlyOnLeft = difference.entriesOnlyOnLeft(); if (entriesOnlyOnLeft != null &amp;&amp; !entriesOnlyOnLeft.isEmpty()) &#123; Iterator it = entriesOnlyOnLeft.entrySet().iterator(); while (it.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = (java.util.Map.Entry) it.next(); Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); String fieldKey = entry.getKey(); map.put(\"fieldKey\", fieldKey); map.put(\"fieldName\", judgeFiledName(fieldKey)); map.put(\"oldValue\", judgeFiledKey(fieldKey, String.valueOf(entry.getValue()))); map.put(\"newValue\", \"\"); result.add(map); &#125; &#125; // 若B中有A中不存在的值 Map&lt;String, Object&gt; onlyOnRightMap = difference.entriesOnlyOnRight(); if (onlyOnRightMap != null &amp;&amp; !onlyOnRightMap.isEmpty()) &#123; Iterator it = onlyOnRightMap.entrySet().iterator(); while (it.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = (java.util.Map.Entry) it.next(); Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); String fieldKey = String.valueOf(entry.getKey()); map.put(\"fieldKey\", fieldKey); map.put(\"fieldName\", judgeFiledName(fieldKey)); map.put(\"oldValue\", \"\"); map.put(\"newValue\", judgeFiledKey(fieldKey, String.valueOf(entry.getValue()))); result.add(map); &#125; &#125; return result; &#125; 定义静态调用方法1234567public static List&lt;Map&lt;String, String&gt;&gt; compareJSON(String jsonOld, String jsonNew) &#123; Map&lt;String, Object&gt; oldVersion = JsonUtils.readJsonToObject(jsonOld, new TypeReference&lt;Map&lt;String, Object&gt;&gt;() &#123; &#125;); Map&lt;String, Object&gt; newVersion = JsonUtils.readJsonToObject(jsonNew, new TypeReference&lt;Map&lt;String, Object&gt;&gt;() &#123; &#125;); return compareMap(oldVersion, newVersion); &#125; 返回结果经过处理后的返回结果 1234567891011121314151617181920212223242526272829303132333435363738[ &#123; \"newValue\": \"10\", \"fieldName\": \"分数\", \"fieldKey\": \"score\", \"oldValue\": \"8\" &#125;, &#123; \"newValue\": \"10\", \"fieldName\": \"教材依据\", \"fieldKey\": \"teachingMaterialBasis\", \"oldValue\": \"2\" &#125;, &#123; \"newValue\": \"2\", \"fieldName\": \"大纲依据\", \"fieldKey\": \"syllabusBasis\", \"oldValue\": \"1\" &#125;, &#123; \"newValue\": \"1.0\", \"fieldName\": \"难度系数\", \"fieldKey\": \"difficult\", \"oldValue\": \"0.4\" &#125;, &#123; \"newValue\": \"掌握\", \"fieldName\": \"能力层次\", \"fieldKey\": \"abilityLevel\", \"oldValue\": \"熟悉\" &#125;, &#123; \"newValue\": \"测试关键词\", \"fieldName\": \"关键字\", \"fieldKey\": \"keyWord\", \"oldValue\": \"个\" &#125; ] 查看所有源码点击：下载 转为JavaBean比较将itemJson字符串转化为JavaBean，比较JavaBean之前的差异。比较JavaBean间的差异可以用Javers。 引入Javers的Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;org.javers&lt;/groupId&gt; &lt;artifactId&gt;javers-core&lt;/artifactId&gt; &lt;version&gt;3.5.0&lt;/version&gt;&lt;/dependency&gt; 将itemJson转为JavaBean方式一：利用Jackson工具类JsonUtils12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class JsonUtils &#123; // 定义jackson对象 private static final ObjectMapper MAPPER = new ObjectMapper(); /** * 将对象转换成json字符串。 * &lt;p&gt;Title: pojoToJson&lt;/p&gt; * &lt;p&gt;Description: &lt;/p&gt; * @param data * @return */ public static String objectToJson(Object data) &#123; try &#123; String string = MAPPER.writeValueAsString(data); return string; &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 将json结果集转化为对象 * * @param jsonData json数据 * @param clazz 对象中的object类型 * @return */ public static &lt;T&gt; T jsonToPojo(String jsonData, Class&lt;T&gt; beanType) &#123; try &#123; T t = MAPPER.readValue(jsonData, beanType); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 将json数据转换成pojo对象list * &lt;p&gt;Title: jsonToList&lt;/p&gt; * &lt;p&gt;Description: &lt;/p&gt; * @param jsonData * @param beanType * @return */ public static &lt;T&gt;List&lt;T&gt; jsonToList(String jsonData, Class&lt;T&gt; beanType) &#123; JavaType javaType = MAPPER.getTypeFactory().constructParametricType(List.class, beanType); try &#123; List&lt;T&gt; list = MAPPER.readValue(jsonData, javaType); return list; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; Json转Pojo12Item item1 = JsonUtils.jsonToPojo(itemJsonOld, Item.class);Item item2 = JsonUtils.jsonToPojo(itemJsonNew, Item.class); 方式二：利用fastJson引入Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.1.26&lt;/version&gt;&lt;/dependency&gt; Json转Pojo12Item item1 = JSONObject.parseObject(json3, Item.class);Item item2 = JSONObject.parseObject(json4, Item.class); 利用Javers比较JavaBean1234567891011Javers j = JaversBuilder.javers().build(); Diff diff = j.compare(item1, item2); if (diff.hasChanges()) &#123; List&lt;Change&gt; changes = diff.getChanges(); for (Change change : changes) &#123; if (change instanceof ValueChange) &#123; ValueChange valChange = (ValueChange) change; System.out.println(valChange.getPropertyName() + \" -- \" + valChange.getLeft() + \"--\" + valChange.getRight()); &#125; &#125; &#125; 版本回滚​ 其实版本回滚在上面的比较中已经说了，就是把需要回滚的版本itemJson转化为JavaBean传给前台，同时生成一份最新的版本为当前版本，记录操作人、操作时间等等记录即可。需要了解及使用Gson、fastJson、Jackson的使用，及使用工具将Map、Json、JavaBean、List、JsonString对象之间状态的转换。 总结 熟悉业务。 掌握Map、Json、JavaBean、List、JsonString对象之间状态的转换。 版本需要存储所有信息便于回滚。 个人倾向使用Javers比较JavaBean进行比较版本差异。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"}]},{"title":"Spring+Mybatis之Mapper热部署","slug":"Spring-Mybatis之Mapper热部署","date":"2017-09-15T08:52:07.000Z","updated":"2018-03-20T02:45:12.021Z","comments":true,"path":"2017/09/15/Spring-Mybatis之Mapper热部署/","link":"","permalink":"https://sunny0715.github.io/2017/09/15/Spring-Mybatis之Mapper热部署/","excerpt":"引言​ Spring+Mybatis经常用，在项目中最痛苦的就是修改mapper文件的时候需要重启一下项目，每修改一次就需要重启一次项目。项目小还好，如果项目大，重启一次项目简直是要命。所以，去网上查资料看有没有办法让mybatis热部署，每次更新mapper文件不需要重启项目。 ​ 功夫不负有心人，终于找到了，这玩意只要发现mapper文件被修改，就会重新加载被修改的mapper文件。且只加载被修改的mapper文件！这个可省事了，效率又高，简直爽到爆。","text":"引言​ Spring+Mybatis经常用，在项目中最痛苦的就是修改mapper文件的时候需要重启一下项目，每修改一次就需要重启一次项目。项目小还好，如果项目大，重启一次项目简直是要命。所以，去网上查资料看有没有办法让mybatis热部署，每次更新mapper文件不需要重启项目。 ​ 功夫不负有心人，终于找到了，这玩意只要发现mapper文件被修改，就会重新加载被修改的mapper文件。且只加载被修改的mapper文件！这个可省事了，效率又高，简直爽到爆。 创建MapperRefresh刷新类在src下创建一个util包，包下面创建一个类，类名为：MapperRefresh 代码为下面的一串，注意修改下mybatis-refresh.properties 的路径。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359package com.talkweb.nets.netsTestLib.data.util;import java.io.File; import java.io.FileInputStream; import java.io.FileNotFoundException; import java.io.InputStream; import java.lang.reflect.Field; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.Properties; import java.util.Set; import org.apache.commons.lang3.StringUtils; import org.apache.ibatis.builder.xml.XMLMapperBuilder; import org.apache.ibatis.executor.ErrorContext; import org.apache.ibatis.session.Configuration; import org.apache.log4j.Logger; import org.springframework.core.NestedIOException; import org.springframework.core.io.Resource; import com.google.common.collect.Sets; /** * 刷新MyBatis Mapper XML 线程 * @author ThinkGem 这个是原著的作者，我只是直接拿来用了，原著莫怪 * @version 2016-5-29 */ public class MapperRefresh implements java.lang.Runnable &#123; public static Logger log = Logger.getLogger(MapperRefresh.class); private static String filename = \"mybatis-refresh.properties\"; //注意修改路径 private static Properties prop = new Properties(); private static boolean enabled; // 是否启用Mapper刷新线程功能 private static boolean refresh; // 刷新启用后，是否启动了刷新线程 private Set&lt;String&gt; location; // Mapper实际资源路径 private Resource[] mapperLocations; // Mapper资源路径 private Configuration configuration; // MyBatis配置对象 private Long beforeTime = 0L; // 上一次刷新时间 private static int delaySeconds; // 延迟刷新秒数 private static int sleepSeconds; // 休眠时间 private static String mappingPath; // xml文件夹匹配字符串，需要根据需要修改 static &#123; try &#123; prop.load(MapperRefresh.class.getResourceAsStream(filename)); &#125; catch (Exception e) &#123; e.printStackTrace(); System.out.println(\"Load mybatis-refresh “\"+filename+\"” file error.\"); &#125; enabled = \"true\".equalsIgnoreCase(getPropString(\"enabled\")); delaySeconds = getPropInt(\"delaySeconds\"); sleepSeconds = getPropInt(\"sleepSeconds\"); mappingPath = getPropString(\"mappingPath\"); delaySeconds = delaySeconds == 0 ? 50 : delaySeconds; sleepSeconds = sleepSeconds == 0 ? 3 : sleepSeconds; mappingPath = StringUtils.isBlank(mappingPath) ? \"mappings\" : mappingPath; log.debug(\"[enabled] \" + enabled); log.debug(\"[delaySeconds] \" + delaySeconds); log.debug(\"[sleepSeconds] \" + sleepSeconds); log.debug(\"[mappingPath] \" + mappingPath); &#125; public static boolean isRefresh() &#123; return refresh; &#125; public MapperRefresh(Resource[] mapperLocations, Configuration configuration) &#123; this.mapperLocations = mapperLocations; this.configuration = configuration; &#125; @Override public void run() &#123; beforeTime = System.currentTimeMillis(); log.debug(\"[location] \" + location); log.debug(\"[configuration] \" + configuration); if (enabled) &#123; // 启动刷新线程 final MapperRefresh runnable = this; new Thread(new java.lang.Runnable() &#123; @Override public void run() &#123; if (location == null)&#123; location = Sets.newHashSet(); log.debug(\"MapperLocation's length:\" + mapperLocations.length); for (Resource mapperLocation : mapperLocations) &#123; String s = mapperLocation.toString().replaceAll(\"\\\\\\\\\", \"/\"); s = s.substring(\"file [\".length(), s.lastIndexOf(mappingPath) + mappingPath.length()); if (!location.contains(s)) &#123; location.add(s); log.debug(\"Location:\" + s); &#125; &#125; log.debug(\"Locarion's size:\" + location.size()); &#125; try &#123; Thread.sleep(delaySeconds * 1000); &#125; catch (InterruptedException e2) &#123; e2.printStackTrace(); &#125; refresh = true; System.out.println(\"========= Enabled refresh mybatis mapper =========\"); while (true) &#123; try &#123; for (String s : location) &#123; runnable.refresh(s, beforeTime); &#125; &#125; catch (Exception e1) &#123; e1.printStackTrace(); &#125; try &#123; Thread.sleep(sleepSeconds * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;, \"MyBatis-Mapper-Refresh\").start(); &#125; &#125; /** * 执行刷新 * @param filePath 刷新目录 * @param beforeTime 上次刷新时间 * @throws NestedIOException 解析异常 * @throws FileNotFoundException 文件未找到 * @author ThinkGem */ @SuppressWarnings(&#123; \"rawtypes\", \"unchecked\" &#125;) private void refresh(String filePath, Long beforeTime) throws Exception &#123; // 本次刷新时间 Long refrehTime = System.currentTimeMillis(); // 获取需要刷新的Mapper文件列表 List&lt;File&gt; fileList = this.getRefreshFile(new File(filePath), beforeTime); if (fileList.size() &gt; 0) &#123; log.debug(\"Refresh file: \" + fileList.size()); &#125; for (int i = 0; i &lt; fileList.size(); i++) &#123; InputStream inputStream = new FileInputStream(fileList.get(i)); String resource = fileList.get(i).getAbsolutePath(); try &#123; // 清理原有资源，更新为自己的StrictMap方便，增量重新加载 String[] mapFieldNames = new String[]&#123; \"mappedStatements\", \"caches\", \"resultMaps\", \"parameterMaps\", \"keyGenerators\", \"sqlFragments\" &#125;; for (String fieldName : mapFieldNames)&#123; Field field = configuration.getClass().getDeclaredField(fieldName); field.setAccessible(true); Map map = ((Map)field.get(configuration)); if (!(map instanceof StrictMap))&#123; Map newMap = new StrictMap(StringUtils.capitalize(fieldName) + \"collection\"); for (Object key : map.keySet())&#123; try &#123; newMap.put(key, map.get(key)); &#125;catch(IllegalArgumentException ex)&#123; newMap.put(key, ex.getMessage()); &#125; &#125; field.set(configuration, newMap); &#125; &#125; // 清理已加载的资源标识，方便让它重新加载。 Field loadedResourcesField = configuration.getClass().getDeclaredField(\"loadedResources\"); loadedResourcesField.setAccessible(true); Set loadedResourcesSet = ((Set)loadedResourcesField.get(configuration)); loadedResourcesSet.remove(resource); //重新编译加载资源文件。 XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(inputStream, configuration, resource, configuration.getSqlFragments()); xmlMapperBuilder.parse(); &#125; catch (Exception e) &#123; throw new NestedIOException(\"Failed to parse mapping resource: '\" + resource + \"'\", e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; System.out.println(\"Refresh file: \" + mappingPath + StringUtils.substringAfterLast(fileList.get(i).getAbsolutePath(), mappingPath)); if (log.isDebugEnabled()) &#123; log.debug(\"Refresh file: \" + fileList.get(i).getAbsolutePath()); log.debug(\"Refresh filename: \" + fileList.get(i).getName()); &#125; &#125; // 如果刷新了文件，则修改刷新时间，否则不修改 if (fileList.size() &gt; 0) &#123; this.beforeTime = refrehTime; &#125; &#125; /** * 获取需要刷新的文件列表 * @param dir 目录 * @param beforeTime 上次刷新时间 * @return 刷新文件列表 */ private List&lt;File&gt; getRefreshFile(File dir, Long beforeTime) &#123; List&lt;File&gt; fileList = new ArrayList&lt;File&gt;(); File[] files = dir.listFiles(); if (files != null) &#123; for (int i = 0; i &lt; files.length; i++) &#123; File file = files[i]; if (file.isDirectory()) &#123; fileList.addAll(this.getRefreshFile(file, beforeTime)); &#125; else if (file.isFile()) &#123; if (this.checkFile(file, beforeTime)) &#123; fileList.add(file); &#125; &#125; else &#123; System.out.println(\"Error file.\" + file.getName()); &#125; &#125; &#125; return fileList; &#125; /** * 判断文件是否需要刷新 * @param file 文件 * @param beforeTime 上次刷新时间 * @return 需要刷新返回true，否则返回false */ private boolean checkFile(File file, Long beforeTime) &#123; if (file.lastModified() &gt; beforeTime) &#123; return true; &#125; return false; &#125; /** * 获取整数属性 * @param key * @return */ private static int getPropInt(String key) &#123; int i = 0; try &#123; i = Integer.parseInt(getPropString(key)); &#125; catch (Exception e) &#123; &#125; return i; &#125; /** * 获取字符串属性 * @param key * @return */ private static String getPropString(String key) &#123; return prop == null ? null : prop.getProperty(key); &#125; /** * 重写 org.apache.ibatis.session.Configuration.StrictMap 类 * 来自 MyBatis3.4.0版本，修改 put 方法，允许反复 put更新。 */ public static class StrictMap&lt;V&gt; extends HashMap&lt;String, V&gt; &#123; private static final long serialVersionUID = -4950446264854982944L; private String name; public StrictMap(String name, int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor); this.name = name; &#125; public StrictMap(String name, int initialCapacity) &#123; super(initialCapacity); this.name = name; &#125; public StrictMap(String name) &#123; super(); this.name = name; &#125; public StrictMap(String name, Map&lt;String, ? extends V&gt; m) &#123; super(m); this.name = name; &#125; @SuppressWarnings(\"unchecked\") public V put(String key, V value) &#123; // ThinkGem 如果现在状态为刷新，则刷新(先删除后添加) if (MapperRefresh.isRefresh()) &#123; remove(key); MapperRefresh.log.debug(\"refresh key:\" + key.substring(key.lastIndexOf(\".\") + 1)); &#125; // ThinkGem end if (containsKey(key)) &#123; throw new IllegalArgumentException(name + \" already contains value for \" + key); &#125; if (key.contains(\".\")) &#123; final String shortKey = getShortName(key); if (super.get(shortKey) == null) &#123; super.put(shortKey, value); &#125; else &#123; super.put(shortKey, (V) new Ambiguity(shortKey)); &#125; &#125; return super.put(key, value); &#125; public V get(Object key) &#123; V value = super.get(key); if (value == null) &#123; throw new IllegalArgumentException(name + \" does not contain value for \" + key); &#125; if (value instanceof Ambiguity) &#123; throw new IllegalArgumentException(((Ambiguity) value).getSubject() + \" is ambiguous in \" + name + \" (try using the full name including the namespace, or rename one of the entries)\"); &#125; return value; &#125; private String getShortName(String key) &#123; final String[] keyparts = key.split(\"\\\\.\"); return keyparts[keyparts.length - 1]; &#125; protected static class Ambiguity &#123; private String subject; public Ambiguity(String subject) &#123; this.subject = subject; &#125; public String getSubject() &#123; return subject; &#125; &#125; &#125; &#125; 重写SqlSessionFactoryBeanMyBatis有几个不太好的地方，是当实体类别名重名的时候，Mapper XML有错误的时候，系统启动时会一直等待无法正常启动（其实是加载失败后又重新加载，进入了死循环），这里重写下SqlSessionFactoryBean.java文件，解决这个问题，在这个文件里也加入启动上面写的线程类： 1、修改实体类重名的时候抛出并打印异常，否则系统会一直递归造成无法启动。2、MapperXML有错误的时候抛出并打印异常，否则系统会一直递归造成无法启动。3、加入启动MapperRefresh.java线程服务。 思路就是用我们自己重写的SqlSessionFactoryBean.class替换mybatis-spring-1.2.2.jar中的SqlSessionFactoryBean.class。 在当前项目下新建一个包：右键 src &gt; new Package &gt; org.mybatis.spring，创建SqlSessionFactoryBean.java类。 复制下面一串代码到SqlSessionFactoryBean.java，注意导入MapperRefresh正确的包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313package org.mybatis.spring;import java.io.IOException;import java.sql.SQLException;import java.util.Properties;import javax.sql.DataSource;import org.apache.ibatis.builder.xml.XMLConfigBuilder;import org.apache.ibatis.builder.xml.XMLMapperBuilder;import org.apache.ibatis.executor.ErrorContext;import org.apache.ibatis.logging.Log;import org.apache.ibatis.logging.LogFactory;import org.apache.ibatis.mapping.DatabaseIdProvider;import org.apache.ibatis.mapping.Environment;import org.apache.ibatis.plugin.Interceptor;import org.apache.ibatis.reflection.factory.ObjectFactory;import org.apache.ibatis.reflection.wrapper.ObjectWrapperFactory;import org.apache.ibatis.session.Configuration;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.apache.ibatis.transaction.TransactionFactory;import org.apache.ibatis.type.TypeAliasRegistry;import org.apache.ibatis.type.TypeHandler;import org.apache.ibatis.type.TypeHandlerRegistry;import org.mybatis.spring.transaction.SpringManagedTransactionFactory;import org.springframework.beans.factory.FactoryBean;import org.springframework.beans.factory.InitializingBean;import org.springframework.context.ApplicationEvent;import org.springframework.context.ApplicationListener;import org.springframework.context.event.ContextRefreshedEvent;import org.springframework.core.NestedIOException;import org.springframework.core.io.Resource;import org.springframework.jdbc.datasource.TransactionAwareDataSourceProxy;import org.springframework.util.Assert;import org.springframework.util.ObjectUtils;import org.springframework.util.StringUtils;import com.talkweb.nets.netsTestLib.data.util.MapperRefresh;public class SqlSessionFactoryBean implements FactoryBean&lt;SqlSessionFactory&gt;, InitializingBean, ApplicationListener&lt;ApplicationEvent&gt; &#123; private static final Log logger = LogFactory.getLog(SqlSessionFactoryBean.class); private Resource configLocation; private Resource[] mapperLocations; private DataSource dataSource; private TransactionFactory transactionFactory; private Properties configurationProperties; private SqlSessionFactoryBuilder sqlSessionFactoryBuilder = new SqlSessionFactoryBuilder(); private SqlSessionFactory sqlSessionFactory; private String environment = SqlSessionFactoryBean.class.getSimpleName(); private boolean failFast; private Interceptor[] plugins; private TypeHandler&lt;?&gt;[] typeHandlers; private String typeHandlersPackage; private Class&lt;?&gt;[] typeAliases; private String typeAliasesPackage; private Class&lt;?&gt; typeAliasesSuperType; private DatabaseIdProvider databaseIdProvider; private ObjectFactory objectFactory; private ObjectWrapperFactory objectWrapperFactory; public void setObjectFactory(ObjectFactory objectFactory) &#123; this.objectFactory = objectFactory; &#125; public void setObjectWrapperFactory(ObjectWrapperFactory objectWrapperFactory) &#123; this.objectWrapperFactory = objectWrapperFactory; &#125; public DatabaseIdProvider getDatabaseIdProvider() &#123; return this.databaseIdProvider; &#125; public void setDatabaseIdProvider(DatabaseIdProvider databaseIdProvider) &#123; this.databaseIdProvider = databaseIdProvider; &#125; public void setPlugins(Interceptor[] plugins) &#123; this.plugins = plugins; &#125; public void setTypeAliasesPackage(String typeAliasesPackage) &#123; this.typeAliasesPackage = typeAliasesPackage; &#125; public void setTypeAliasesSuperType(Class&lt;?&gt; typeAliasesSuperType) &#123; this.typeAliasesSuperType = typeAliasesSuperType; &#125; public void setTypeHandlersPackage(String typeHandlersPackage) &#123; this.typeHandlersPackage = typeHandlersPackage; &#125; public void setTypeHandlers(TypeHandler&lt;?&gt;[] typeHandlers) &#123; this.typeHandlers = typeHandlers; &#125; public void setTypeAliases(Class&lt;?&gt;[] typeAliases) &#123; this.typeAliases = typeAliases; &#125; public void setFailFast(boolean failFast) &#123; this.failFast = failFast; &#125; public void setConfigLocation(Resource configLocation) &#123; this.configLocation = configLocation; &#125; public void setMapperLocations(Resource[] mapperLocations) &#123; this.mapperLocations = mapperLocations; &#125; public void setConfigurationProperties(Properties sqlSessionFactoryProperties) &#123; this.configurationProperties = sqlSessionFactoryProperties; &#125; public void setDataSource(DataSource dataSource) &#123; if ((dataSource instanceof TransactionAwareDataSourceProxy)) &#123; this.dataSource = ((TransactionAwareDataSourceProxy) dataSource).getTargetDataSource(); &#125; else this.dataSource = dataSource; &#125; public void setSqlSessionFactoryBuilder(SqlSessionFactoryBuilder sqlSessionFactoryBuilder) &#123; this.sqlSessionFactoryBuilder = sqlSessionFactoryBuilder; &#125; public void setTransactionFactory(TransactionFactory transactionFactory) &#123; this.transactionFactory = transactionFactory; &#125; public void setEnvironment(String environment) &#123; this.environment = environment; &#125; public void afterPropertiesSet() throws Exception &#123; Assert.notNull(this.dataSource, \"Property 'dataSource' is required\"); Assert.notNull(this.sqlSessionFactoryBuilder, \"Property 'sqlSessionFactoryBuilder' is required\"); this.sqlSessionFactory = buildSqlSessionFactory(); &#125; protected SqlSessionFactory buildSqlSessionFactory() throws IOException &#123; XMLConfigBuilder xmlConfigBuilder = null; Configuration configuration; if (this.configLocation != null) &#123; xmlConfigBuilder = new XMLConfigBuilder(this.configLocation.getInputStream(), null, this.configurationProperties); configuration = xmlConfigBuilder.getConfiguration(); &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Property 'configLocation' not specified, using default MyBatis Configuration\"); &#125; configuration = new Configuration(); configuration.setVariables(this.configurationProperties); &#125; if (this.objectFactory != null) &#123; configuration.setObjectFactory(this.objectFactory); &#125; if (this.objectWrapperFactory != null) &#123; configuration.setObjectWrapperFactory(this.objectWrapperFactory); &#125; if (StringUtils.hasLength(this.typeAliasesPackage)) &#123; String[] typeAliasPackageArray = StringUtils.tokenizeToStringArray(this.typeAliasesPackage, \",; \\t\\n\"); for (String packageToScan : typeAliasPackageArray) &#123; // 修改处：ThinkGem 修改实体类重名的时候抛出并打印异常，否则系统会一直递归造成无法启动 try &#123; configuration.getTypeAliasRegistry().registerAliases(packageToScan, typeAliasesSuperType == null ? Object.class : typeAliasesSuperType); &#125; catch (Exception ex) &#123; logger.error(\"Scanned package: '\" + packageToScan + \"' for aliases\", ex); throw new NestedIOException(\"Scanned package: '\" + packageToScan + \"' for aliases\", ex); &#125; finally &#123; ErrorContext.instance().reset(); &#125; // 修改处：ThinkGem end if (logger.isDebugEnabled()) &#123; logger.debug(\"Scanned package: '\" + packageToScan + \"' for aliases\"); &#125; &#125; &#125; if (!ObjectUtils.isEmpty(this.typeAliases)) &#123; for (Class typeAlias : this.typeAliases) &#123; configuration.getTypeAliasRegistry().registerAlias(typeAlias); if (logger.isDebugEnabled()) &#123; logger.debug(\"Registered type alias: '\" + typeAlias + \"'\"); &#125; &#125; &#125; if (!ObjectUtils.isEmpty(this.plugins)) &#123; for (Interceptor plugin : this.plugins) &#123; configuration.addInterceptor(plugin); if (logger.isDebugEnabled()) &#123; logger.debug(\"Registered plugin: '\" + plugin + \"'\"); &#125; &#125; &#125; if (StringUtils.hasLength(this.typeHandlersPackage)) &#123; String[] typeHandlersPackageArray = StringUtils.tokenizeToStringArray(this.typeHandlersPackage, \",; \\t\\n\"); for (String packageToScan : typeHandlersPackageArray) &#123; configuration.getTypeHandlerRegistry().register(packageToScan); if (logger.isDebugEnabled()) &#123; logger.debug(\"Scanned package: '\" + packageToScan + \"' for type handlers\"); &#125; &#125; &#125; if (!ObjectUtils.isEmpty(this.typeHandlers)) &#123; for (TypeHandler typeHandler : this.typeHandlers) &#123; configuration.getTypeHandlerRegistry().register(typeHandler); if (logger.isDebugEnabled()) &#123; logger.debug(\"Registered type handler: '\" + typeHandler + \"'\"); &#125; &#125; &#125; if (xmlConfigBuilder != null) &#123; try &#123; xmlConfigBuilder.parse(); if (logger.isDebugEnabled()) logger.debug(\"Parsed configuration file: '\" + this.configLocation + \"'\"); &#125; catch (Exception ex) &#123; throw new NestedIOException(\"Failed to parse config resource: \" + this.configLocation, ex); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; if (this.transactionFactory == null) &#123; this.transactionFactory = new SpringManagedTransactionFactory(); &#125; Environment environment = new Environment(this.environment, this.transactionFactory, this.dataSource); configuration.setEnvironment(environment); if (this.databaseIdProvider != null) &#123; try &#123; configuration.setDatabaseId(this.databaseIdProvider.getDatabaseId(this.dataSource)); &#125; catch (SQLException e) &#123; throw new NestedIOException(\"Failed getting a databaseId\", e); &#125; &#125; if (!ObjectUtils.isEmpty(this.mapperLocations)) &#123; for (Resource mapperLocation : this.mapperLocations) &#123; if (mapperLocation == null) &#123; continue; &#125; try &#123; XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(mapperLocation.getInputStream(), configuration, mapperLocation.toString(), configuration.getSqlFragments()); xmlMapperBuilder.parse(); &#125; catch (Exception e) &#123; // 修改处：ThinkGem MapperXML有错误的时候抛出并打印异常，否则系统会一直递归造成无法启动 logger.error(\"Failed to parse mapping resource: '\" + mapperLocation + \"'\", e); throw new NestedIOException(\"Failed to parse mapping resource: '\" + mapperLocation + \"'\", e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(\"Parsed mapper file: '\" + mapperLocation + \"'\"); &#125; &#125; // 修改处：ThinkGem 启动刷新MapperXML定时器（有助于开发者调试）。 new MapperRefresh(this.mapperLocations, configuration).run(); &#125; else if (logger.isDebugEnabled()) &#123; logger.debug(\"Property 'mapperLocations' was not specified or no matching resources found\"); &#125; return this.sqlSessionFactoryBuilder.build(configuration); &#125; public SqlSessionFactory getObject() throws Exception &#123; if (this.sqlSessionFactory == null) &#123; afterPropertiesSet(); &#125; return this.sqlSessionFactory; &#125; public Class&lt;? extends SqlSessionFactory&gt; getObjectType() &#123; return this.sqlSessionFactory == null ? SqlSessionFactory.class : this.sqlSessionFactory.getClass(); &#125; public boolean isSingleton() &#123; return true; &#125; public void onApplicationEvent(ApplicationEvent event) &#123; if ((this.failFast) &amp;&amp; ((event instanceof ContextRefreshedEvent))) &#123; this.sqlSessionFactory.getConfiguration().getMappedStatementNames(); &#125; &#125;&#125; 接下来我们就需要把这个SqlSessionFactoryBean.java文件编译成class文件，然后再复制到mybatis-spring-1.2.2.jar包里面 。重新部署当前项目 Servers &gt; Tomcat 8.x &gt; 右键你的项目 Remove deployment 然后再 Add Deployment…你的项目。 去Tomcat 8的根目录找到对应的SqlSessionFactoryBean.class文件复制出来。 这里记得检查一下编译过的class文件是否正确，将你编译好的SqlSessionFactoryBean.class文件再次拖入，用jd-gui.exe(一款JAVA反编译工具)比较是不是和上面写的代码对应！！！！ 检查无误之后，把SqlSessionFactoryBean.class复制到mybatis-spring-1.2.2.jar(是你本地项目中的jar)包中，替换原来的class文件。 ​ 创建mybatis-refresh.properties文件一切准备就绪，还剩下最后一个属性文件， 创建mybatis-refresh.properties文件，记得把文件格式改成UTF-8。 mybatis-refresh.properties文件内容为： 12345678#是否开启刷新线程enabled=true#延迟启动刷新程序的秒数delaySeconds=60 #刷新扫描间隔的时长秒数sleepSeconds=3#扫描Mapper文件的资源路径mappingPath=mapper 测试 删除org.mybatis.spring包及下面的SqlSessionFactoryBean.java文件。 启动项目，然后随便修改一个mapper.xml文件，然后稍等片刻，在控制台出现如下输出，就表示你成功啦！这样就不用重启项目，也能加载到你修改的mapper.xml文件了 。 注意 注意各个文件的位置和名称。 注意MapperRefresh.java文件中mybatis-refresh.properties的路径。 注意用jd-gui.exe检查编译后的SqlSessionFactoryBean.class文件。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"mybatis","slug":"mybatis","permalink":"https://sunny0715.github.io/tags/mybatis/"}]},{"title":"Spring-AOP两种配置方式","slug":"Spring-AOP两种配置方式","date":"2017-09-09T11:44:46.000Z","updated":"2018-03-13T05:52:30.194Z","comments":true,"path":"2017/09/09/Spring-AOP两种配置方式/","link":"","permalink":"https://sunny0715.github.io/2017/09/09/Spring-AOP两种配置方式/","excerpt":"引言AOPAOP（Aspect Oriented Programming），即面向切面编程，可以说是OOP（Object Oriented Programming，面向对象编程）的补充和完善。OOP引入封装、继承、多态等概念来建立一种对象层次结构，用于模拟公共行为的一个集合。不过OOP允许开发者定义纵向的关系，但并不适合定义横向的关系，例如日志功能。日志代码往往横向地散布在所有对象层次中，而与它对应的对象的核心功能毫无关系对于其他类型的代码，如安全性、异常处理和透明的持续性也都是如此，这种散布在各处的无关的代码被称为横切（cross cutting），在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。","text":"引言AOPAOP（Aspect Oriented Programming），即面向切面编程，可以说是OOP（Object Oriented Programming，面向对象编程）的补充和完善。OOP引入封装、继承、多态等概念来建立一种对象层次结构，用于模拟公共行为的一个集合。不过OOP允许开发者定义纵向的关系，但并不适合定义横向的关系，例如日志功能。日志代码往往横向地散布在所有对象层次中，而与它对应的对象的核心功能毫无关系对于其他类型的代码，如安全性、异常处理和透明的持续性也都是如此，这种散布在各处的无关的代码被称为横切（cross cutting），在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。 AOP技术恰恰相反，它利用一种称为”横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其命名为”Aspect”，即切面。所谓”切面”，简单说就是那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块之间的耦合度，并有利于未来的可操作性和可维护性。 使用”横切”技术，AOP把软件系统分为两个部分：核心关注点和横切关注点。业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。横切关注点的一个特点是，他们经常发生在核心关注点的多处，而各处基本相似，比如权限认证、日志、事务。AOP的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来。 AOP核心概念1、横切关注点 对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点 2、切面（Aspect） 类是对物体特征的抽象，切面就是对横切关注点的抽象 3、连接点（Joinpoint） 被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器 4、切入点（Pointcut） 对连接点进行拦截的定义 5、通知（Advice） 所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类 6、目标对象 代理的目标对象 7、织入（Weave） 将切面应用到目标对象并导致代理对象创建的过程 8、引入（Introduction） 在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段 Spring对AOP的支持Spring中AOP代理由Spring的IOC容器负责生成、管理，其依赖关系也由IOC容器负责管理。因此，AOP代理可以直接使用容器中的其它bean实例作为目标，这种关系可由IOC容器的依赖注入提供。Spring创建代理的规则为： 1、默认使用Java动态代理来创建AOP代理，这样就可以为任何接口实例创建代理了 2、当需要代理的类不是代理接口的时候，Spring会切换为使用CGLIB代理，也可强制使用CGLIB AOP编程其实是很简单的事情，纵观AOP编程，程序员只需要参与三个部分： 1、定义普通业务组件 2、定义切入点，一个切入点可能横切多个业务组件 3、定义增强处理，增强处理就是在AOP框架为普通业务组件织入的处理动作 所以进行AOP编程的关键就是定义切入点和定义增强处理，一旦定义了合适的切入点和增强处理，AOP框架将自动生成AOP代理，即：代理对象的方法=增强处理+被代理对象的方法。 Spring配置AOP的两种方式注解配置AOP注解配置AOP（使用 AspectJ 类库实现的），大致分为三步： 使用注解@Aspect来定义一个切面，在切面中定义切入点(@Pointcut),通知类型(@Before, @AfterReturning,@After,@AfterThrowing,@Around). 开发需要被拦截的类。 将切面配置到xml中，当然，我们也可以使用自动扫描Bean的方式。这样的话，那就交由Spring AOP容器管理。 applicationContext的配置 123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.1.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd\"&gt; &lt;!-- proxy-target-class等于true是强制使用cglib代理，proxy-target-class默认是false，如果你的类实现了接口 就走JDK代理，如果没有，走cglib代理 --&gt; &lt;!-- 对于单例模式建议使用cglib代理，虽然JDK动态代理比cglib代理速度快，但性能不如cglib --&gt; &lt;!-- 激活自动代理功能 打开aop对@Aspectj的注解支持 ,相当于为注解提供解析功能--&gt; &lt;aop:aspectj-autoproxy proxy-target-class=\"true\"/&gt; &lt;!-- 激活组件扫描功能,在包com.spring.aop及其子包下面自动扫描通过注解配置的组件 --&gt; &lt;context:component-scan base-package=\"com.spring.aop\"/&gt; &lt;!-- 切面 --&gt; &lt;bean id=\"serviceAspect\" class=\"com.spring.aop.aspect.ServiceAspect\" /&gt;&lt;/beans&gt; 为Aspect切面类添加注解 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package com.spring.aop.aspect;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.springframework.stereotype.Component;/** * 系统服务组件Aspect切面Bean *///声明这是一个组件@Component//声明这是一个切面Bean@Aspectpublic class ServiceAspect &#123; private final static Log log = LogFactory.getLog(ServiceAspect.class); //配置切入点,该方法无方法体,主要为方便同类中其他方法使用此处配置的切入点 @Pointcut(\"execution(* com.spring.aop.service..*(..))\") public void aspect()&#123; &#125; /* * 配置前置通知,使用在方法aspect()上注册的切入点 * 同时接受JoinPoint切入点对象,可以没有该参数 */ @Before(\"aspect()\") public void before(JoinPoint joinPoint)&#123; if(log.isInfoEnabled())&#123; log.info(\"before \" + joinPoint); &#125; &#125; //配置后置通知,使用在方法aspect()上注册的切入点 @After(\"aspect()\") public void after(JoinPoint joinPoint)&#123; if(log.isInfoEnabled())&#123; log.info(\"after \" + joinPoint); &#125; &#125; //配置环绕通知,使用在方法aspect()上注册的切入点 @Around(\"aspect()\") public void around(JoinPoint joinPoint)&#123; long start = System.currentTimeMillis(); try &#123; ((ProceedingJoinPoint) joinPoint).proceed(); long end = System.currentTimeMillis(); if(log.isInfoEnabled())&#123; log.info(\"around \" + joinPoint + \"\\tUse time : \" + (end - start) + \" ms!\"); &#125; &#125; catch (Throwable e) &#123; long end = System.currentTimeMillis(); if(log.isInfoEnabled())&#123; log.info(\"around \" + joinPoint + \"\\tUse time : \" + (end - start) + \" ms with exception : \" + e.getMessage()); &#125; &#125; &#125; //配置后置返回通知,使用在方法aspect()上注册的切入点 @AfterReturning(\"aspect()\") public void afterReturn(JoinPoint joinPoint)&#123; if(log.isInfoEnabled())&#123; log.info(\"afterReturn \" + joinPoint); &#125; &#125; //配置抛出异常后通知,使用在方法aspect()上注册的切入点 @AfterThrowing(pointcut=\"aspect()\", throwing=\"ex\") public void afterThrow(JoinPoint joinPoint, Exception ex)&#123; if(log.isInfoEnabled())&#123; log.info(\"afterThrow \" + joinPoint + \"\\t\" + ex.getMessage()); &#125; &#125; &#125; UserService.java 123456789101112131415161718192021222324252627282930313233343536package com.spring.aop.service;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import com.spring.mvc.bean.User;/** * 用户服务模型 */public class UserService &#123; private final static Log log = LogFactory.getLog(UserService.class); public User get(long id)&#123; if(log.isInfoEnabled())&#123; log.info(\"getUser method . . .\"); &#125; return new User(); &#125; public void save(User user)&#123; if(log.isInfoEnabled())&#123; log.info(\"saveUser method . . .\"); &#125; &#125; public boolean delete(long id) throws Exception&#123; if(log.isInfoEnabled())&#123; log.info(\"delete method . . .\"); throw new Exception(\"spring aop ThrowAdvice演示\"); &#125; return false; &#125; &#125; 测试代码 12345678910111213141516171819202122232425262728293031323334package com.spring.aop;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.spring.aop.service.UserService;import com.spring.mvc.bean.User;/** * Spring AOP测试 */public class Tester &#123; private final static Log log = LogFactory.getLog(Tester.class); public static void main(String[] args) &#123; //启动Spring容器 ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //获取service组件 UserService service = (UserService) context.getBean(\"userService\"); //以普通的方式调用UserService对象的三个方法 User user = service.get(1L); service.save(user); try &#123; service.delete(1L); &#125; catch (Exception e) &#123; if(log.isWarnEnabled())&#123; log.warn(\"Delete user : \" + e.getMessage()); &#125; &#125; &#125;&#125; 控制台输出如下： 12345678910111213141516INFO [spring.aop.aspect.ServiceAspect:40] before execution(User com.spring.aop.service.UserService.get(long))INFO [spring.aop.service.UserService:19] getUser method . . .INFO [spring.aop.aspect.ServiceAspect:60] around execution(User com.spring.aop.service.UserService.get(long)) Use time : 42 ms!INFO [spring.aop.aspect.ServiceAspect:48] after execution(User com.spring.aop.service.UserService.get(long))INFO [spring.aop.aspect.ServiceAspect:74] afterReturn execution(User com.spring.aop.service.UserService.get(long))INFO [spring.aop.aspect.ServiceAspect:40] before execution(void com.spring.aop.service.UserService.save(User))INFO [spring.aop.service.UserService:26] saveUser method . . .INFO [spring.aop.aspect.ServiceAspect:60] around execution(void com.spring.aop.service.UserService.save(User)) Use time : 2 ms!INFO [spring.aop.aspect.ServiceAspect:48] after execution(void com.spring.aop.service.UserService.save(User))INFO [spring.aop.aspect.ServiceAspect:74] afterReturn execution(void com.spring.aop.service.UserService.save(User))INFO [spring.aop.aspect.ServiceAspect:40] before execution(boolean com.spring.aop.service.UserService.delete(long))INFO [spring.aop.service.UserService:32] delete method . . .INFO [spring.aop.aspect.ServiceAspect:65] around execution(boolean com.spring.aop.service.UserService.delete(long)) Use time : 5 ms with exception : spring aop ThrowAdvice演示INFO [spring.aop.aspect.ServiceAspect:48] after execution(boolean com.spring.aop.service.UserService.delete(long))INFO [spring.aop.aspect.ServiceAspect:74] afterReturn execution(boolean com.spring.aop.service.UserService.delete(long))WARN [studio.spring.aop.Tester:32] Delete user : Null return value from advice does not match primitive return type for: public boolean com.spring.aop.service.UserService.delete(long) throws java.lang.Exception 可以看到，正如我们预期的那样，虽然我们并没有对UserSerivce类包括其调用方式做任何改变，但是Spring仍然拦截到了其中方法的调用，或许这正是AOP的魔力所在。 XML配置AOPXML配置 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.1.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd\"&gt; &lt;!-- 系统服务组件的切面Bean --&gt; &lt;bean id=\"serviceAspect\" class=\"com.spring.aop.aspect.ServiceAspect\"/&gt; &lt;!-- AOP配置 --&gt; &lt;aop:config&gt; &lt;!-- 声明一个切面,并注入切面Bean,相当于@Aspect --&gt; &lt;aop:aspect id=\"simpleAspect\" ref=\"serviceAspect\"&gt; &lt;!-- 配置一个切入点,相当于@Pointcut --&gt; &lt;aop:pointcut expression=\"execution(* com.spring.aop.service..*(..))\" id=\"simplePointcut\"/&gt; &lt;!-- 配置通知,相当于@Before、@After、@AfterReturn、@Around、@AfterThrowing --&gt; &lt;aop:before pointcut-ref=\"simplePointcut\" method=\"before\"/&gt; &lt;aop:after pointcut-ref=\"simplePointcut\" method=\"after\"/&gt; &lt;aop:after-returning pointcut-ref=\"simplePointcut\" method=\"afterReturn\"/&gt; &lt;aop:after-throwing pointcut-ref=\"simplePointcut\" method=\"afterThrow\" throwing=\"ex\"/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; ServiceAspect.java 1234567891011121314151617181920212223//配置前置通知,拦截返回值为com.spring.mvc.bean.User的方法@Before(\"execution(com.spring.mvc.bean.User com.spring.aop.service..*(..))\")public void beforeReturnUser(JoinPoint joinPoint)&#123; if(log.isInfoEnabled())&#123; log.info(\"beforeReturnUser \" + joinPoint); &#125;&#125;//配置前置通知,拦截参数为com.spring.mvc.bean.User的方法@Before(\"execution(* com.spring.aop.service..*(com.spring.mvc.bean.User))\")public void beforeArgUser(JoinPoint joinPoint)&#123; if(log.isInfoEnabled())&#123; log.info(\"beforeArgUser \" + joinPoint); &#125;&#125;//配置前置通知,拦截含有long类型参数的方法,并将参数值注入到当前方法的形参id中@Before(\"aspect()&amp;&amp;args(id)\")public void beforeArgId(JoinPoint joinPoint, long id)&#123; if(log.isInfoEnabled())&#123; log.info(\"beforeArgId \" + joinPoint + \"\\tID:\" + id); &#125;&#125; UserService.java 123456789101112131415161718192021222324252627282930313233343536package com.spring.aop.service;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import com.spring.mvc.bean.User;/** * 用户服务模型 */public class UserService &#123; private final static Log log = LogFactory.getLog(UserService.class); public User get(long id)&#123; if(log.isInfoEnabled())&#123; log.info(\"getUser method . . .\"); &#125; return new User(); &#125; public void save(User user)&#123; if(log.isInfoEnabled())&#123; log.info(\"saveUser method . . .\"); &#125; &#125; public boolean delete(long id) throws Exception&#123; if(log.isInfoEnabled())&#123; log.info(\"delete method . . .\"); throw new Exception(\"spring aop ThrowAdvice演示\"); &#125; return false; &#125; &#125; 总结SpringAop可以用来： Spring声明式事务管理配置。 在执行方法前,判断是否具有权限。 对部分函数的调用进行日志记录。监控部分重要函数，若抛出指定的异常，可以以短信或邮件方式通知相关人员。 使用Spring AOP实现MySQL数据库读写分离。 信息过滤 ……","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"spring","slug":"spring","permalink":"https://sunny0715.github.io/tags/spring/"}]},{"title":"跨页面(Tab/Window)通信的几种方法","slug":"跨页面-Tab-Window-通信的几种方法","date":"2017-09-01T02:32:43.000Z","updated":"2018-03-13T05:54:15.005Z","comments":true,"path":"2017/09/01/跨页面-Tab-Window-通信的几种方法/","link":"","permalink":"https://sunny0715.github.io/2017/09/01/跨页面-Tab-Window-通信的几种方法/","excerpt":"​ 今天开发一个功能遇到一个需求，在A页面点击查看详情后打开B页面进行修改或删除，删除后B页面关闭，然后刷新A页面里面的数据。相当于就是两个页面之间进行通讯，作为后端的我第一想法是利用webSocket 进行通讯，之后通过谷歌和百度找出了更为简便的方法。","text":"​ 今天开发一个功能遇到一个需求，在A页面点击查看详情后打开B页面进行修改或删除，删除后B页面关闭，然后刷新A页面里面的数据。相当于就是两个页面之间进行通讯，作为后端的我第一想法是利用webSocket 进行通讯，之后通过谷歌和百度找出了更为简便的方法。 利用webSocket进行通讯​ 第一想法是这个，但是这样的话工作量巨大而且还需要后端支持，太麻烦了，对于我这种懒人直接就放弃了，去寻找有没有更简便的方法。 定时器不断检查cookies变化在stackoverflow上看到一个方案，大致思路是： 在页面A设置一个使用 setInterval 定时器不断刷新，检查 Cookies 的值是否发生变化，如果变化就进行刷新的操作。 由于 Cookies 是在同域可读的，所以在页面 B 审核的时候改变 Cookies 的值，页面 A 自然是可以拿到的。这样做确实可以实现我想要的功能，但是这样的方法相当浪费资源。虽然在这个性能过盛的时代，浪费不浪费也感觉不出来，但是这种实现方案，确实不够优(zhuāng)雅（bī）。 localStorage的事件功夫不负有心人，后来发现 window 有一个 StorageEvent ，每当 localStorage 改变的时候可以触发这个事件。（这个原理就像你给一个DOM 绑定了 click 事件，当你点击它的时候，就会自动触发。）也就是说，我给 window 绑定这个事件后，每当我改变 localStorage 的时候，他都会触发这个事件。 123window.addEventListener(&apos;storage&apos;, function (event) &#123; console.log(event);&#125;); 这个回调中的event与普通的EVNET,基本差不多，但是它比其他的event多了如下几个属性: 属性 描述 key 受影响的 localStorage 的 key newValue 新的值 oldValue 旧的值 url 触发此事件的url 每当一个页面改变了 localStorage 的值，都会触发这个事件。也就是说可以很容易的通过改变 localStorage 的值，来实现浏览器中跨页面( tab / window )之间的通讯。记住这个事件只有在 localStorage 发生改变的时候才会被触发，如果没改变则不会触发此事件。 123localStorage.setItem(&apos;delete&apos;,1); //触发localStorage.setItem(&apos;delete&apos;,1); //不触发localStorage.setItem(&apos;delete&apos;,2); //触发 在使用的时候务必注意这一点。最终实现代码: 页面A： 123456//页面 Awindow.addEventListener(&apos;storage&apos;, function (event) &#123; if(event.key === &apos;delete_verify_list&apos;)&#123; //页面操作 &#125;&#125;); 页面B： 1234567891011121314//页面 B/** * 获取一个随机id * @return &#123;String&#125; - 返回一个5位的随机字符串 */function randomId() &#123; return (Math.random() * 1E18).toString(36).slice(0, 5).toUpperCase();&#125;//每当需要页面A更新时 执行此方法if (localStorage) &#123; //为保证每次页面A都执行，此处我设置里一个随机字符串 localStorage.setItem(&apos;delete_verify_list&apos;, randomId());&#125; 参考：https://ponyfoo.com/articles/cross-tab-communication","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"前端","slug":"前端","permalink":"https://sunny0715.github.io/tags/前端/"}]},{"title":"JAVA实现HTML转PDF","slug":"JAVA实现HTML转PDF","date":"2017-07-27T05:22:32.000Z","updated":"2018-03-13T05:49:40.032Z","comments":true,"path":"2017/07/27/JAVA实现HTML转PDF/","link":"","permalink":"https://sunny0715.github.io/2017/07/27/JAVA实现HTML转PDF/","excerpt":"​ 最近公司里面有一个任务，在线题卡，就是把客户在线编辑的题卡样式保存下来，然后可以导出为PDF格式。于是上网找了一系列的资料，找到了以下两种方法： 使用wkhtmltox 使用iText+Flying Saucer 但是还是强烈推荐用第一种方法。","text":"​ 最近公司里面有一个任务，在线题卡，就是把客户在线编辑的题卡样式保存下来，然后可以导出为PDF格式。于是上网找了一系列的资料，找到了以下两种方法： 使用wkhtmltox 使用iText+Flying Saucer 但是还是强烈推荐用第一种方法。 使用wkhtmltox(推荐)wkhtmltox实现网页转换成图片或PDF 命令实现 进入wkhtmltox官网软件下载 ：https://wkhtmltopdf.org/downloads.html 安装完成后进入${home}/bin目录下有两个exe文件，通过名称就可以辨别wkhtmltoimage.exe是将HTML转化为image，wkhtmltopdf.exe是将HTML转化为PDF文件，这正是我们想要的。 进入${home}/bin目录下打开cmd输入以下命令验证 12wkhtmltopdf HTML路径 保存路径如： wkhtmltopdf www.baidu.com d:\\test.pdf 生成完成后会出现Done。 代码实现JAVA代码中调用wkhtmltopdf生成PDF文件，以下为代码片段 1234567891011121314151617181920212223242526272829303132333435363738/** * HTMLTOPPDF * 利用wkhtmltopdf生成PDF */public class HtmlToPDF &#123; //wkhtmltopdf.exe安装路径 public static final String toPdfTool = \"E:\\\\SmallTools\\\\wkhtmltox\\\\wkhtmltopdf\\\\bin\\\\wkhtmltopdf.exe\"; //需要生成PDF的URL public static final String srcPath = \"http://www.jianshu.com/p/4d65857ffe5e\"; public static void main(String[] args) throws Exception&#123; //设置纸张大小: A4, Letter, etc. String pageSize = \"A4\"; //生成后存放路径 String destPath = \"E:\\\\PDF生成教程及讲解.pdf\"; convert(pageSize, destPath); &#125; public static void convert(String pageSize, String destPath)&#123; File file = new File(destPath); File parent = file.getParentFile(); if (!parent.exists())&#123; parent.mkdirs(); &#125; StringBuilder cmd = new StringBuilder(); cmd.append(toPdfTool).append(\" \"); cmd.append(\"--page-size \"); cmd.append(pageSize).append(\" \"); cmd.append(srcPath).append(\" \"); cmd.append(destPath); try &#123; Runtime.getRuntime().exec(cmd.toString()); &#125;catch (IOException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 详细参数说明可参考：http://www.jianshu.com/p/4d65857ffe5e 使用iText+Flying Saucer123456789itext可实现 1.可以进行块的创建2.表格的使用3.设置页面的事件4.字体的设置5.图片的设置（包含水印）6.HTML转化成PDF（支持css,javascript）7.表单创建8.PDF之间的操作等详细的内容可以查看网站的说明。 Maven配置12345678910&lt;dependency&gt; &lt;groupId&gt;com.itextpdf&lt;/groupId&gt; &lt;artifactId&gt;itextpdf&lt;/artifactId&gt; &lt;version&gt;5.8.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.xhtmlrenderer&lt;/groupId&gt; &lt;artifactId&gt;flying-saucer-pdf&lt;/artifactId&gt; &lt;version&gt;9.1.6&lt;/version&gt;&lt;/dependency&gt; 代码片段1234567891011121314151617181920212223/** * 生成pdf，添加生成pdf所使用的字符集.注：这里字符集要和模板中使用的字符集一一致。 */public class HtmlToPDF &#123; public static void main(String[] args) throws Exception&#123; Document document = new Document(PageSize.A4.rotate()); //设置为A4纸大小 ITextRenderer renderer = new ITextRenderer(); ITextFontResolver fontResolver = renderer.getFontResolver(); fontResolver.addFont(\"D:/simsun.ttc\", BaseFont.IDENTITY_H, BaseFont.NOT_EMBEDDED); // step 2 PdfWriter writer = PdfWriter.getInstance(document, new FileOutputStream(\"D:\\\\pdf.pdf\")); // step 3 document.open(); // step 4 XMLWorkerHelper.getInstance().parseXHtml(writer, document, new FileInputStream(\"D:/a.html\")); //step 5 document.close(); System.out.println( \"PDF Created!\" ); &#125;&#125; 注意事项 .输入的HTML页面必须是标准的XHTML页面。页面的顶上必须是这样的格式： 12&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt; &lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt; 生成PDF，添加生成PDF所使用的字符集.注：这里字符集要和模板中使用的字符集一一致。 比如:java中使用宋体 renderer.getFontResolver().addFont(“C:/Windows/Fonts/simsun.ttc”, BaseFont.IDENTITY_H, BaseFont.NOT_EMBEDDED); 那么HTML的body中样式必须加上 style=’font-family:SimSun’，要是使用其他字符生成pdf时候，中文就会不显示生成PDF 设置PDF的页面大小模板页面中添加该样式：@page { size: 8.5in 11in; }这时候生成PDF页面正好是A4纸大小 所需的jar包，下载点我。核心jar是修改后的 比较和总结比较itext 1231. java生成PDF大部分都是用itext，itext的确是java开源组件的第一选择。不过itext也有局限，就是要自己写模版，系统中的表单数量有好几百个，为每个表单做一个导出模版不现实。2. 并且itext中文适配不是很好和换行问题。3. 且对HTML格式要求严格。 wkhtmltopdf 1231. 生成PDF时会自动根据你在HTML页面中H标签生成树形目录结构。2. 小巧方便，转换速度快。3. 跨平台，在Liunx下用，在win下也可以用。 总结​ 综上比较，wkhtmltopdf是将HTML转为图片或是PDF最好的选择。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"}]},{"title":"静态服务器搭建及前端知识点","slug":"静态服务器搭建及前端知识点","date":"2017-06-13T06:24:21.000Z","updated":"2018-03-13T05:54:03.361Z","comments":true,"path":"2017/06/13/静态服务器搭建及前端知识点/","link":"","permalink":"https://sunny0715.github.io/2017/06/13/静态服务器搭建及前端知识点/","excerpt":"引言​ 虽然是做后台开发，但是很多时候自己也兼顾了前台，并不是所有的项目都是前后台分离开发，所以在开发期间自己也总结和学习了前端了一些小知识，在这里进行总结，以便自己温习。","text":"引言​ 虽然是做后台开发，但是很多时候自己也兼顾了前台，并不是所有的项目都是前后台分离开发，所以在开发期间自己也总结和学习了前端了一些小知识，在这里进行总结，以便自己温习。 # NodeJS搭建静态资源服务器对Node.js只有浅显的认识，但是有时候又要自己搭建静态服务器进行测试。搭建静态服务器需要以下几个步骤：1. 下载node.js，进入node.js官网http://nodejs.cn下载对应的版本。2. 安装node.js。3. 启动node.js，在命令行输入命令安装需要的模块，依次执行命令。123npm install expressnpm install requestnpm install http-server## 简单的静态服务器新建server.js，内容为1234567891011var express = require('express');var http = require(\"http\");var request = require('request');var app = express();//启动端口为81var port = process.env.PORT||81; //静态资源存放的路径 app.use(express.static('E:/SmallTools/StaticServer'));http.createServer(app).listen(port);console.log(\"服务启动成功\"); 启动server.js 通过http请求访问a.html页面 可以访问说明搭建成功！ 带反向代理静态服务器搭建新建server-kaow-school.js，内容为 123456789101112131415161718192021222324var express = require('express');var http = require(\"http\");var https = require('https');var request = require('request');var app = express();//app.disable('x-powered-by');var port = process.env.PORT||81;app.use(express.static('E:/SmallTools/StaticServer'));function proxy(app,route,remoteDomain)&#123; app.use(route,function(req,res)&#123; var url = remoteDomain+req.url; req.pipe(request(url)).pipe(res); &#125;);&#125;//10.9.4.215:8380 测试服务器ipproxy(app,'/163','http://www.163.com');//proxy(app,'/MonitorService/','http://10.9.4.215:9192/MonitorService/'); http.createServer(app).listen(port);console.log(\"服务器启动完成,请使用locahost:\"+port+\"访问\"); 启动server-kaow-school.js 通过http请求访问b.html页面 访问/163 可以访问说明带反向代理的静态服务器搭建成功！ 前端知识点 JS字符串截取空白trim()的原型实现123String.prototype.trim = function()&#123; return this.replace( /(^\\s*)|(\\s*$)/g , ''\");&#125; JS屏蔽键盘按键12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;body oncontextmenu=\"return false\" onselectstart=\"return false\" ondragstart=\"return false\" onbeforecopy=\"return false\" onmouseup=document.selection.empty() oncopy=document.selection.empty() onselect=document.selection.empty()&gt;&lt;/body&gt; 讲上面红色显示的插入到网页中就可以实现鼠标右击无效禁止选择 onselectstart=\"return false\" 禁止拖放 ondragstart=\"return false\" 禁止拷贝 ncopy=document.selection.empty() 禁止保存(放在head里面) &lt;noscript&gt;&lt;iframe src=\"*.htm\"&gt;&lt;/iframe&gt;&lt;/noscript&gt;禁止粘贴 &lt;input type=text onpaste=\"return false\"&gt;关闭输入法 &lt;input style=\"ime-mode:disabled\"&gt;屏蔽鼠标右键 function document.oncontextmenu()&#123;event.returnValue=false;&#125;屏蔽F1帮助 function window.onhelp()&#123;return false&#125; 屏蔽其他键function document.onkeydown() &#123; if ((window.event.altKey)&amp;&amp; ((window.event.keyCode==37)|| //屏蔽 Alt+ 方向键 ← (window.event.keyCode==39))) //屏蔽 Alt+ 方向键 → &#123; alert(\"不准你使用ALT+方向键前进或后退网页！\"); event.returnValue=false; &#125; /* 注：这还不是真正地屏蔽 Alt+ 方向键， 因为 Alt+ 方向键弹出警告框时，按住 Alt 键不放， 用鼠标点掉警告框，这种屏蔽方法就失效了。以后若 有哪位高手有真正屏蔽 Alt 键的方法，请告知。*/ if ((event.keyCode==8) || //屏蔽退格删除键 (event.keyCode==116)|| //屏蔽 F5 刷新键 (event.ctrlKey &amp;&amp; event.keyCode==82))&#123; //Ctrl + R event.keyCode=0; event.returnValue=false; &#125; 屏蔽F11 if (event.keyCode==122)&#123;event.keyCode=0;event.returnValue=false;&#125;屏蔽 Ctrl+n if (event.ctrlKey &amp;&amp; event.keyCode==78) event.returnValue=false; if (event.shiftKey &amp;&amp; event.keyCode==121) event.returnValue=false; //屏蔽 shift+F10 if (window.event.srcElement.tagName == \"A\" &amp;&amp; window.event.shiftKey) window.event.returnValue = false; //屏蔽 shift 加鼠标左键新开一网页 if ((window.event.altKey)&amp;&amp;(window.event.keyCode==115))&#123; //屏蔽Alt+F4 window.showModelessDialog(\"about:blank\",\"\",\"dialogWidth:1px;dialogheight:1px\"); return false; &#125; &#125;屏蔽打印：&lt;style&gt; @media print&#123; * &#123;display:none&#125; &#125; &lt;/style&gt; HTML之间传值(通过解析url)123456789101112131415161718192021222324var hrefInfo = getUrlVars(window.location.href); // 得到参数信息 if (hrefInfo.logId &amp;&amp; hrefInfo.logId != \"undefined\") &#123; fillData(hrefInfo.logId); logId = hrefInfo.logId; &#125; else &#123; &#125;//解析url中的参数function getUrlVars(hrf) &#123; var vars = [], hash; var locationHref = !hrf ? window.location.href : hrf; locationHref = locationHref.replace(/#/g, \"\"); if (locationHref.indexOf('%') &gt; 0) &#123; locationHref = unescape(locationHref); &#125; var hashes = locationHref.slice(locationHref.indexOf('?') + 1).split('&amp;'); for ( var i = 0; i &lt; hashes.length; i++) &#123; hash = hashes[i].split('='); vars.push(hash[0]); vars[hash[0]] = hash[1]; &#125; return vars;&#125; Jquery获取radio,checkbox123456789101112//获取radio的id $(\"input[name='r']:checked\").attr(\"id\"); //获得checkbox数目$(\"input[name='c']:checked\").length;//遍历checkbox$(\"input[name='c']:check\").eq(i).attr(\"id\");//全选checkbox$(\"input[name='c']:checkbox\").attr(\"checked\",\"true\");//获取选中的checkbox $(\"input[name='c']:checked\").map(function()&#123;return $(this).val();&#125;).get().join(\",\");//获取下拉框选中的id $(\"#s option:selected\").attr(\"value\"); Jquery页面查询(数据量大时禁用)12345678910111213141516 function search()&#123; var nameSearch = $(\"#itemName\").val(); //搜索框ID var tableObj = $(\"#itemList tr:gt(0)\"); // table的ID if(nameSearch.trim()!=\"\")&#123; tableObj.hide(); tableObj.each(function()&#123; var tr = $(this); var fuHe = tr.children(\":eq(0)\").html(); if(fuHe.indexOf(nameSearch)==0)&#123; tr.show(); &#125; &#125;); &#125;else&#123; tableObj.show(); &#125; &#125; Jquery 回车(Enter)移到下一个输入框1234567891011121314$(document).ready(function () &#123; $('input:text:first').focus(); $('input:text').bind(\"keydown\", function (e) &#123; if (e.which == 13) &#123; //Enter key e.preventDefault(); //to skip default behaviour of enter key var nextinput = $('input:text')[$('input:text').index(this) + 1]; if (nextinput != undefined) &#123; nextinput.focus(); &#125; else &#123; alert(\"没有下一个输入框！\"); &#125; &#125; &#125;); &#125;); JS,Jquery获取各种屏幕的宽度和高度12345678910111213141516171819202122232425262728//Javascript:网页可见区域宽： document.body.clientWidth网页可见区域高： document.body.clientHeight网页可见区域宽： document.body.offsetWidth (包括边线的宽)网页可见区域高： document.body.offsetHeight (包括边线的高)网页正文全文宽： document.body.scrollWidth网页正文全文高： document.body.scrollHeight网页被卷去的高： document.body.scrollTop网页被卷去的左： document.body.scrollLeft网页正文部分上： window.screenTop网页正文部分左： window.screenLeft屏幕分辨率的高： window.screen.height屏幕分辨率的宽： window.screen.width屏幕可用工作区高度： window.screen.availHeight屏幕可用工作区宽度： window.screen.availWidth //JQuery:$(document).ready(function()&#123; alert($(window).height()); //浏览器当前窗口可视区域高度 alert($(document).height()); //浏览器当前窗口文档的高度 alert($(document.body).height());//浏览器当前窗口文档body的高度 alert($(document.body).outerHeight(true));//浏览器当前窗口文档body的总高度 包括border padding margin alert($(window).width()); //浏览器当前窗口可视区域宽度 alert($(document).width());//浏览器当前窗口文档对象宽度 alert($(document.body).width());//浏览器当前窗口文档body的宽度 alert($(document.body).outerWidth(true));//浏览器当前窗口文档body的总宽度 包括border padding margin&#125;) Ajax,Get时请求异步缓存问题用Ajax的Get方式请求同一个地址获取数据时，经常碰到回调函数成功执行，前台又有数据的情况，但是无法请求到后台获得最新的数据。原因是ajax存在异步缓存的问题。 因为ajax本身自带有实时异步请求的功能，而IE缓存导致请求时不会请求后台，会直接读取缓存的数据。 解决办法： ajax get请求时比较简单 只需将cache设置为false就好。 123456789$.ajax(&#123; type: 'get', //get请求时 url: '....', cache: false, //不缓存 data: &#123; &#125;, success: function (result) &#123; // &#125; &#125;); 访问就在URL后面加上 URL?+new Date();[总之就是使每次访问的URL字符串不一样的] 设计WEB页面的时候 也应该遵守这个原则，因为请求同一个地址会直接读取缓存，所以可以在参数中加一个随机数数 让每次参数不一样就好。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"前端","slug":"前端","permalink":"https://sunny0715.github.io/tags/前端/"}]},{"title":"微信公众号网页开发","slug":"微信公众号网页开发","date":"2017-06-01T07:27:34.000Z","updated":"2018-03-13T05:54:42.015Z","comments":true,"path":"2017/06/01/微信公众号网页开发/","link":"","permalink":"https://sunny0715.github.io/2017/06/01/微信公众号网页开发/","excerpt":"引言最近一直参与公司开发公众号，关于项目实施平台PC端简化为微信公众号，主要架构为前台H5，使用时的微信的WeUI的SDK，后台就是现在流行的SpringMVC+Mybaties，参与了全程开发，开发过程中也遇到了不少的问题，现在记录下来，希望以后能够用得到。","text":"引言最近一直参与公司开发公众号，关于项目实施平台PC端简化为微信公众号，主要架构为前台H5，使用时的微信的WeUI的SDK，后台就是现在流行的SpringMVC+Mybaties，参与了全程开发，开发过程中也遇到了不少的问题，现在记录下来，希望以后能够用得到。 HTML页面之间传值JSP之间传值已经很熟悉，HTML之间传值是通过解析URL获取所需参数。 12//URL传值URL + \"?logId=\" + logId; 12345678910111213141516171819202122232425262728//获取所需参数var hrefInfo = getUrlVars(window.location.href); if (hrefInfo.logId &amp;&amp; hrefInfo.logId != \"undefined\") &#123; fillData(hrefInfo.logId); logId = hrefInfo.logId; &#125; else &#123;&#125;// 得到url中的参数function getUrlVars(hrf) &#123; var vars = [], hash; var locationHref = !hrf ? window.location.href : hrf; locationHref = locationHref.replace(/#/g, \"\"); if (locationHref.indexOf('%') &gt; 0) &#123; locationHref = unescape(locationHref); &#125; var hashes = locationHref.slice(locationHref.indexOf('?') + 1).split('&amp;'); for ( var i = 0; i &lt; hashes.length; i++) &#123; hash = hashes[i].split('='); vars.push(hash[0]); vars[hash[0]] = hash[1]; &#125; return vars;&#125; JS、JQuery获取各种屏幕的高度和宽度在移动端经常会用到获取屏幕的高度和宽度，在这里总结一下。 12345678910111213Javascript:document.body.clientWidth //网页可见区域宽document.body.clientHeight //网页可见区域高document.body.offsetWidth (包括边线的宽) //网页可见区域宽document.body.offsetHeight (包括边线的高) //网页可见区域高document.body.scrollWidth //网页正文全文宽document.body.scrollHeight //网页正文全文高document.body.scrollTop //网页被卷去的高document.body.scrollLeft //网页被卷去的左window.screenTop //网页正文部分上window.screen.width //屏幕分辨率的宽window.screen.availHeight //屏幕可用工作区高度window.screen.availWidth //屏幕可用工作区宽度 123456789101112JQuery:$(document).ready(function()&#123;alert($(window).height()); //浏览器当前窗口可视区域高度alert($(document).height()); //浏览器当前窗口文档的高度alert($(document.body).height()); //浏览器当前窗口文档body的高度alert($(document.body).outerHeight(true));//浏览器当前窗口文档body的总高度 包括border padding marginalert($(window).width()); //浏览器当前窗口可视区域宽度alert($(document).width()); //浏览器当前窗口文档对象宽度alert($(document.body).width()); //浏览器当前窗口文档body的宽度alert($(document.body).outerWidth(true));//浏览器当前窗口文档body的总宽度 包括border padding margin&#125;) 微信浏览器缓存清理微信浏览器缓存一直都是相当恶心的存在，只要页面加载，那么静态页面就会被缓存，通过Google和百度找到了以下两种方法： 设置HTTP头部通过这只HTTP头部禁止浏览器缓存，效果没有达到要求，不建议使用 Android下可在微信中打开http://debugx5.qq.com清除微信缓存。 1234567891011&lt;html manifest=\"IGNORE.manifest\"&gt; &lt;meta charset=\"utf-8\"&gt;&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;&lt;meta http-equiv=\"pragma\" content=\"no-cache\"&gt;&lt;meta http-equiv=\"cache-control\" content=\"no-cache\"&gt;&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=no\"&gt;&lt;link rel=\"shortcut icon\" href=\"../../view/images/favicon.ico\" type=\"image/x-icon\" /&gt; 更新版本号HTTP头部禁止缓存不能用，就只能每次都更新版本号(加随机数)来达到自己的要求了。 使用SeaJs拦截所有.js和.css并在尾部加上随机数。 引入SeaJs1&lt;script src=\"../../util/sea.js\"&gt;&lt;/script&gt; 配置seajs_config.js 需要配置seajs_config.js全局变量，每一个HTML页面都要引入，关于更多SeaJs配置http://yslove.net/seajs/ 1234567891011var time = new Date().getTime();var sea_config = &#123; \"base\":\"/\", //web发布路径 \"debug\":\"true\", //2:每次从后台获取新的js,true:console出bug,false:默认 \"charset\":\"utf-8\", //字符集 preload: [\"util/jquery-1.8.0.min.js\"], //预加载jquery map: [ //配置映射，用来版本更新强制浏览器刷新 ['.js','.js?version=' + time], ['.css','.css?version=' + time] ]&#125;; 12345//HTML页面引入&lt;script src=\"../../config/seaConfig/seajs_config.js\"&gt;&lt;/script&gt;&lt;script&gt; seajs.config(sea_config);&lt;/script&gt; Ajax请求缓存 在编码期间，因为有个角色是查看所有项目且数据量也比较大，所以把Ajax请求方式从POST改为了GET，结果就发现Ajax请求被缓存，只有第一次查询有效，其后全部是从缓存中取，查询资料后才发现是POST改为GET引起的 解决方法一12345678910//ajax get请求时比较简单 只需将cache设置为false就好 $.ajax(&#123; type: 'get',//get请求时 url: '........', cache: false,//不缓存 data: &#123; &#125;, success: function (result) &#123; &#125; &#125;); 解决方法二1234访问就在URL后面加上[总之就是使每次访问的URL字符串不一样的]URL?+new Date();设计WEB页面的时候 也应该遵守这个原则因为请求同一个地址会直接读取缓存，所以可以在参数中加一个随机数数 让每次参数不一样就好 IOS下Iframe滚动问题 移动端在IOS下的问题居多，后来测试组测出的bug多数属于在IOS下属性不兼容问题，其中就有IOS下Iframe里面页面无法滚动，解决方法如下 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=\"zh-cn\"&gt;&lt;head&gt;&lt;meta charset=\"utf-8\" /&gt;&lt;title&gt;IOS frame 滚动条 demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;style&gt;#wrapper&#123;height:500px;-webkit-overflow-scrolling:touch;overflow:auto;&#125;&lt;/style&gt;&lt;div class=\"container\"&gt; 我是一堆很长。很长，很高，很高的内容。&lt;/div&gt;&lt;script src=\"../jquery.js\"&gt;&lt;/script&gt;&lt;script&gt; var UA = navigator.userAgent; var forIOS = function()&#123; if(!UA.match(/iPad/) &amp;&amp; !UA.match(/iPhone/) &amp;&amp; !UA.match(/iPod/))&#123; return; &#125; if($('#wrapper').length)&#123;return;&#125; $('body').children().not('script').wrapAll('&lt;div id=\"wrapper\"&gt;&lt;/div&gt;'); &#125;();&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 总结这次是我第一次开发微信端，从前端的不熟悉到熟练，自己成长了许多。同时遇到了很多问题，尤其是在IOS下的兼容问题，比如还有像IOS下fixed属性不能用等问题。很多东西只有自己摸索才知道，这次也算是让自己在全栈工程师的道路上又进了一步。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"移动端","slug":"移动端","permalink":"https://sunny0715.github.io/tags/移动端/"},{"name":"前端","slug":"前端","permalink":"https://sunny0715.github.io/tags/前端/"}]},{"title":"MySQL优化","slug":"MySQL优化","date":"2017-04-17T04:31:55.000Z","updated":"2018-03-13T05:51:53.654Z","comments":true,"path":"2017/04/17/MySQL优化/","link":"","permalink":"https://sunny0715.github.io/2017/04/17/MySQL优化/","excerpt":"引言数据库在每个项目中都会用到，现在又分为两个门派，一种关系型数据库，常见的有MySQL、SQL Server、Oracle、DB2等。另一种是非关系型数据库，也就是NOSQL( Not Only SQL)，常见的NOSQL数据库有Redis 、MongoDB、Cassandra等。数据库的优化直接影响到网站的性能，在这里记录一下MySQL的优化。","text":"引言数据库在每个项目中都会用到，现在又分为两个门派，一种关系型数据库，常见的有MySQL、SQL Server、Oracle、DB2等。另一种是非关系型数据库，也就是NOSQL( Not Only SQL)，常见的NOSQL数据库有Redis 、MongoDB、Cassandra等。数据库的优化直接影响到网站的性能，在这里记录一下MySQL的优化。 关于MySQLMySQL 是一个跨平台的开源关系型数据库管理系统，目前 MySQL 被广泛地应用在 Internet 上的中小型网站中。由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，许多中小型网站为了降低网站总体拥有成本而选择了 MySQL 作为网站数据库。比如淘宝、京东等知名公司也都在使用。 MySQL的存储引擎有分为很多种。MyISAM、InnoDB等。每个引擎的特性都不一样，可以在不同的情况下选择不同的存储引擎。 MySQL的优化对于一个小项目来说，MySQL优化与否可能没有那么重要，带来的优化效果也没有那么明显。但是如果面对的是一个千万级的大表、千万级甚至上亿的数据量时，优化是必不可少的。那么要从如下几方面来做优化： 存储引擎一般情况可以选择MyISAM存储引擎，如果需要事务支持必须使用InnoDB存储引擎。 MyISAM类型的表强调的是性能，其执行数度比InnoDB类型更快，但是不提供事务支持，而InnoDB提供事务支持以及外部键等高级数据库功能。 命名规则本着约定优先于配置（Convention Over Configuration）的原则，表的命名规则一样很重要。 MySQL数据库、表、字段等名称统一使用小写，单词间用_下划线分隔。 表名和字段名不宜过长（不超过64个字符）。 建议数据库统一设置编码为utf8，不仅仅是为了应付数据库间导入导出过程中、因编码格式不统一而导致的恼人的乱码问题，也是因为utf8是一种万国码（Unicode）。 语句+索引索引的合理建立和查询语句的优化可以迅速提升数据库性能。 设计阶段就需要预计QPS（Query Per Second）及数据规模，参考业务场景对数据的要求，合理设计表结构（参考mysql在线DDL问题），甚至违反设计范式做到适当冗余。生产环境分析慢日志，优化语句。索引的设计需要知道索引是怎么用的，比如innodb的加锁机制。 垃圾查询拖慢性能。不合理的schema设计也会导致数据存取慢。索引的作用不必多说，但如innodb下，错的索引带来的可能不只是查询变慢。 MySQL语句优化是我们最常见也是开发过程中最需要注意的。各种关键字的使用场合、多表之间的关联(据说阿里的要求是关联表不超多三个)、索引的合理使用、批量插入、批量更新、批量删除、临时表的使用等等。 缓存当数据库的压力太大时可以将一部分压力转嫁到缓存（我常用的是Redis），其流程如下： 复制及读写分离这个是大多数场景下都是必须的。因为复制可以实现备份、高可用、负载均衡。 其中读写分离可以在应用层做，效率高，也可以用三方工具，如360的atlas。 切分切分包括垂直切分和水平切分，实现方式上又包括分库、分表。 垂直切分保证业务的独立性，防止不同业务争抢资源，毕竟业务是有优先级的。 水平切分主要用于突破单机瓶颈。除了主主外，只有切分能真正做到将负载分配下去。 切分后也可对不同片数据进行不同优化。如按时间切分，超过一定时间数据不允许修改，就可以引入压缩了，数据传输及读取减少很多。 根据业务垂直切分。业务内部分库、分表。一般都需要修改应用。除分表外，其余实现不是很复杂。有第三方组件可用，但通用高效又灵活的方式，还是自己写client。 垂直切分一般都要做，只不过业务粒度大小而已。 分库有是经常用的，就算当前压力小，也尽量分出几个逻辑库出来。等规模上去了，很方便就迁移扩展。 水平拆分有一定难度，但如果将来一定会到这个规模，又可能用到，建议越早做越好。因为对应用的改动较大，而且迁移成本高。 总结MySQL总结可以说是： 优化SQL，优化结构，优化存储。 对于MySQL的优化我还需要进一步提高，从表的设计建立到后期的维护考虑的问题有很多，每一步都需要注意。没有DBA，只有自己来实现。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://sunny0715.github.io/tags/mysql/"}]},{"title":"Linux基本命令","slug":"Linux基本命令","date":"2017-04-07T07:59:22.000Z","updated":"2018-03-13T05:50:31.591Z","comments":true,"path":"2017/04/07/Linux基本命令/","link":"","permalink":"https://sunny0715.github.io/2017/04/07/Linux基本命令/","excerpt":"引言之前的公司有用过Linux，自己也学习了一段时间，但是很久没有用了，最近又在腾讯云租了个空间把系统装成Centos系统了，所以又把Linux系统重新捡起来，重温下Linux的基本知识。","text":"引言之前的公司有用过Linux，自己也学习了一段时间，但是很久没有用了，最近又在腾讯云租了个空间把系统装成Centos系统了，所以又把Linux系统重新捡起来，重温下Linux的基本知识。 Linux简介简介Linux，免费开源，多用户多任务系统。基于Linux有多个版本的衍生。RedHat、Ubuntu、Debian 安装VMware或VirtualBox虚拟机。Linux的定义和历史右转百度百科。具体安装步骤，找百度。 常用版本我常用的Linux版本有两个Centos和Ubuntu，全都是开源免费的,其中Ubuntu属于桌面版。 123Centos是免费的企业版Linux操作系统。是RedHat企业版的优化操作系统。具体可以参照百科：http://baike.baidu.com/view/26404.htm。里面有详解。另附其官网:http://www.centos.org/。另外，它适合作为服务器用。 1Ubuntu之前有在环境中开发过项目，虽然时间不久，但还是有所体会。免费、无毒、免折腾、比较接近底层。 基本命令 Linux 操作系统位数识别: uname -a（uname -p） Linux 32位操作系统：Linux x86 i586 i386 i686 i… Linux 64位操作系统：Linux x64x86_64 X64 … man 命令不会用了，找男人 如：man ls ifconfig 显示系统信息 ls 或ll 查看目录文件 pwd 查看目前路径 cat 文件名 从第一个字节开始正向查看文件的内容 head -2 file1 查看一个文件的前两行 tail -2 file1 查看一个文件的最后两行 mv 老名 新名 重命名/剪切 cp 老文件路径+文件名 新文件路径（+文件名） 复制 cd 进入个人的主目录 cd 路径名 进入新路径 cd .. 后退一步 date 显示系统日期 shutdown -h now 关闭系统(1) shutdown -r now 重启(1) reboot 重启(2) halt 关机(推荐) logout 注销 mkdir dir1 创建一个叫做 ‘dir1’ 的目录’ rm -f file1 删除一个叫做 ‘file1’ 的文件’ rmdir dir1 删除一个叫做 ‘dir1’ 的目录’ rm -rf dir1 删除一个叫做 ‘dir1’ 的目录并同时删除其内 find / -name file1 从 ‘/‘ 开始进入根文件系统搜索文件和目录 tar -zxvf archive.tar 解压一个包 rpm -ivh package.rpm 安装一个rpm包 高级一点的命令，也是比较难懂、需要实践和琢磨的命令： chmod +权限(ugo) (u、g、o表示user、group、other) 三种基本权限 R 读 数值表示为4 W 写 数值表示为2 X 可执行 数值表示为1 ​ 例如：chmod 777 表示user、group、other都具有RWX权限。 grep [options] grep命令是一种强大的文本搜索工具 grep ‘test’ d*显示所有以d开头的文件中包含 test的行。 ps [options] 对进程进行监测和控制 ps -aux|grep 8080 查看8080端口占用情况 yum yum [options][command] [package ...] 工具 yum list 列出当前系统中安装的所有包 wget wget [OPTION]… [URL]… wget是一个从网络上自动下载文件的自由工具 wget http://example.com/file.iso 从网上下载单个文件 …………… 总结Linux博大精深，有很多的命令自己使用的比较少也没有用到，用到的时候再去查资料。 更多的命令可以查看http://www.cnblogs.com/skillup/articles/1877812.html","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"},{"name":"linux","slug":"linux","permalink":"https://sunny0715.github.io/tags/linux/"}]},{"title":"JAVA实现简单网络爬虫","slug":"JAVA实现简单爬虫","date":"2017-04-01T06:17:39.000Z","updated":"2018-03-13T05:49:52.521Z","comments":true,"path":"2017/04/01/JAVA实现简单爬虫/","link":"","permalink":"https://sunny0715.github.io/2017/04/01/JAVA实现简单爬虫/","excerpt":"","text":"爬虫基本理解 通俗一点，爬虫是用来快速、批量获取我们在网络需要的东西，过滤掉不需要的东西，比如我可以爬一个网站的所有图片省的一张一张去保存，也可以爬其他数据来做研究、统计、数据分析，即是： (1) 对抓取目标的描述或定义； (2) 对网页或数据的分析与过滤； (3) 对URL的搜索策略。 很多语言都可以做爬虫，在这里记录JAVA做一个简单的爬虫，等以后学会其他语言了再用其他语言做爬虫，哈哈… 实现爬虫需要知识点 简单HTML、CSS、JS等前端知识 正则表达式（很重要，用于过滤不需要的信息） JAVA语言知识（可换成其他语言） 参数 首先你要给它一个种子链接URL 在种子链接的页面查找其他的URL，重复1步骤 有链接有页面，然后你可以在页面中查找需要的内容 简单爬虫代码在这里做个示例：把网站https://www.baidu.com/home/news/data/newspage?nid=7953839918275534&amp;n_type=0&amp;p_from=1 图片全部down下来并保存到本地磁盘的操作。 JAVA基本方式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class Reptile &#123; public static String doGet(String urlStr) throws Exception &#123; URL url; String html = \"\"; try &#123; url = new URL(urlStr); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestProperty(\"Accept\", \"text/html\"); connection.setRequestProperty(\"Accept-Charset\", \"utf-8\"); connection.setRequestProperty(\"Accept-Language\", \"en-US,en\"); connection.setRequestProperty(\"User-Agent\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.22 (KHTML, like Gecko) Chrome/25.0.1364.160 Safari/537.22\"); connection.setRequestMethod(\"GET\"); connection.setConnectTimeout(5000); connection.setDoInput(true); connection.setDoOutput(true); if (connection.getResponseCode() == 200) &#123; System.out.println(\"已连接，正在解析。。。。。。\"); InputStream in = connection.getInputStream(); html = StreamTool.inToStringByByte(in); &#125; else &#123; System.out.println(connection.getResponseCode()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw new Exception(\"get请求失败\"); &#125; return html; &#125; public static void main(String[] args) throws Exception &#123; Reptile reptile = new Reptile() String htmlStr = Reptile.doGet(\"https://www.baidu.com/home/news/data/ newspagenid=7953839918275534&amp;n_type=0&amp;p_from=1\"); File f = new File(\"E://imgs\"); if (!f.exists()) &#123; f.mkdirs(); &#125; Pattern pattern = Pattern.compile(\"&lt;img.*src=(.*?)[^&gt;]*?&gt;\"); //匹配Imag标签 Matcher matcher = pattern.matcher(htmlStr); // 定义一个matcher用来做匹配 System.out.println(\"正在下载\"); while (matcher.find()) &#123; String imgs = matcher.group(); Matcher srcMatcher = Pattern.compile(\"https:\\\"?(.*?)(\\\"|&gt;|\\\\s+)\").matcher(imgs); while (srcMatcher.find()) &#123; String src = srcMatcher.group().substring(0,srcMatcher.group().length() - 1); System.out.println(src); // 获取后缀名 String imageName = src.substring(src.lastIndexOf(\"/\") + 1,src.length()); reptile.downLoad(src, imageName); //下载图片到本地 &#125; &#125; &#125; //下载图片到本地 public void downLoad(String src, String imageName) throws Exception &#123; URL url = new URL(src); URLConnection uri = url.openConnection(); InputStream is = uri.getInputStream(); // 获取数据流 // 写入数据流 OutputStream os = new FileOutputStream(new File(\"E://imgs\", imageName)); byte[] buf = new byte[1024]; int len = 0; while ((len = is.read(buf)) != -1) &#123; os.write(buf, 0, len); &#125; os.close(); is.close(); &#125;&#125; JAVA基本方法主要是利用JAVA中的正则表达式匹配我们我需要的元素，然后再进行其他操作。简单、粗暴。 Jsoup方式 Jsoup 是一个 Java 的开源HTML解析器，可直接解析某个URL地址、HTML文本内容。同时提供了一套非常省力的API，可通过DOM，CSS以及类似于jQuery的操作方法来取出和操作数据。可以直接使用DOM或者JQuery方法和表达式取出数据。 需要下载JAR包，下载地址：点我 Jsoup API：详见：http://www.open-open.com/jsoup/ 工具类StreamTool ：将byte对象转化为String对象 1234567891011121314public class StreamTool &#123;// 将byte对象转化为String对象 public static String inToStringByByte(InputStream in) throws Exception &#123; ByteArrayOutputStream outStr = new ByteArrayOutputStream(); byte[] buffer = new byte[1024]; int len = 0; StringBuilder content = new StringBuilder(); while ((len = in.read(buffer)) != -1) &#123; content.append(new String(buffer, 0, len, \"UTF-8\")); &#125; outStr.close(); return content.toString(); &#125;&#125; 基本实现类Reptile 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class Reptile &#123; public static String doGet(String urlStr) throws Exception &#123; URL url; String html = \"\"; try &#123; url = new URL(urlStr); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); //伪装爬虫，不然会报403错误 connection.setRequestProperty(\"Accept\", \"text/html\"); connection.setRequestProperty(\"Accept-Charset\", \"utf-8\"); connection.setRequestProperty(\"Accept-Language\", \"en-US,en\"); connection.setRequestProperty(\"User-Agent\",\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.22 (KHTML, like Gecko) Chrome/25.0.1364.160 Safari/537.22\"); connection.setRequestMethod(\"GET\"); // 定义请求方式 connection.setConnectTimeout(5000); connection.setDoInput(true); //设置是否向httpUrlConnection输出， 默认情况下是false; connection.setDoOutput(true); // 设置是否从httpUrlConnection读入，默认情况下是true; if (connection.getResponseCode() == 200) &#123; //连接成功 System.out.println(\"已连接，正在解析。。。。。。\"); InputStream in = connection.getInputStream(); html = StreamTool.inToStringByByte(in); &#125; else &#123; System.out.println(connection.getResponseCode()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw new Exception(\"get请求失败\"); &#125; return html; &#125; public static void main(String[] args) throws Exception &#123; URL url ; InputStream is = null; OutputStream os = null; String urlStr = \"https://www.baidu.com/home/news/data/newspage?nid=7953839918275534&amp;n_type=0&amp;p_from=1\"; String htmlStr = Reptile.doGet(urlStr); Document doc = Jsoup.parse(htmlStr); // 将获取的网页 HTML 源代码转化为 Document对象 File f = new File(\"E://imgs\"); //把文件存在E://imgs if (!f.exists()) &#123; f.mkdirs(); &#125; Elements pngs = doc.select(\"img[src]\"); //获取所有图片// Elements pngs = doc.select(\"img[src$=.png]\");只爬取png图片 int i = 1; //计数 for (Element e : pngs) &#123; String src = e.attr(\"src\"); // 获取img中的src路径 String imageName = src.substring(src.lastIndexOf(\"/\") + 1, src.length()); // 获取后缀名 System.out.println(\"正在下载第\" + i + \"张图片：\"+ imageName); URL url = new URL(src); // 连接url URLConnection uri = url.openConnection(); is = uri.getInputStream(); // 获取数据流 os = new FileOutputStream(new File(\"E://imgs\",imageName));// 写入数据流 byte[] buf = new byte[1024]; int len = 0; while ((len = is.read(buf)) != -1) &#123; os.write(buf, 0, len); &#125; i++; &#125; os.close(); is.close(); System.out.println(\"共有\" + (i-1) + \"张图片。\"); &#125;&#125; 总结在这里只做个一个简单的爬虫示例，通过两种方式的比较后，发现Jsoup更佳。 JAVA基本的方式能用正则表达式来匹配所需要的元素，灵活性不高。 Jsoup这个强大的工具提供了DOM和JQuery方法，可以直接操作节点，同时也支持正则表达式，更加的灵活、省力，同时选择性、可玩性和扩展性更高。Jsoup更多的方法可以查看Jsoup的API。 现在已经有很多开源的爬虫的框架供我们选择，比如webmagic、Heritrix等，可以适当选择。 附还有一种更为简单强大的方式，在Linux环境下，利用wget命令只需要一行命令就可以实现以上功能。 1wget -m -H -nd -l 1 -t 1 -A .jpg,.png,.jpeg,.JPEG -e robots=off -P /opt/download --no-check-certificate https://www.baidu.com/home/news/data/newspage?nid=7953839918275534&amp;n_type=0&amp;p_from=1 在下篇博客写一下Linux的基本命令。","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tool","slug":"tool","permalink":"https://sunny0715.github.io/tags/tool/"}]},{"title":"JAVA定时调度 Timer和Executors","slug":"JAVA定时调度-Timer和Executors","date":"2017-03-27T09:42:49.000Z","updated":"2018-03-13T05:49:02.851Z","comments":true,"path":"2017/03/27/JAVA定时调度-Timer和Executors/","link":"","permalink":"https://sunny0715.github.io/2017/03/27/JAVA定时调度-Timer和Executors/","excerpt":"近期在公司做了一个关于定时执行任务的功能（没有使用框架定时），查了一下资料，有Thread、Timer和Executors三种方法，之前使用的是Timer，但是详细查了资料觉得Executors更优，所以在这里比较一下它们的区别。","text":"近期在公司做了一个关于定时执行任务的功能（没有使用框架定时），查了一下资料，有Thread、Timer和Executors三种方法，之前使用的是Timer，但是详细查了资料觉得Executors更优，所以在这里比较一下它们的区别。 Thread类这是最基本的，创建一个Thread，然后让它在while循环里一直运行着，通过sleep方法来达到定时任务的效果。这样可以快速简单的实现，代码如下： 12345678910111213141516171819202122public class Task1 &#123; public static void main(String[] args) &#123; // run in a second final long timeInterval = 1000; Runnable runnable = new Runnable() &#123; public void run() &#123; while (true) &#123; // ------- code for task to run System.out.println(\"Hello !!\"); // ------- ends here try &#123; Thread.sleep(timeInterval); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;; Thread thread = new Thread(runnable); thread.start(); &#125; &#125; Thread的做定时任务的几率不大，因为不可控制启动停止时间、不能指定想要的delay时间。 Timer类 于第一种方式相比，优势 : (1) 当启动和去取消任务时可以控制 ; (2) 第一次执行任务时可以指定你想要的delay时间。 在实现时，Timer类可以调度任务，TimerTask则是通过在run()方法里实现具体任务。 Timer实例可以调度多任务，它是线程安全的。 当Timer的构造器被调用时，它创建了一个线程，这个线程可以用来调度任务。 12345678910111213141516public class Task2 &#123; public static void main(String[] args) &#123; TimerTask task = new TimerTask() &#123; @Override public void run() &#123; // task to run goes here System.out.println(\"Hello !!!\"); &#125; &#125;; Timer timer = new Timer(); long delay = 0; long intevalPeriod = 1 * 1000; // schedules the task to be run in an interval timer.scheduleAtFixedRate(task, delay, intevalPeriod); &#125; // end of main &#125; 缺点：如果TimerTask抛出未检查的异常，Timer将会产生无法预料的行为。Timer线程并不捕获异常，所以 TimerTask抛出的未检查的异常会终止timer线程。这种情况下，Timer也不会再重新恢复线程的执行了;它错误的认为整个Timer都被取消了。此时，已经被安排但尚未执行的TimerTask永远不会再执行了，新的任务也不能被调度了。 Executors ScheduledExecutorService是从Java SE5的java.util.concurrent里，做为并发工具类被引进的，这是最理想的定时任务实现方式。 相比于上两个方法，它有以下好处 : (1) 相比于Timer的单线程，它是通过线程池的方式来执行任务的 ; (2) 可以很灵活的去设定第一次执行任务delay时间 ; (3) 提供了良好的约定，以便设定执行的时间间隔 。 下面是实现代码，我们通过ScheduledExecutorService展示这个例子，通过代码里参数的控制，首次执行加了delay时间。 1234567891011121314public class Task3 &#123; public static void main(String[] args) &#123; Runnable runnable = new Runnable() &#123; public void run() &#123; // task to run goes here System.out.println(\"Hello !!\"); &#125; &#125;; ScheduledExecutorService service = Executors .newSingleThreadScheduledExecutor(); // 第二个参数为首次执行的延时时间，第三个参数为定时执行的间隔时间 service.scheduleAtFixedRate(runnable, 10, 1, TimeUnit.SECONDS); &#125; &#125; 线程池能按时间计划来执行任务，允许用户设定计划执行任务的时间。 当任务较多时，线程池可能会自动创建更多的工作线程来执行任务 。 支持多个任务并发执行。 总结Timer是单线程的。所以task都是串行执行。假如其中一个task执行需要很长的时间，那其他的task只能干巴巴的等着。 ScheduledThreadPoolExecutor是一个可以重复执行任务的线程池，并且可以指定任务的间隔和延迟时间。它作为比Timer/TimerTask更加通用的替代品。因为它允许多个服务线程，接受不同的时间单位，且不需要继承TimeTask（仅仅需要实现Runnable接口）。配置ScheduledThreadPoolExecutor为单线程，则与使用Timer等效。 上述，基本说明了在以后的开发中尽可能使用ScheduledExecutorService(JDK1.5以后)替代Timer。 下面是自己做的功能，通过短信API定时查询教师回复信息并更新数据库。 123456789101112131415161718192021222324252627282930/** * 定时查询教师回复状态 * @param a */public void getStatusSchedule(final Date replyEnd)&#123; final SendMessage sendMsg = new SendMessage(); final ScheduledExecutorService service = Executors.newSingleThreadScheduledExecutor(); service.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; Date nowDate = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").parse(new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(new Date()));//当前时间 //若截至时间在当前时间之前执行定时任务 否则不执行 if (!nowDate.before(replyEnd)) &#123; service.shutdown(); //停止任务 return; &#125;else &#123; Map&lt;String,Object&gt; map = sendMsg.getReplyMsg(); //获取回复信息 if(!map.isEmpty())&#123; //当map不为空时执行 updateMsgStatus(map); //更新数据库 &#125; &#125; &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, 1, 1, TimeUnit.MINUTES); //执行后第一次查询在1分钟之后，每隔1分钟查询一次。 &#125;","categories":[{"name":"technology","slug":"technology","permalink":"https://sunny0715.github.io/categories/technology/"}],"tags":[{"name":"java","slug":"java","permalink":"https://sunny0715.github.io/tags/java/"},{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"}]},{"title":"浅谈共享单车","slug":"浅谈共享单车","date":"2017-03-24T09:40:22.000Z","updated":"2018-01-05T01:16:52.638Z","comments":true,"path":"2017/03/24/浅谈共享单车/","link":"","permalink":"https://sunny0715.github.io/2017/03/24/浅谈共享单车/","excerpt":"​ 要说目前最火爆的互联网现象，当属共享单车。这里所谓的共享，本质上指的是需求共享。就骑车的需求，从一个地方骑到另一个地方，就是用户触点（产品投放）、产品操作（骑行过程）、付费模式（支付）的流程，从个体上来讲，这个流程是单向的。但在“共享”的模式下，扫码骑走，停车即走，把不同的人，在不同的时间和地点，相同的骑行需求，通过产品操作节点，形成需求闭环，停车点同时又是骑行出发点，这就是需求共享。","text":"​ 要说目前最火爆的互联网现象，当属共享单车。这里所谓的共享，本质上指的是需求共享。就骑车的需求，从一个地方骑到另一个地方，就是用户触点（产品投放）、产品操作（骑行过程）、付费模式（支付）的流程，从个体上来讲，这个流程是单向的。但在“共享”的模式下，扫码骑走，停车即走，把不同的人，在不同的时间和地点，相同的骑行需求，通过产品操作节点，形成需求闭环，停车点同时又是骑行出发点，这就是需求共享。 共享单车是指企业与政府合作，在地铁、学校、公交站点、居民区、商业区、公共服务区等提供自行车单车共享服务，是共享经济的一种新形态。 ​ 2016年底以来，国内共享单车突然就火爆了起来，而在街头，仿佛一夜之间，共享单车已经到了“泛滥”的地步，各大城市路边排满各种颜色的共享单车。 场景分析​ 我们站在用户角度，无非就是找车，然后骑车，交钱。 在“寻车-用车-骑车-还车”的场景闭环中，需要考虑的问题有很多，列举以下若干种： 什么样的寻车方式更符合大众的日常行为？ 用户与单车之间如何建立一一对应的联系？ 使用何种开锁构件实现远程开锁和上锁？ 计费方式及费用节点、价格、操作流程分别采取什么方案？ 如何对车辆进行远程管理？ 如何防止逃费、盗窃、破坏等衍生问题？ 场景分析的过程，就是解决以上若干问题的过程，针对这些问题，分别提出不同的业务流程和技术方案。 业务逻辑分析以摩拜单车为例，用户-管理平台-单车 之间的关系如下图: 技术实现方案​ 现在共享单车最火的要数摩拜单车和OFO小黄车了，摩拜采用智能锁而ofo采用的是机械锁。 1 机械锁​ 原理：机械锁的原理很简单，只需要打开软件，输入对应的车牌号就可以了，其实就是后台查询数据库，判断单车是否处于正常状态，返回给用户开锁密码，用户拿着开锁密码开锁。 2 智能锁​ 原理：对于单车的远程开锁机制，采用远程通信控制机械构件的电磁运动来实现。远程通信可采用传统的SIM卡通信的方式。 一、手机扫描自行车，获得自行车唯一的ID标志，手机接着会像服务器提交一个请求（提交信息里包含：用户信息，请求动作，车辆ID）；二、服务器收到用户开锁请求，此时会根据请求信息，接着向指定ID的自行车发出开锁指令;三、自行车收到服务器请求，会执行相应的开锁动作。 智能锁是耗费电能的，所以摩拜单车车篮中装有太阳能电池板，减少人力物力维护的成本、简单、高效。 ​ 这是一个典型的大容量互联网O2O场景，连结用户、车辆，管理平台进行实时处理效率要求非常高，需制定可靠、高效的网络方案。根据业务流程，我们梳理出网络节点的职能，并从成本考虑使用最优方案： ​ 明确流程，界面，那么接下来的任务就是通过用户语言去实现产品流程了，即界面设计与开发实现，这里我们就不阐述了。 一点感想​ 从2016年到现在已有近半年时间，共享单车的竞争也愈演愈烈，近几天，共享单车从免费到红包“撒钱” 导致竞争升级，共享单车方便了我们的出行，但是带来的问题也很多，政府也在不断的规范使用。现在都在处于资本投入和烧钱大战中，希望共享单车一直存货下去，同时希望大家能合理、合法使用。 之前有听说过摩拜的扫一扫可以远程使用，假如我需要车又没有注册，我可以拍照给有车的朋友，让他们帮我远程扫就可以开启，亲测：不行！还是自己乖乖注册一个吧。","categories":[{"name":"society","slug":"society","permalink":"https://sunny0715.github.io/categories/society/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://sunny0715.github.io/tags/随笔/"}]},{"title":"Markdown语法指南","slug":"Markdown语法","date":"2017-03-21T14:51:10.000Z","updated":"2018-03-13T05:51:37.762Z","comments":true,"path":"2017/03/21/Markdown语法/","link":"","permalink":"https://sunny0715.github.io/2017/03/21/Markdown语法/","excerpt":"花了一段时间把自己的个人博客搭建好了，但是博客必须是要用Markdown书写，所以查了一下Markdown编辑器的语法，在这里做个记录。","text":"花了一段时间把自己的个人博客搭建好了，但是博客必须是要用Markdown书写，所以查了一下Markdown编辑器的语法，在这里做个记录。 Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。Markdown的语法简洁明了、学习容易，而且功能比纯文本更强，因此有很多人用它写博客。世界上最流行的博客平台WordPress和大型CMS如Joomla、Drupal都能很好的支持Markdown。完全采用Markdown编辑器的博客平台有Ghost和Typecho。 基本技巧1 代码如果你只想高亮语句中的某个函数名或关键字，可以使用 `function_name()` 实现 通常编辑器根据代码片段适配合适的高亮方法，但你也可以用 ``` 包裹一段代码，并指定一种语言 12345​```javascript$(document).ready(function () &#123; alert('hello world');&#125;);​``` 支持的语言：actionscript, apache, bash, clojure, cmake, coffeescript, cpp, cs, css, d, delphi, django, erlang, go, haskell, html, http, ini, java, javascript, json, lisp, lua, markdown, matlab, nginx, objectivec, perl, php, python, r, ruby, scala, smalltalk, sql, tex, vbscript, xml 也可以使用 4 空格缩进，再贴上代码，实现相同的的效果 123 def g(x): yield from range(x, 0, -1) yield from range(x) 2 标题文章内容较多时，可以用标题分段： 12345678标题1======标题2-----## 大标题 ##### 小标题 ### 3 粗斜体123*斜体文本* _斜体文本_**粗体文本** __粗体文本__***粗斜体文本*** ___粗斜体文本___ 4 链接4.1 常用链接方法 12文字链接 ![链接名称](http://链接网址)网址链接 &lt;http://链接网址&gt; 4.2 高级链接技巧 123456这个链接用 1 作为网址变量 [Google][1].这个链接用 yahoo 作为网址变量 [Yahoo!][yahoo].然后在文档的结尾为变量赋值（网址） [1]: http://www.google.com/ [yahoo]: http://www.yahoo.com/ 5 列表5.1 普通无序列表 123- 列表文本前使用 [减号+空格]+ 列表文本前使用 [加号+空格]* 列表文本前使用 [星号+空格] 5.2 普通有序列表 1231. 列表前使用 [数字+空格]2. 我们会自动帮你添加数字7. 不用担心数字不对，显示的时候我们会自动把这行的 7 纠正为 3 5.3 列表嵌套 12345678910111213141516171. 列出所有元素： - 无序列表元素 A 1. 元素 A 的有序子列表 - 前面加四个空格2. 列表里的多段换行： 前面必须加四个空格， 这样换行，整体的格式不会乱3. 列表里引用： &gt; 前面空一行 &gt; 仍然需要在 &gt; 前面加四个空格4. 列表里代码段：前面四个空格，之后按代码语法 ``` 书写​``` 或者直接空八个，引入代码块 6 引用6.1 普通引用 12&gt; 引用文本前使用 [大于号+空格]&gt; 折行可以不加，新起一行都要加上哦 6.2 引用里嵌套引用 123&gt; 最外层引用&gt; &gt; 多一个 &gt; 嵌套一层引用&gt; &gt; &gt; 可以嵌套很多层 6.3 引用里嵌套列表 123&gt; - 这是引用里嵌套的一个列表&gt; - 还可以有子列表&gt; * 子列表需要从 - 之后延后四个空格开始 6.4 引用里嵌套代码块 12345&gt; 同样的，在前面加四个空格形成代码块&gt; &gt; &gt; 或者使用 ``` 形成代码块&gt; `` 7 图片7.1 跟链接的方法区别在于前面加了个感叹号 !，这样是不是觉得好记多了呢？ 1![图片名称](http://图片网址) 7.2 当然，你也可以像网址那样对图片网址使用变量 1234这个链接用 1 作为网址变量 [Google][1].然后在文档的结尾位变量赋值（网址） [1]: http://www.google.com/logo.png 也可以使用 HTML 的图片语法来自定义图片的宽高大小 1&lt;img src=\"htt://example.com/sample.png\" width=\"400\" height=\"100\"&gt; 8 换行如果另起一行，只需在当前行结尾加 2 个空格 12在当前行的结尾加 2 个空格 这行就会新起一行 如果是要起一个新段落，只需要空出一行即可。 9 分隔符如果你有写分割线的习惯，可以新起一行输入三个减号-。当前后都有段落时，请空出一行： 12345前面的段落---后面的段落 高级技巧1 行内 HTML 元素目前只支持部分段内 HTML 元素效果，包括 ，如 键位显示 1使用 &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Alt&lt;/kbd&gt;+&lt;kbd&gt;Del&lt;/kbd&gt; 重启电脑 代码块 1使用 &lt;pre&gt;&lt;/pre&gt; 元素同样可以形成代码块 粗斜体 1&lt;b&gt; Markdown 在此处同样适用，如 *加粗* &lt;/b&gt; 2 符号转义如果你的描述中需要用到 markdown 的符号，比如 _ # * 等，但又不想它被转义，这时候可以在这些符号前加反斜杠，如 \\_ \\#``\\* 进行避免。 12\\_不想这里的文本变斜体\\_\\*\\*不想这里的文本被加粗\\*\\* 3 扩展支持 jsfiddle、gist、runjs、优酷视频，直接填写 url，在其之后会自动添加预览点击会展开相关内容。 1234http://&#123;url_of_the_fiddle&#125;/embedded/[&#123;tabs&#125;/[&#123;style&#125;]]/https://gist.github.com/&#123;gist_id&#125;http://runjs.cn/detail/&#123;id&#125;http://v.youku.com/v_show/id_&#123;video_id&#125;.html 4 公式当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或 LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax 对数学公式进行渲染。如： 12345$$ x = &#123;-b \\pm \\sqrt&#123;b^2-4ac&#125; \\over 2a&#125;. $$$$x \\href&#123;why-equal.html&#125;&#123;=&#125; y^2 + 1$$ 同时也支持 HTML 属性，如： 12345$$ (x+1)^2 = \\class&#123;hidden&#125;&#123;(x+1)(x+1)&#125; $$$$(x+1)^2 = \\cssId&#123;step1&#125;&#123;\\style&#123;visibility:hidden&#125;&#123;(x+1)(x+1)&#125;&#125;$$ 总结markdown语法写多了自然就会了，网上有很多markdown语法编辑器，比如有道云、马克飞象、Typora等。我目前使用的是Typora编辑器，使用起来比其他的更简单、舒适，方便。","categories":[],"tags":[{"name":"tips","slug":"tips","permalink":"https://sunny0715.github.io/tags/tips/"}]},{"title":"博客建成第一天","slug":"博客建成第一天","date":"2017-03-07T05:30:26.000Z","updated":"2018-01-05T01:17:10.015Z","comments":true,"path":"2017/03/07/博客建成第一天/","link":"","permalink":"https://sunny0715.github.io/2017/03/07/博客建成第一天/","excerpt":"","text":"经过几天的努力，自己的博客终于搭建起来了，打心里很开心。 这不是贴吧，不是豆瓣，不是CSDN，这是我自己在互联网上的一小点领地。 从开始博客基本样式，到域名，自己一步一步摸索搭建起来的，我相信自己可以做的更好，加油!","categories":[{"name":"life","slug":"life","permalink":"https://sunny0715.github.io/categories/life/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://sunny0715.github.io/tags/随笔/"}]}]}